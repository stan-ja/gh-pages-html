<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Stan Development Team">
  <meta name="author" content="翻訳: stan-jaチーム">
  <title>Stan モデリング言語: ユーザーガイド・リファレンスマニュアル</title>
  <style type="text/css">code{white-space: pre;}</style>
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
  <link href="data:text/css;charset=utf-8,%2Ehljs%7Bdisplay%3Ablock%3Boverflow%2Dx%3Aauto%3Bpadding%3A0%2E5em%3Bbackground%3A%23F0F0F0%7D%2Ehljs%2C%2Ehljs%2Dsubst%7Bcolor%3A%23444%7D%2Ehljs%2Dkeyword%2C%2Ehljs%2Dattribute%2C%2Ehljs%2Dselector%2Dtag%2C%2Ehljs%2Dmeta%2Dkeyword%2C%2Ehljs%2Ddoctag%2C%2Ehljs%2Dname%7Bfont%2Dweight%3Abold%7D%2Ehljs%2Dbuilt%5Fin%2C%2Ehljs%2Dliteral%2C%2Ehljs%2Dbullet%2C%2Ehljs%2Dcode%2C%2Ehljs%2Daddition%7Bcolor%3A%231F811F%7D%2Ehljs%2Dregexp%2C%2Ehljs%2Dsymbol%2C%2Ehljs%2Dvariable%2C%2Ehljs%2Dtemplate%2Dvariable%2C%2Ehljs%2Dlink%2C%2Ehljs%2Dselector%2Dattr%2C%2Ehljs%2Dselector%2Dpseudo%7Bcolor%3A%23BC6060%7D%2Ehljs%2Dtype%2C%2Ehljs%2Dstring%2C%2Ehljs%2Dnumber%2C%2Ehljs%2Dselector%2Did%2C%2Ehljs%2Dselector%2Dclass%2C%2Ehljs%2Dquote%2C%2Ehljs%2Dtemplate%2Dtag%2C%2Ehljs%2Ddeletion%7Bcolor%3A%23880000%7D%2Ehljs%2Dtitle%2C%2Ehljs%2Dsection%7Bcolor%3A%23880000%3Bfont%2Dweight%3Abold%7D%2Ehljs%2Dcomment%7Bcolor%3A%23888888%7D%2Ehljs%2Dmeta%7Bcolor%3A%232B6EA1%7D%2Ehljs%2Demphasis%7Bfont%2Dstyle%3Aitalic%7D%2Ehljs%2Dstrong%7Bfont%2Dweight%3Abold%7D" rel="stylesheet" />
  <script src="data:application/javascript; charset=utf-8;base64,/*! highlight.js v9.1.0 | BSD3 License | git.io/hljslicense */
!function(e){"undefined"!=typeof exports?e(exports):(self.hljs=e({}),"function"==typeof define&&define.amd&&define("hljs",[],function(){return self.hljs}))}(function(e){function t(e){return e.replace(/&/gm,"&amp;").replace(/</gm,"&lt;").replace(/>/gm,"&gt;")}function r(e){return e.nodeName.toLowerCase()}function a(e,t){var r=e&&e.exec(t);return r&&0==r.index}function n(e){return/^(no-?highlight|plain|text)$/i.test(e)}function i(e){var t,r,a,i=e.className+" ";if(i+=e.parentNode?e.parentNode.className:"",r=/\blang(?:uage)?-([\w-]+)\b/i.exec(i))return y(r[1])?r[1]:"no-highlight";for(i=i.split(/\s+/),t=0,a=i.length;a>t;t++)if(y(i[t])||n(i[t]))return i[t]}function s(e,t){var r,a={};for(r in e)a[r]=e[r];if(t)for(r in t)a[r]=t[r];return a}function c(e){var t=[];return function a(e,n){for(var i=e.firstChild;i;i=i.nextSibling)3==i.nodeType?n+=i.nodeValue.length:1==i.nodeType&&(t.push({event:"start",offset:n,node:i}),n=a(i,n),r(i).match(/br|hr|img|input/)||t.push({event:"stop",offset:n,node:i}));return n}(e,0),t}function o(e,a,n){function i(){return e.length&&a.length?e[0].offset!=a[0].offset?e[0].offset<a[0].offset?e:a:"start"==a[0].event?e:a:e.length?e:a}function s(e){function a(e){return" "+e.nodeName+'="'+t(e.value)+'"'}u+="<"+r(e)+Array.prototype.map.call(e.attributes,a).join("")+">"}function c(e){u+="</"+r(e)+">"}function o(e){("start"==e.event?s:c)(e.node)}for(var l=0,u="",d=[];e.length||a.length;){var b=i();if(u+=t(n.substr(l,b[0].offset-l)),l=b[0].offset,b==e){d.reverse().forEach(c);do o(b.splice(0,1)[0]),b=i();while(b==e&&b.length&&b[0].offset==l);d.reverse().forEach(s)}else"start"==b[0].event?d.push(b[0].node):d.pop(),o(b.splice(0,1)[0])}return u+t(n.substr(l))}function l(e){function t(e){return e&&e.source||e}function r(r,a){return new RegExp(t(r),"m"+(e.cI?"i":"")+(a?"g":""))}function a(n,i){if(!n.compiled){if(n.compiled=!0,n.k=n.k||n.bK,n.k){var c={},o=function(t,r){e.cI&&(r=r.toLowerCase()),r.split(" ").forEach(function(e){var r=e.split("|");c[r[0]]=[t,r[1]?Number(r[1]):1]})};"string"==typeof n.k?o("keyword",n.k):Object.keys(n.k).forEach(function(e){o(e,n.k[e])}),n.k=c}n.lR=r(n.l||/\b\w+\b/,!0),i&&(n.bK&&(n.b="\\b("+n.bK.split(" ").join("|")+")\\b"),n.b||(n.b=/\B|\b/),n.bR=r(n.b),n.e||n.eW||(n.e=/\B|\b/),n.e&&(n.eR=r(n.e)),n.tE=t(n.e)||"",n.eW&&i.tE&&(n.tE+=(n.e?"|":"")+i.tE)),n.i&&(n.iR=r(n.i)),void 0===n.r&&(n.r=1),n.c||(n.c=[]);var l=[];n.c.forEach(function(e){e.v?e.v.forEach(function(t){l.push(s(e,t))}):l.push("self"==e?n:e)}),n.c=l,n.c.forEach(function(e){a(e,n)}),n.starts&&a(n.starts,i);var u=n.c.map(function(e){return e.bK?"\\.?("+e.b+")\\.?":e.b}).concat([n.tE,n.i]).map(t).filter(Boolean);n.t=u.length?r(u.join("|"),!0):{exec:function(){return null}}}}a(e)}function u(e,r,n,i){function s(e,t){for(var r=0;r<t.c.length;r++)if(a(t.c[r].bR,e))return t.c[r]}function c(e,t){if(a(e.eR,t)){for(;e.endsParent&&e.parent;)e=e.parent;return e}return e.eW?c(e.parent,t):void 0}function o(e,t){return!n&&a(t.iR,e)}function b(e,t){var r=v.cI?t[0].toLowerCase():t[0];return e.k.hasOwnProperty(r)&&e.k[r]}function p(e,t,r,a){var n=a?"":w.classPrefix,i='<span class="'+n,s=r?"":"</span>";return i+=e+'">',i+t+s}function m(){if(!x.k)return t(E);var e="",r=0;x.lR.lastIndex=0;for(var a=x.lR.exec(E);a;){e+=t(E.substr(r,a.index-r));var n=b(x,a);n?(B+=n[1],e+=p(n[0],t(a[0]))):e+=t(a[0]),r=x.lR.lastIndex,a=x.lR.exec(E)}return e+t(E.substr(r))}function f(){var e="string"==typeof x.sL;if(e&&!N[x.sL])return t(E);var r=e?u(x.sL,E,!0,C[x.sL]):d(E,x.sL.length?x.sL:void 0);return x.r>0&&(B+=r.r),e&&(C[x.sL]=r.top),p(r.language,r.value,!1,!0)}function g(){return void 0!==x.sL?f():m()}function h(e,r){var a=e.cN?p(e.cN,"",!0):"";e.rB?(M+=a,E=""):e.eB?(M+=t(r)+a,E=""):(M+=a,E=r),x=Object.create(e,{parent:{value:x}})}function _(e,r){if(E+=e,void 0===r)return M+=g(),0;var a=s(r,x);if(a)return M+=g(),h(a,r),a.rB?0:r.length;var n=c(x,r);if(n){var i=x;i.rE||i.eE||(E+=r),M+=g();do x.cN&&(M+="</span>"),B+=x.r,x=x.parent;while(x!=n.parent);return i.eE&&(M+=t(r)),E="",n.starts&&h(n.starts,""),i.rE?0:r.length}if(o(r,x))throw new Error('Illegal lexeme "'+r+'" for mode "'+(x.cN||"<unnamed>")+'"');return E+=r,r.length||1}var v=y(e);if(!v)throw new Error('Unknown language: "'+e+'"');l(v);var k,x=i||v,C={},M="";for(k=x;k!=v;k=k.parent)k.cN&&(M=p(k.cN,"",!0)+M);var E="",B=0;try{for(var $,z,L=0;;){if(x.t.lastIndex=L,$=x.t.exec(r),!$)break;z=_(r.substr(L,$.index-L),$[0]),L=$.index+z}for(_(r.substr(L)),k=x;k.parent;k=k.parent)k.cN&&(M+="</span>");return{r:B,value:M,language:e,top:x}}catch(R){if(-1!=R.message.indexOf("Illegal"))return{r:0,value:t(r)};throw R}}function d(e,r){r=r||w.languages||Object.keys(N);var a={r:0,value:t(e)},n=a;return r.forEach(function(t){if(y(t)){var r=u(t,e,!1);r.language=t,r.r>n.r&&(n=r),r.r>a.r&&(n=a,a=r)}}),n.language&&(a.second_best=n),a}function b(e){return w.tabReplace&&(e=e.replace(/^((<[^>]+>|\t)+)/gm,function(e,t){return t.replace(/\t/g,w.tabReplace)})),w.useBR&&(e=e.replace(/\n/g,"<br>")),e}function p(e,t,r){var a=t?k[t]:r,n=[e.trim()];return e.match(/\bhljs\b/)||n.push("hljs"),-1===e.indexOf(a)&&n.push(a),n.join(" ").trim()}function m(e){var t=i(e);if(!n(t)){var r;w.useBR?(r=document.createElementNS("http://www.w3.org/1999/xhtml","div"),r.innerHTML=e.innerHTML.replace(/\n/g,"").replace(/<br[ \/]*>/g,"\n")):r=e;var a=r.textContent,s=t?u(t,a,!0):d(a),l=c(r);if(l.length){var m=document.createElementNS("http://www.w3.org/1999/xhtml","div");m.innerHTML=s.value,s.value=o(l,c(m),a)}s.value=b(s.value),e.innerHTML=s.value,e.className=p(e.className,t,s.language),e.result={language:s.language,re:s.r},s.second_best&&(e.second_best={language:s.second_best.language,re:s.second_best.r})}}function f(e){w=s(w,e)}function g(){if(!g.called){g.called=!0;var e=document.querySelectorAll("pre code");Array.prototype.forEach.call(e,m)}}function h(){addEventListener("DOMContentLoaded",g,!1),addEventListener("load",g,!1)}function _(t,r){var a=N[t]=r(e);a.aliases&&a.aliases.forEach(function(e){k[e]=t})}function v(){return Object.keys(N)}function y(e){return e=(e||"").toLowerCase(),N[e]||N[k[e]]}var w={classPrefix:"hljs-",tabReplace:null,useBR:!1,languages:void 0},N={},k={};return e.highlight=u,e.highlightAuto=d,e.fixMarkup=b,e.highlightBlock=m,e.configure=f,e.initHighlighting=g,e.initHighlightingOnLoad=h,e.registerLanguage=_,e.listLanguages=v,e.getLanguage=y,e.inherit=s,e.IR="[a-zA-Z]\\w*",e.UIR="[a-zA-Z_]\\w*",e.NR="\\b\\d+(\\.\\d+)?",e.CNR="(-?)(\\b0[xX][a-fA-F0-9]+|(\\b\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)",e.BNR="\\b(0b[01]+)",e.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|-|-=|/=|/|:|;|<<|<<=|<=|<|===|==|=|>>>=|>>=|>=|>>>|>>|>|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~",e.BE={b:"\\\\[\\s\\S]",r:0},e.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[e.BE]},e.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[e.BE]},e.PWM={b:/\b(a|an|the|are|I|I'm|isn't|don't|doesn't|won't|but|just|should|pretty|simply|enough|gonna|going|wtf|so|such|will|you|your|like)\b/},e.C=function(t,r,a){var n=e.inherit({cN:"comment",b:t,e:r,c:[]},a||{});return n.c.push(e.PWM),n.c.push({cN:"doctag",b:"(?:TODO|FIXME|NOTE|BUG|XXX):",r:0}),n},e.CLCM=e.C("//","$"),e.CBCM=e.C("/\\*","\\*/"),e.HCM=e.C("#","$"),e.NM={cN:"number",b:e.NR,r:0},e.CNM={cN:"number",b:e.CNR,r:0},e.BNM={cN:"number",b:e.BNR,r:0},e.CSSNM={cN:"number",b:e.NR+"(%|em|ex|ch|rem|vw|vh|vmin|vmax|cm|mm|in|pt|pc|px|deg|grad|rad|turn|s|ms|Hz|kHz|dpi|dpcm|dppx)?",r:0},e.RM={cN:"regexp",b:/\//,e:/\/[gimuy]*/,i:/\n/,c:[e.BE,{b:/\[/,e:/\]/,r:0,c:[e.BE]}]},e.TM={cN:"title",b:e.IR,r:0},e.UTM={cN:"title",b:e.UIR,r:0},e.registerLanguage("apache",function(e){var t={cN:"number",b:"[\\$%]\\d+"};return{aliases:["apacheconf"],cI:!0,c:[e.HCM,{cN:"section",b:"</?",e:">"},{cN:"attribute",b:/\w+/,r:0,k:{nomarkup:"order deny allow setenv rewriterule rewriteengine rewritecond documentroot sethandler errordocument loadmodule options header listen serverroot servername"},starts:{e:/$/,r:0,k:{literal:"on off all"},c:[{cN:"meta",b:"\\s\\[",e:"\\]$"},{cN:"variable",b:"[\\$%]\\{",e:"\\}",c:["self",t]},t,e.QSM]}}],i:/\S/}}),e.registerLanguage("bash",function(e){var t={cN:"variable",v:[{b:/\$[\w\d#@][\w\d_]*/},{b:/\$\{(.*?)}/}]},r={cN:"string",b:/"/,e:/"/,c:[e.BE,t,{cN:"variable",b:/\$\(/,e:/\)/,c:[e.BE]}]},a={cN:"string",b:/'/,e:/'/};return{aliases:["sh","zsh"],l:/-?[a-z\.]+/,k:{keyword:"if then else elif fi for while in do done case esac function",literal:"true false",built_in:"break cd continue eval exec exit export getopts hash pwd readonly return shift test times trap umask unset alias bind builtin caller command declare echo enable help let local logout mapfile printf read readarray source type typeset ulimit unalias set shopt autoload bg bindkey bye cap chdir clone comparguments compcall compctl compdescribe compfiles compgroups compquote comptags comptry compvalues dirs disable disown echotc echoti emulate fc fg float functions getcap getln history integer jobs kill limit log noglob popd print pushd pushln rehash sched setcap setopt stat suspend ttyctl unfunction unhash unlimit unsetopt vared wait whence where which zcompile zformat zftp zle zmodload zparseopts zprof zpty zregexparse zsocket zstyle ztcp",_:"-ne -eq -lt -gt -f -d -e -s -l -a"},c:[{cN:"meta",b:/^#![^\n]+sh\s*$/,r:10},{cN:"function",b:/\w[\w\d_]*\s*\(\s*\)\s*\{/,rB:!0,c:[e.inherit(e.TM,{b:/\w[\w\d_]*/})],r:0},e.HCM,r,a,t]}}),e.registerLanguage("coffeescript",function(e){var t={keyword:"in if for while finally new do return else break catch instanceof throw try this switch continue typeof delete debugger super then unless until loop of by when and or is isnt not",literal:"true false null undefined yes no on off",built_in:"npm require console print module global window document"},r="[A-Za-z$_][0-9A-Za-z$_]*",a={cN:"subst",b:/#\{/,e:/}/,k:t},n=[e.BNM,e.inherit(e.CNM,{starts:{e:"(\\s*/)?",r:0}}),{cN:"string",v:[{b:/'''/,e:/'''/,c:[e.BE]},{b:/'/,e:/'/,c:[e.BE]},{b:/"""/,e:/"""/,c:[e.BE,a]},{b:/"/,e:/"/,c:[e.BE,a]}]},{cN:"regexp",v:[{b:"///",e:"///",c:[a,e.HCM]},{b:"//[gim]*",r:0},{b:/\/(?![ *])(\\\/|.)*?\/[gim]*(?=\W|$)/}]},{b:"@"+r},{b:"`",e:"`",eB:!0,eE:!0,sL:"javascript"}];a.c=n;var i=e.inherit(e.TM,{b:r}),s="(\\(.*\\))?\\s*\\B[-=]>",c={cN:"params",b:"\\([^\\(]",rB:!0,c:[{b:/\(/,e:/\)/,k:t,c:["self"].concat(n)}]};return{aliases:["coffee","cson","iced"],k:t,i:/\/\*/,c:n.concat([e.C("###","###"),e.HCM,{cN:"function",b:"^\\s*"+r+"\\s*=\\s*"+s,e:"[-=]>",rB:!0,c:[i,c]},{b:/[:\(,=]\s*/,r:0,c:[{cN:"function",b:s,e:"[-=]>",rB:!0,c:[c]}]},{cN:"class",bK:"class",e:"$",i:/[:="\[\]]/,c:[{bK:"extends",eW:!0,i:/[:="\[\]]/,c:[i]},i]},{b:r+":",e:":",rB:!0,rE:!0,r:0}])}}),e.registerLanguage("cpp",function(e){var t={cN:"keyword",b:"\\b[a-z\\d_]*_t\\b"},r={cN:"string",v:[e.inherit(e.QSM,{b:'((u8?|U)|L)?"'}),{b:'(u8?|U)?R"',e:'"',c:[e.BE]},{b:"'\\\\?.",e:"'",i:"."}]},a={cN:"number",v:[{b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},{b:e.CNR}],r:0},n={cN:"meta",b:"#",e:"$",k:{"meta-keyword":"if else elif endif define undef warning error line pragma ifdef ifndef"},c:[{b:/\\\n/,r:0},{bK:"include",e:"$",k:{"meta-keyword":"include"},c:[e.inherit(r,{cN:"meta-string"}),{cN:"meta-string",b:"<",e:">",i:"\\n"}]},r,e.CLCM,e.CBCM]},i=e.IR+"\\s*\\(",s={keyword:"int float while private char catch export virtual operator sizeof dynamic_cast|10 typedef const_cast|10 const struct for static_cast|10 union namespace unsigned long volatile static protected bool template mutable if public friend do goto auto void enum else break extern using class asm case typeid short reinterpret_cast|10 default double register explicit signed typename try this switch continue inline delete alignof constexpr decltype noexcept static_assert thread_local restrict _Bool complex _Complex _Imaginary atomic_bool atomic_char atomic_schar atomic_uchar atomic_short atomic_ushort atomic_int atomic_uint atomic_long atomic_ulong atomic_llong atomic_ullong",built_in:"std string cin cout cerr clog stdin stdout stderr stringstream istringstream ostringstream auto_ptr deque list queue stack vector map set bitset multiset multimap unordered_set unordered_map unordered_multiset unordered_multimap array shared_ptr abort abs acos asin atan2 atan calloc ceil cosh cos exit exp fabs floor fmod fprintf fputs free frexp fscanf isalnum isalpha iscntrl isdigit isgraph islower isprint ispunct isspace isupper isxdigit tolower toupper labs ldexp log10 log malloc realloc memchr memcmp memcpy memset modf pow printf putchar puts scanf sinh sin snprintf sprintf sqrt sscanf strcat strchr strcmp strcpy strcspn strlen strncat strncmp strncpy strpbrk strrchr strspn strstr tanh tan vfprintf vprintf vsprintf endl initializer_list unique_ptr",literal:"true false nullptr NULL"};return{aliases:["c","cc","h","c++","h++","hpp"],k:s,i:"</",c:[t,e.CLCM,e.CBCM,a,r,n,{b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:s,c:["self",t]},{b:e.IR+"::",k:s},{bK:"new throw return else",r:0},{cN:"function",b:"("+e.IR+"[\\*&\\s]+)+"+i,rB:!0,e:/[{;=]/,eE:!0,k:s,i:/[^\w\s\*&]/,c:[{b:i,rB:!0,c:[e.TM],r:0},{cN:"params",b:/\(/,e:/\)/,k:s,r:0,c:[e.CLCM,e.CBCM,r,a]},e.CLCM,e.CBCM,n]}]}}),e.registerLanguage("cs",function(e){var t="abstract as base bool break byte case catch char checked const continue decimal dynamic default delegate do double else enum event explicit extern false finally fixed float for foreach goto if implicit in int interface internal is lock long null when object operator out override params private protected public readonly ref sbyte sealed short sizeof stackalloc static string struct switch this true try typeof uint ulong unchecked unsafe ushort using virtual volatile void while async protected public private internal ascending descending from get group into join let orderby partial select set value var where yield",r=e.IR+"(<"+e.IR+">)?";return{aliases:["csharp"],k:t,i:/::/,c:[e.C("///","$",{rB:!0,c:[{cN:"doctag",v:[{b:"///",r:0},{b:"<!--|-->"},{b:"</?",e:">"}]}]}),e.CLCM,e.CBCM,{cN:"meta",b:"#",e:"$",k:{"meta-keyword":"if else elif endif define undef warning error line region endregion pragma checksum"}},{cN:"string",b:'@"',e:'"',c:[{b:'""'}]},e.ASM,e.QSM,e.CNM,{bK:"class interface",e:/[{;=]/,i:/[^\s:]/,c:[e.TM,e.CLCM,e.CBCM]},{bK:"namespace",e:/[{;=]/,i:/[^\s:]/,c:[e.inherit(e.TM,{b:"[a-zA-Z](\\.?\\w)*"}),e.CLCM,e.CBCM]},{bK:"new return throw await",r:0},{cN:"function",b:"("+r+"\\s+)+"+e.IR+"\\s*\\(",rB:!0,e:/[{;=]/,eE:!0,k:t,c:[{b:e.IR+"\\s*\\(",rB:!0,c:[e.TM],r:0},{cN:"params",b:/\(/,e:/\)/,eB:!0,eE:!0,k:t,r:0,c:[e.ASM,e.QSM,e.CNM,e.CBCM]},e.CLCM,e.CBCM]}]}}),e.registerLanguage("css",function(e){var t="[a-zA-Z-][a-zA-Z0-9_-]*",r={b:/[A-Z\_\.\-]+\s*:/,rB:!0,e:";",eW:!0,c:[{cN:"attribute",b:/\S/,e:":",eE:!0,starts:{eW:!0,eE:!0,c:[{b:/[\w-]+\s*\(/,rB:!0,c:[{cN:"built_in",b:/[\w-]+/}]},e.CSSNM,e.QSM,e.ASM,e.CBCM,{cN:"number",b:"#[0-9A-Fa-f]+"},{cN:"meta",b:"!important"}]}}]};return{cI:!0,i:/[=\/|'\$]/,c:[e.CBCM,{cN:"selector-id",b:/#[A-Za-z0-9_-]+/},{cN:"selector-class",b:/\.[A-Za-z0-9_-]+/},{cN:"selector-attr",b:/\[/,e:/\]/,i:"$"},{cN:"selector-pseudo",b:/:(:)?[a-zA-Z0-9\_\-\+\(\)"'.]+/},{b:"@(font-face|page)",l:"[a-z-]+",k:"font-face page"},{b:"@",e:"[{;]",c:[{cN:"keyword",b:/\S+/},{b:/\s/,eW:!0,eE:!0,r:0,c:[e.ASM,e.QSM,e.CSSNM]}]},{cN:"selector-tag",b:t,r:0},{b:"{",e:"}",i:/\S/,c:[e.CBCM,r]}]}}),e.registerLanguage("diff",function(e){return{aliases:["patch"],c:[{cN:"meta",r:10,v:[{b:/^@@ +\-\d+,\d+ +\+\d+,\d+ +@@$/},{b:/^\*\*\* +\d+,\d+ +\*\*\*\*$/},{b:/^\-\-\- +\d+,\d+ +\-\-\-\-$/}]},{cN:"comment",v:[{b:/Index: /,e:/$/},{b:/=====/,e:/=====$/},{b:/^\-\-\-/,e:/$/},{b:/^\*{3} /,e:/$/},{b:/^\+\+\+/,e:/$/},{b:/\*{5}/,e:/\*{5}$/}]},{cN:"addition",b:"^\\+",e:"$"},{cN:"deletion",b:"^\\-",e:"$"},{cN:"addition",b:"^\\!",e:"$"}]}}),e.registerLanguage("http",function(e){var t="HTTP/[0-9\\.]+";return{aliases:["https"],i:"\\S",c:[{b:"^"+t,e:"$",c:[{cN:"number",b:"\\b\\d{3}\\b"}]},{b:"^[A-Z]+ (.*?) "+t+"$",rB:!0,e:"$",c:[{cN:"string",b:" ",e:" ",eB:!0,eE:!0},{b:t},{cN:"keyword",b:"[A-Z]+"}]},{cN:"attribute",b:"^\\w",e:": ",eE:!0,i:"\\n|\\s|=",starts:{e:"$",r:0}},{b:"\\n\\n",starts:{sL:[],eW:!0}}]}}),e.registerLanguage("ini",function(e){var t={cN:"string",c:[e.BE],v:[{b:"'''",e:"'''",r:10},{b:'"""',e:'"""',r:10},{b:'"',e:'"'},{b:"'",e:"'"}]};return{aliases:["toml"],cI:!0,i:/\S/,c:[e.C(";","$"),e.HCM,{cN:"section",b:/^\s*\[+/,e:/\]+/},{b:/^[a-z0-9\[\]_-]+\s*=\s*/,e:"$",rB:!0,c:[{cN:"attr",b:/[a-z0-9\[\]_-]+/},{b:/=/,eW:!0,r:0,c:[{cN:"literal",b:/\bon|off|true|false|yes|no\b/},{cN:"variable",v:[{b:/\$[\w\d"][\w\d_]*/},{b:/\$\{(.*?)}/}]},t,{cN:"number",b:/([\+\-]+)?[\d]+_[\d_]+/},e.NM]}]}]}}),e.registerLanguage("java",function(e){var t=e.UIR+"(<"+e.UIR+"(\\s*,\\s*"+e.UIR+")*>)?",r="false synchronized int abstract float private char boolean static null if const for true while long strictfp finally protected import native final void enum else break transient catch instanceof byte super volatile case assert short package default double public try this switch continue throws protected public private",a="\\b(0[bB]([01]+[01_]+[01]+|[01]+)|0[xX]([a-fA-F0-9]+[a-fA-F0-9_]+[a-fA-F0-9]+|[a-fA-F0-9]+)|(([\\d]+[\\d_]+[\\d]+|[\\d]+)(\\.([\\d]+[\\d_]+[\\d]+|[\\d]+))?|\\.([\\d]+[\\d_]+[\\d]+|[\\d]+))([eE][-+]?\\d+)?)[lLfF]?",n={cN:"number",b:a,r:0};return{aliases:["jsp"],k:r,i:/<\/|#/,c:[e.C("/\\*\\*","\\*/",{r:0,c:[{b:/\w+@/,r:0},{cN:"doctag",b:"@[A-Za-z]+"}]}),e.CLCM,e.CBCM,e.ASM,e.QSM,{cN:"class",bK:"class interface",e:/[{;=]/,eE:!0,k:"class interface",i:/[:"\[\]]/,c:[{bK:"extends implements"},e.UTM]},{bK:"new throw return else",r:0},{cN:"function",b:"("+t+"\\s+)+"+e.UIR+"\\s*\\(",rB:!0,e:/[{;=]/,eE:!0,k:r,c:[{b:e.UIR+"\\s*\\(",rB:!0,r:0,c:[e.UTM]},{cN:"params",b:/\(/,e:/\)/,k:r,r:0,c:[e.ASM,e.QSM,e.CNM,e.CBCM]},e.CLCM,e.CBCM]},n,{cN:"meta",b:"@[A-Za-z]+"}]}}),e.registerLanguage("javascript",function(e){return{aliases:["js"],k:{keyword:"in of if for while finally var new function do return void else break catch instanceof with throw case default try this switch continue typeof delete let yield const export super debugger as async await import from as",literal:"true false null undefined NaN Infinity",built_in:"eval isFinite isNaN parseFloat parseInt decodeURI decodeURIComponent encodeURI encodeURIComponent escape unescape Object Function Boolean Error EvalError InternalError RangeError ReferenceError StopIteration SyntaxError TypeError URIError Number Math Date String RegExp Array Float32Array Float64Array Int16Array Int32Array Int8Array Uint16Array Uint32Array Uint8Array Uint8ClampedArray ArrayBuffer DataView JSON Intl arguments require module console window document Symbol Set Map WeakSet WeakMap Proxy Reflect Promise"},c:[{cN:"meta",r:10,b:/^\s*['"]use (strict|asm)['"]/},{cN:"meta",b:/^#!/,e:/$/},e.ASM,e.QSM,{cN:"string",b:"`",e:"`",c:[e.BE,{cN:"subst",b:"\\$\\{",e:"\\}"}]},e.CLCM,e.CBCM,{cN:"number",v:[{b:"\\b(0[bB][01]+)"},{b:"\\b(0[oO][0-7]+)"},{b:e.CNR}],r:0},{b:"("+e.RSR+"|\\b(case|return|throw)\\b)\\s*",k:"return throw case",c:[e.CLCM,e.CBCM,e.RM,{b:/</,e:/>\s*[);\]]/,r:0,sL:"xml"}],r:0},{cN:"function",bK:"function",e:/\{/,eE:!0,c:[e.inherit(e.TM,{b:/[A-Za-z$_][0-9A-Za-z$_]*/}),{cN:"params",b:/\(/,e:/\)/,eB:!0,eE:!0,c:[e.CLCM,e.CBCM]}],i:/\[|%/},{b:/\$[(.]/},{b:"\\."+e.IR,r:0},{cN:"class",bK:"class",e:/[{;=]/,eE:!0,i:/[:"\[\]]/,c:[{bK:"extends"},e.UTM]},{bK:"constructor",e:/\{/,eE:!0}],i:/#(?!!)/}}),e.registerLanguage("json",function(e){var t={literal:"true false null"},r=[e.QSM,e.CNM],a={e:",",eW:!0,eE:!0,c:r,k:t},n={b:"{",e:"}",c:[{cN:"attr",b:'\\s*"',e:'"\\s*:\\s*',eB:!0,eE:!0,c:[e.BE],i:"\\n",starts:a}],i:"\\S"},i={b:"\\[",e:"\\]",c:[e.inherit(a)],i:"\\S"};return r.splice(r.length,0,n,i),{c:r,k:t,i:"\\S"}}),e.registerLanguage("makefile",function(e){var t={cN:"variable",b:/\$\(/,e:/\)/,c:[e.BE]};return{aliases:["mk","mak"],c:[e.HCM,{b:/^\w+\s*\W*=/,rB:!0,r:0,starts:{e:/\s*\W*=/,eE:!0,starts:{e:/$/,r:0,c:[t]}}},{cN:"section",b:/^[\w]+:\s*$/},{cN:"meta",b:/^\.PHONY:/,e:/$/,k:{"meta-keyword":".PHONY"},l:/[\.\w]+/},{b:/^\t+/,e:/$/,r:0,c:[e.QSM,t]}]}}),e.registerLanguage("xml",function(e){var t="[A-Za-z0-9\\._:-]+",r={b:/<\?(php)?(?!\w)/,e:/\?>/,sL:"php"},a={eW:!0,i:/</,r:0,c:[r,{cN:"attr",b:t,r:0},{b:"=",r:0,c:[{cN:"string",c:[r],v:[{b:/"/,e:/"/},{b:/'/,e:/'/},{b:/[^\s\/>]+/}]}]}]};return{aliases:["html","xhtml","rss","atom","xsl","plist"],cI:!0,c:[{cN:"meta",b:"<!DOCTYPE",e:">",r:10,c:[{b:"\\[",e:"\\]"}]},e.C("<!--","-->",{r:10}),{b:"<\\!\\[CDATA\\[",e:"\\]\\]>",r:10},{cN:"tag",b:"<style(?=\\s|>|$)",e:">",k:{name:"style"},c:[a],starts:{e:"</style>",rE:!0,sL:["css","xml"]}},{cN:"tag",b:"<script(?=\\s|>|$)",e:">",k:{name:"script"},c:[a],starts:{e:"</script>",rE:!0,sL:["actionscript","javascript","handlebars","xml"]}},r,{cN:"meta",b:/<\?\w+/,e:/\?>/,r:10},{cN:"tag",b:"</?",e:"/?>",c:[{cN:"name",b:/[^\/><\s]+/,r:0},a]}]}}),e.registerLanguage("markdown",function(e){return{aliases:["md","mkdown","mkd"],c:[{cN:"section",v:[{b:"^#{1,6}",e:"$"},{b:"^.+?\\n[=-]{2,}$"}]},{b:"<",e:">",sL:"xml",r:0},{cN:"bullet",b:"^([*+-]|(\\d+\\.))\\s+"},{cN:"strong",b:"[*_]{2}.+?[*_]{2}"},{cN:"emphasis",v:[{b:"\\*.+?\\*"},{b:"_.+?_",r:0}]},{cN:"quote",b:"^>\\s+",e:"$"},{cN:"code",v:[{b:"`.+?`"},{b:"^( {4}|	)",e:"$",r:0}]},{b:"^[-\\*]{3,}",e:"$"},{b:"\\[.+?\\][\\(\\[].*?[\\)\\]]",rB:!0,c:[{cN:"string",b:"\\[",e:"\\]",eB:!0,rE:!0,r:0},{cN:"link",b:"\\]\\(",e:"\\)",eB:!0,eE:!0},{cN:"symbol",b:"\\]\\[",e:"\\]",eB:!0,eE:!0}],r:10},{b:"^\\[.+\\]:",rB:!0,c:[{cN:"symbol",b:"\\[",e:"\\]:",eB:!0,eE:!0,starts:{cN:"link",e:"$"}}]}]}}),e.registerLanguage("nginx",function(e){var t={cN:"variable",v:[{b:/\$\d+/},{b:/\$\{/,e:/}/},{b:"[\\$\\@]"+e.UIR}]},r={eW:!0,l:"[a-z/_]+",k:{literal:"on off yes no true false none blocked debug info notice warn error crit select break last permanent redirect kqueue rtsig epoll poll /dev/poll"},r:0,i:"=>",c:[e.HCM,{cN:"string",c:[e.BE,t],v:[{b:/"/,e:/"/},{b:/'/,e:/'/}]},{b:"([a-z]+):/",e:"\\s",eW:!0,eE:!0,c:[t]},{cN:"regexp",c:[e.BE,t],v:[{b:"\\s\\^",e:"\\s|{|;",rE:!0},{b:"~\\*?\\s+",e:"\\s|{|;",rE:!0},{b:"\\*(\\.[a-z\\-]+)+"},{b:"([a-z\\-]+\\.)+\\*"}]},{cN:"number",b:"\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}(:\\d{1,5})?\\b"},{cN:"number",b:"\\b\\d+[kKmMgGdshdwy]*\\b",r:0},t]};return{aliases:["nginxconf"],c:[e.HCM,{b:e.UIR+"\\s+{",rB:!0,e:"{",c:[{cN:"section",b:e.UIR}],r:0},{b:e.UIR+"\\s",e:";|{",rB:!0,c:[{cN:"attribute",b:e.UIR,starts:r}],r:0}],i:"[^\\s\\}]"}}),e.registerLanguage("objectivec",function(e){var t={cN:"built_in",b:"(AV|CA|CF|CG|CI|MK|MP|NS|UI|XC)\\w+"},r={keyword:"int float while char export sizeof typedef const struct for union unsigned long volatile static bool mutable if do return goto void enum else break extern asm case short default double register explicit signed typename this switch continue wchar_t inline readonly assign readwrite self @synchronized id typeof nonatomic super unichar IBOutlet IBAction strong weak copy in out inout bycopy byref oneway __strong __weak __block __autoreleasing @private @protected @public @try @property @end @throw @catch @finally @autoreleasepool @synthesize @dynamic @selector @optional @required",literal:"false true FALSE TRUE nil YES NO NULL",built_in:"BOOL dispatch_once_t dispatch_queue_t dispatch_sync dispatch_async dispatch_once"},a=/[a-zA-Z@][a-zA-Z0-9_]*/,n="@interface @class @protocol @implementation";return{aliases:["mm","objc","obj-c"],k:r,l:a,i:"</",c:[t,e.CLCM,e.CBCM,e.CNM,e.QSM,{cN:"string",v:[{b:'@"',e:'"',i:"\\n",c:[e.BE]},{b:"'",e:"[^\\\\]'",i:"[^\\\\][^']"}]},{cN:"meta",b:"#",e:"$",c:[{cN:"meta-string",v:[{b:'"',e:'"'},{b:"<",e:">"}]}]},{cN:"class",b:"("+n.split(" ").join("|")+")\\b",e:"({|$)",eE:!0,k:n,l:a,c:[e.UTM]},{b:"\\."+e.UIR,r:0}]}}),e.registerLanguage("perl",function(e){var t="getpwent getservent quotemeta msgrcv scalar kill dbmclose undef lc ma syswrite tr send umask sysopen shmwrite vec qx utime local oct semctl localtime readpipe do return format read sprintf dbmopen pop getpgrp not getpwnam rewinddir qqfileno qw endprotoent wait sethostent bless s|0 opendir continue each sleep endgrent shutdown dump chomp connect getsockname die socketpair close flock exists index shmgetsub for endpwent redo lstat msgctl setpgrp abs exit select print ref gethostbyaddr unshift fcntl syscall goto getnetbyaddr join gmtime symlink semget splice x|0 getpeername recv log setsockopt cos last reverse gethostbyname getgrnam study formline endhostent times chop length gethostent getnetent pack getprotoent getservbyname rand mkdir pos chmod y|0 substr endnetent printf next open msgsnd readdir use unlink getsockopt getpriority rindex wantarray hex system getservbyport endservent int chr untie rmdir prototype tell listen fork shmread ucfirst setprotoent else sysseek link getgrgid shmctl waitpid unpack getnetbyname reset chdir grep split require caller lcfirst until warn while values shift telldir getpwuid my getprotobynumber delete and sort uc defined srand accept package seekdir getprotobyname semop our rename seek if q|0 chroot sysread setpwent no crypt getc chown sqrt write setnetent setpriority foreach tie sin msgget map stat getlogin unless elsif truncate exec keys glob tied closedirioctl socket readlink eval xor readline binmode setservent eof ord bind alarm pipe atan2 getgrent exp time push setgrent gt lt or ne m|0 break given say state when",r={cN:"subst",b:"[$@]\\{",e:"\\}",k:t},a={b:"->{",e:"}"},n={v:[{b:/\$\d/},{b:/[\$%@](\^\w\b|#\w+(::\w+)*|{\w+}|\w+(::\w*)*)/},{b:/[\$%@][^\s\w{]/,r:0}]},i=[e.BE,r,n],s=[n,e.HCM,e.C("^\\=\\w","\\=cut",{eW:!0}),a,{cN:"string",c:i,v:[{b:"q[qwxr]?\\s*\\(",e:"\\)",r:5},{b:"q[qwxr]?\\s*\\[",e:"\\]",r:5},{b:"q[qwxr]?\\s*\\{",e:"\\}",r:5},{b:"q[qwxr]?\\s*\\|",e:"\\|",r:5},{b:"q[qwxr]?\\s*\\<",e:"\\>",r:5},{b:"qw\\s+q",e:"q",r:5},{b:"'",e:"'",c:[e.BE]},{b:'"',e:'"'},{b:"`",e:"`",c:[e.BE]},{b:"{\\w+}",c:[],r:0},{b:"-?\\w+\\s*\\=\\>",c:[],r:0}]},{cN:"number",b:"(\\b0[0-7_]+)|(\\b0x[0-9a-fA-F_]+)|(\\b[1-9][0-9_]*(\\.[0-9_]+)?)|[0_]\\b",r:0},{b:"(\\/\\/|"+e.RSR+"|\\b(split|return|print|reverse|grep)\\b)\\s*",k:"split return print reverse grep",r:0,c:[e.HCM,{cN:"regexp",b:"(s|tr|y)/(\\\\.|[^/])*/(\\\\.|[^/])*/[a-z]*",r:10},{cN:"regexp",b:"(m|qr)?/",e:"/[a-z]*",c:[e.BE],r:0}]},{cN:"function",bK:"sub",e:"(\\s*\\(.*?\\))?[;{]",eE:!0,r:5,c:[e.TM]},{b:"-\\w\\b",r:0},{b:"^__DATA__$",e:"^__END__$",sL:"mojolicious",c:[{b:"^@@.*",e:"$",cN:"comment"}]}];return r.c=s,a.c=s,{aliases:["pl"],k:t,c:s}}),e.registerLanguage("php",function(e){var t={b:"\\$+[a-zA-Z_-ÿ][a-zA-Z0-9_-ÿ]*"},r={cN:"meta",b:/<\?(php)?|\?>/},a={cN:"string",c:[e.BE,r],v:[{b:'b"',e:'"'},{b:"b'",e:"'"},e.inherit(e.ASM,{i:null}),e.inherit(e.QSM,{i:null})]},n={v:[e.BNM,e.CNM]};return{aliases:["php3","php4","php5","php6"],cI:!0,k:"and include_once list abstract global private echo interface as static endswitch array null if endwhile or const for endforeach self var while isset public protected exit foreach throw elseif include __FILE__ empty require_once do xor return parent clone use __CLASS__ __LINE__ else break print eval new catch __METHOD__ case exception default die require __FUNCTION__ enddeclare final try switch continue endfor endif declare unset true false trait goto instanceof insteadof __DIR__ __NAMESPACE__ yield finally",c:[e.CLCM,e.HCM,e.C("/\\*","\\*/",{c:[{cN:"doctag",b:"@[A-Za-z]+"},r]}),e.C("__halt_compiler.+?;",!1,{eW:!0,k:"__halt_compiler",l:e.UIR}),{cN:"string",b:/<<<['"]?\w+['"]?$/,e:/^\w+;?$/,c:[e.BE,{cN:"subst",v:[{b:/\$\w+/},{b:/\{\$/,e:/\}/}]}]},r,t,{b:/(::|->)+[a-zA-Z_\x7f-\xff][a-zA-Z0-9_\x7f-\xff]*/},{cN:"function",bK:"function",e:/[;{]/,eE:!0,i:"\\$|\\[|%",c:[e.UTM,{cN:"params",b:"\\(",e:"\\)",c:["self",t,e.CBCM,a,n]}]},{cN:"class",bK:"class interface",e:"{",eE:!0,i:/[:\(\$"]/,c:[{bK:"extends implements"},e.UTM]},{bK:"namespace",e:";",i:/[\.']/,c:[e.UTM]},{bK:"use",e:";",c:[e.UTM]},{b:"=>"},a,n]}}),e.registerLanguage("python",function(e){var t={cN:"meta",b:/^(>>>|\.\.\.) /},r={cN:"string",c:[e.BE],v:[{b:/(u|b)?r?'''/,e:/'''/,c:[t],r:10},{b:/(u|b)?r?"""/,e:/"""/,c:[t],r:10},{b:/(u|r|ur)'/,e:/'/,r:10},{b:/(u|r|ur)"/,e:/"/,r:10},{b:/(b|br)'/,e:/'/},{b:/(b|br)"/,e:/"/},e.ASM,e.QSM]},a={cN:"number",r:0,v:[{b:e.BNR+"[lLjJ]?"},{b:"\\b(0o[0-7]+)[lLjJ]?"},{b:e.CNR+"[lLjJ]?"}]},n={cN:"params",b:/\(/,e:/\)/,c:["self",t,a,r]};return{aliases:["py","gyp"],k:{keyword:"and elif is global as in if from raise for except finally print import pass return exec else break not with class assert yield try while continue del or def lambda async await nonlocal|10 None True False",built_in:"Ellipsis NotImplemented"},i:/(<\/|->|\?)/,c:[t,a,r,e.HCM,{v:[{cN:"function",bK:"def",r:10},{cN:"class",bK:"class"}],e:/:/,i:/[${=;\n,]/,c:[e.UTM,n,{b:/->/,eW:!0,k:"None"}]},{cN:"meta",b:/^[\t ]*@/,e:/$/},{b:/\b(print|exec)\(/}]}}),e.registerLanguage("ruby",function(e){var t="[a-zA-Z_]\\w*[!?=]?|[-+~]\\@|<<|>>|=~|===?|<=>|[<>]=?|\\*\\*|[-/+%^&*~`|]|\\[\\]=?",r="and false then defined module in return redo if BEGIN retry end for true self when next until do begin unless END rescue nil else break undef not super class case require yield alias while ensure elsif or include attr_reader attr_writer attr_accessor",a={cN:"doctag",b:"@[A-Za-z]+"},n={b:"#<",e:">"},i=[e.C("#","$",{c:[a]}),e.C("^\\=begin","^\\=end",{c:[a],r:10}),e.C("^__END__","\\n$")],s={cN:"subst",b:"#\\{",e:"}",k:r},c={cN:"string",c:[e.BE,s],v:[{b:/'/,e:/'/},{b:/"/,e:/"/},{b:/`/,e:/`/},{b:"%[qQwWx]?\\(",e:"\\)"},{b:"%[qQwWx]?\\[",e:"\\]"},{b:"%[qQwWx]?{",e:"}"},{b:"%[qQwWx]?<",e:">"},{b:"%[qQwWx]?/",e:"/"},{b:"%[qQwWx]?%",e:"%"},{b:"%[qQwWx]?-",e:"-"},{b:"%[qQwWx]?\\|",e:"\\|"},{b:/\B\?(\\\d{1,3}|\\x[A-Fa-f0-9]{1,2}|\\u[A-Fa-f0-9]{4}|\\?\S)\b/}]},o={cN:"params",b:"\\(",e:"\\)",endsParent:!0,k:r},l=[c,n,{cN:"class",bK:"class module",e:"$|;",i:/=/,c:[e.inherit(e.TM,{b:"[A-Za-z_]\\w*(::\\w+)*(\\?|\\!)?"}),{b:"<\\s*",c:[{b:"("+e.IR+"::)?"+e.IR}]}].concat(i)},{cN:"function",bK:"def",e:"$|;",c:[e.inherit(e.TM,{b:t}),o].concat(i)},{cN:"symbol",b:e.UIR+"(\\!|\\?)?:",r:0},{cN:"symbol",b:":",c:[c,{b:t}],r:0},{cN:"number",b:"(\\b0[0-7_]+)|(\\b0x[0-9a-fA-F_]+)|(\\b[1-9][0-9_]*(\\.[0-9_]+)?)|[0_]\\b",r:0},{b:"(\\$\\W)|((\\$|\\@\\@?)(\\w+))"},{b:"("+e.RSR+")\\s*",c:[n,{cN:"regexp",c:[e.BE,s],i:/\n/,v:[{b:"/",e:"/[a-z]*"},{b:"%r{",e:"}[a-z]*"},{b:"%r\\(",e:"\\)[a-z]*"},{b:"%r!",e:"![a-z]*"},{b:"%r\\[",e:"\\][a-z]*"}]}].concat(i),r:0}].concat(i);s.c=l,o.c=l;var u="[>?]>",d="[\\w#]+\\(\\w+\\):\\d+:\\d+>",b="(\\w+-)?\\d+\\.\\d+\\.\\d(p\\d+)?[^>]+>",p=[{b:/^\s*=>/,starts:{e:"$",c:l}},{cN:"meta",b:"^("+u+"|"+d+"|"+b+")",starts:{e:"$",c:l}}];return{aliases:["rb","gemspec","podspec","thor","irb"],k:r,i:/\/\*/,c:i.concat(p).concat(l)}}),e.registerLanguage("sql",function(e){var t=e.C("--","$");return{cI:!0,i:/[<>{}*]/,c:[{bK:"begin end start commit rollback savepoint lock alter create drop rename call delete do handler insert load replace select truncate update set show pragma grant merge describe use explain help declare prepare execute deallocate release unlock purge reset change stop analyze cache flush optimize repair kill install uninstall checksum restore check backup revoke",e:/;/,eW:!0,k:{keyword:"abort abs absolute acc acce accep accept access accessed accessible account acos action activate add addtime admin administer advanced advise aes_decrypt aes_encrypt after agent aggregate ali alia alias allocate allow alter always analyze ancillary and any anydata anydataset anyschema anytype apply archive archived archivelog are as asc ascii asin assembly assertion associate asynchronous at atan atn2 attr attri attrib attribu attribut attribute attributes audit authenticated authentication authid authors auto autoallocate autodblink autoextend automatic availability avg backup badfile basicfile before begin beginning benchmark between bfile bfile_base big bigfile bin binary_double binary_float binlog bit_and bit_count bit_length bit_or bit_xor bitmap blob_base block blocksize body both bound buffer_cache buffer_pool build bulk by byte byteordermark bytes c cache caching call calling cancel capacity cascade cascaded case cast catalog category ceil ceiling chain change changed char_base char_length character_length characters characterset charindex charset charsetform charsetid check checksum checksum_agg child choose chr chunk class cleanup clear client clob clob_base clone close cluster_id cluster_probability cluster_set clustering coalesce coercibility col collate collation collect colu colum column column_value columns columns_updated comment commit compact compatibility compiled complete composite_limit compound compress compute concat concat_ws concurrent confirm conn connec connect connect_by_iscycle connect_by_isleaf connect_by_root connect_time connection consider consistent constant constraint constraints constructor container content contents context contributors controlfile conv convert convert_tz corr corr_k corr_s corresponding corruption cos cost count count_big counted covar_pop covar_samp cpu_per_call cpu_per_session crc32 create creation critical cross cube cume_dist curdate current current_date current_time current_timestamp current_user cursor curtime customdatum cycle d data database databases datafile datafiles datalength date_add date_cache date_format date_sub dateadd datediff datefromparts datename datepart datetime2fromparts day day_to_second dayname dayofmonth dayofweek dayofyear days db_role_change dbtimezone ddl deallocate declare decode decompose decrement decrypt deduplicate def defa defau defaul default defaults deferred defi defin define degrees delayed delegate delete delete_all delimited demand dense_rank depth dequeue des_decrypt des_encrypt des_key_file desc descr descri describ describe descriptor deterministic diagnostics difference dimension direct_load directory disable disable_all disallow disassociate discardfile disconnect diskgroup distinct distinctrow distribute distributed div do document domain dotnet double downgrade drop dumpfile duplicate duration e each edition editionable editions element ellipsis else elsif elt empty enable enable_all enclosed encode encoding encrypt end end-exec endian enforced engine engines enqueue enterprise entityescaping eomonth error errors escaped evalname evaluate event eventdata events except exception exceptions exchange exclude excluding execu execut execute exempt exists exit exp expire explain export export_set extended extent external external_1 external_2 externally extract f failed failed_login_attempts failover failure far fast feature_set feature_value fetch field fields file file_name_convert filesystem_like_logging final finish first first_value fixed flash_cache flashback floor flush following follows for forall force form forma format found found_rows freelist freelists freepools fresh from from_base64 from_days ftp full function g general generated get get_format get_lock getdate getutcdate global global_name globally go goto grant grants greatest group group_concat group_id grouping grouping_id groups gtid_subtract guarantee guard handler hash hashkeys having hea head headi headin heading heap help hex hierarchy high high_priority hosts hour http i id ident_current ident_incr ident_seed identified identity idle_time if ifnull ignore iif ilike ilm immediate import in include including increment index indexes indexing indextype indicator indices inet6_aton inet6_ntoa inet_aton inet_ntoa infile initial initialized initially initrans inmemory inner innodb input insert install instance instantiable instr interface interleaved intersect into invalidate invisible is is_free_lock is_ipv4 is_ipv4_compat is_not is_not_null is_used_lock isdate isnull isolation iterate java join json json_exists k keep keep_duplicates key keys kill l language large last last_day last_insert_id last_value lax lcase lead leading least leaves left len lenght length less level levels library like like2 like4 likec limit lines link list listagg little ln load load_file lob lobs local localtime localtimestamp locate locator lock locked log log10 log2 logfile logfiles logging logical logical_reads_per_call logoff logon logs long loop low low_priority lower lpad lrtrim ltrim m main make_set makedate maketime managed management manual map mapping mask master master_pos_wait match matched materialized max maxextents maximize maxinstances maxlen maxlogfiles maxloghistory maxlogmembers maxsize maxtrans md5 measures median medium member memcompress memory merge microsecond mid migration min minextents minimum mining minus minute minvalue missing mod mode model modification modify module monitoring month months mount move movement multiset mutex n name name_const names nan national native natural nav nchar nclob nested never new newline next nextval no no_write_to_binlog noarchivelog noaudit nobadfile nocheck nocompress nocopy nocycle nodelay nodiscardfile noentityescaping noguarantee nokeep nologfile nomapping nomaxvalue nominimize nominvalue nomonitoring none noneditionable nonschema noorder nopr nopro noprom nopromp noprompt norely noresetlogs noreverse normal norowdependencies noschemacheck noswitch not nothing notice notrim novalidate now nowait nth_value nullif nulls num numb numbe nvarchar nvarchar2 object ocicoll ocidate ocidatetime ociduration ociinterval ociloblocator ocinumber ociref ocirefcursor ocirowid ocistring ocitype oct octet_length of off offline offset oid oidindex old on online only opaque open operations operator optimal optimize option optionally or oracle oracle_date oradata ord ordaudio orddicom orddoc order ordimage ordinality ordvideo organization orlany orlvary out outer outfile outline output over overflow overriding p package pad parallel parallel_enable parameters parent parse partial partition partitions pascal passing password password_grace_time password_lock_time password_reuse_max password_reuse_time password_verify_function patch path patindex pctincrease pctthreshold pctused pctversion percent percent_rank percentile_cont percentile_disc performance period period_add period_diff permanent physical pi pipe pipelined pivot pluggable plugin policy position post_transaction pow power pragma prebuilt precedes preceding precision prediction prediction_cost prediction_details prediction_probability prediction_set prepare present preserve prior priority private private_sga privileges procedural procedure procedure_analyze processlist profiles project prompt protection public publishingservername purge quarter query quick quiesce quota quotename radians raise rand range rank raw read reads readsize rebuild record records recover recovery recursive recycle redo reduced ref reference referenced references referencing refresh regexp_like register regr_avgx regr_avgy regr_count regr_intercept regr_r2 regr_slope regr_sxx regr_sxy reject rekey relational relative relaylog release release_lock relies_on relocate rely rem remainder rename repair repeat replace replicate replication required reset resetlogs resize resource respect restore restricted result result_cache resumable resume retention return returning returns reuse reverse revoke right rlike role roles rollback rolling rollup round row row_count rowdependencies rowid rownum rows rtrim rules safe salt sample save savepoint sb1 sb2 sb4 scan schema schemacheck scn scope scroll sdo_georaster sdo_topo_geometry search sec_to_time second section securefile security seed segment select self sequence sequential serializable server servererror session session_user sessions_per_user set sets settings sha sha1 sha2 share shared shared_pool short show shrink shutdown si_averagecolor si_colorhistogram si_featurelist si_positionalcolor si_stillimage si_texture siblings sid sign sin size size_t sizes skip slave sleep smalldatetimefromparts smallfile snapshot some soname sort soundex source space sparse spfile split sql sql_big_result sql_buffer_result sql_cache sql_calc_found_rows sql_small_result sql_variant_property sqlcode sqldata sqlerror sqlname sqlstate sqrt square standalone standby start starting startup statement static statistics stats_binomial_test stats_crosstab stats_ks_test stats_mode stats_mw_test stats_one_way_anova stats_t_test_ stats_t_test_indep stats_t_test_one stats_t_test_paired stats_wsr_test status std stddev stddev_pop stddev_samp stdev stop storage store stored str str_to_date straight_join strcmp strict string struct stuff style subdate subpartition subpartitions substitutable substr substring subtime subtring_index subtype success sum suspend switch switchoffset switchover sync synchronous synonym sys sys_xmlagg sysasm sysaux sysdate sysdatetimeoffset sysdba sysoper system system_user sysutcdatetime t table tables tablespace tan tdo template temporary terminated tertiary_weights test than then thread through tier ties time time_format time_zone timediff timefromparts timeout timestamp timestampadd timestampdiff timezone_abbr timezone_minute timezone_region to to_base64 to_date to_days to_seconds todatetimeoffset trace tracking transaction transactional translate translation treat trigger trigger_nestlevel triggers trim truncate try_cast try_convert try_parse type ub1 ub2 ub4 ucase unarchived unbounded uncompress under undo unhex unicode uniform uninstall union unique unix_timestamp unknown unlimited unlock unpivot unrecoverable unsafe unsigned until untrusted unusable unused update updated upgrade upped upper upsert url urowid usable usage use use_stored_outlines user user_data user_resources users using utc_date utc_timestamp uuid uuid_short validate validate_password_strength validation valist value values var var_samp varcharc vari varia variab variabl variable variables variance varp varraw varrawc varray verify version versions view virtual visible void wait wallet warning warnings week weekday weekofyear wellformed when whene whenev wheneve whenever where while whitespace with within without work wrapped xdb xml xmlagg xmlattributes xmlcast xmlcolattval xmlelement xmlexists xmlforest xmlindex xmlnamespaces xmlpi xmlquery xmlroot xmlschema xmlserialize xmltable xmltype xor year year_to_month years yearweek",
literal:"true false null",built_in:"array bigint binary bit blob boolean char character date dec decimal float int int8 integer interval number numeric real record serial serial8 smallint text varchar varying void"},c:[{cN:"string",b:"'",e:"'",c:[e.BE,{b:"''"}]},{cN:"string",b:'"',e:'"',c:[e.BE,{b:'""'}]},{cN:"string",b:"`",e:"`",c:[e.BE]},e.CNM,e.CBCM,t]},e.CBCM,t]}}),e});" type="text/javascript"></script>
  <script src="data:application/javascript; charset=utf-8;base64,aGxqcy5yZWdpc3Rlckxhbmd1YWdlKCJzdGFuIixmdW5jdGlvbihlKXtyZXR1cm57YzpbZS5IQ00sZS5DTENNLGUuQ0JDTSx7YjplLlVJUixsOmUuVUlSLGs6e25hbWU6ImZvciBpbiB3aGlsZSByZXBlYXQgdW50aWwgaWYgdGhlbiBlbHNlIixzeW1ib2w6ImJlcm5vdWxsaSBiZXJub3VsbGlfbG9naXQgYmlub21pYWwgYmlub21pYWxfbG9naXQgYmV0YV9iaW5vbWlhbCBoeXBlcmdlb21ldHJpYyBjYXRlZ29yaWNhbCBjYXRlZ29yaWNhbF9sb2dpdCBvcmRlcmVkX2xvZ2lzdGljIG5lZ19iaW5vbWlhbCBuZWdfYmlub21pYWxfMiBuZWdfYmlub21pYWxfMl9sb2cgcG9pc3NvbiBwb2lzc29uX2xvZyBtdWx0aW5vbWlhbCBub3JtYWwgZXhwX21vZF9ub3JtYWwgc2tld19ub3JtYWwgc3R1ZGVudF90IGNhdWNoeSBkb3VibGVfZXhwb25lbnRpYWwgbG9naXN0aWMgZ3VtYmVsIGxvZ25vcm1hbCBjaGlfc3F1YXJlIGludl9jaGlfc3F1YXJlIHNjYWxlZF9pbnZfY2hpX3NxdWFyZSBleHBvbmVudGlhbCBpbnZfZ2FtbWEgd2VpYnVsbCBmcmVjaGV0IHJheWxlaWdoIHdpZW5lciBwYXJldG8gcGFyZXRvX3R5cGVfMiB2b25fbWlzZXMgdW5pZm9ybSBtdWx0aV9ub3JtYWwgbXVsdGlfbm9ybWFsX3ByZWMgbXVsdGlfbm9ybWFsX2Nob2xlc2t5IG11bHRpX2dwIG11bHRpX2dwX2Nob2xlc2t5IG11bHRpX3N0dWRlbnRfdCBnYXVzc2lhbl9kbG1fb2JzIGRpcmljaGxldCBsa2pfY29yciBsa2pfY29ycl9jaG9sZXNreSB3aXNoYXJ0IGludl93aXNoYXJ0Iiwic2VsZWN0b3ItdGFnIjoiaW50IHJlYWwgdmVjdG9yIHNpbXBsZXggdW5pdF92ZWN0b3Igb3JkZXJlZCBwb3NpdGl2ZV9vcmRlcmVkIHJvd192ZWN0b3IgbWF0cml4IGNob2xlc2t5X2ZhY3Rvcl9jb3JyIGNob2xlc2t5X2ZhY3Rvcl9jb3YgY29ycl9tYXRyaXggY292X21hdHJpeCIsdGl0bGU6ImZ1bmN0aW9ucyBtb2RlbCBkYXRhIHBhcmFtZXRlcnMgcXVhbnRpdGllcyB0cmFuc2Zvcm1lZCBnZW5lcmF0ZWQiLGxpdGVyYWw6InRydWUgZmFsc2UifSxyOjB9LHtjTjoibnVtYmVyIixiOiIwW3hYXVswLTlhLWZBLUZdK1tMaV0/XFxiIixyOjB9LHtjTjoibnVtYmVyIixiOiIwW3hYXVswLTlhLWZBLUZdK1tMaV0/XFxiIixyOjB9LHtjTjoibnVtYmVyIixiOiJcXGQrKD86W2VFXVsrXFwtXT9cXGQqKT9MXFxiIixyOjB9LHtjTjoibnVtYmVyIixiOiJcXGQrXFwuKD8hXFxkKSg/OmlcXGIpPyIscjowfSx7Y046Im51bWJlciIsYjoiXFxkKyg/OlxcLlxcZCopPyg/OltlRV1bK1xcLV0/XFxkKik/aT9cXGIiLHI6MH0se2NOOiJudW1iZXIiLGI6IlxcLlxcZCsoPzpbZUVdWytcXC1dP1xcZCopP2k/XFxiIixyOjB9XX19KTs=" type="text/javascript"></script>
  <script type="text/javascript">
    hljs.initHighlightingOnLoad();
  </script>
  <script src="data:application/javascript;base64,/*
 *  /MathJax.js
 *
 *  Copyright (c) 2009-2016 The MathJax Consortium
 *
 *  Licensed under the Apache License, Version 2.0 (the "License");
 *  you may not use this file except in compliance with the License.
 *  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 *  Unless required by applicable law or agreed to in writing, software
 *  distributed under the License is distributed on an "AS IS" BASIS,
 *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *  See the License for the specific language governing permissions and
 *  limitations under the License.
 */

if(document.getElementById&&document.childNodes&&document.createElement){if(!(window.MathJax&&MathJax.Hub)){if(window.MathJax){window.MathJax={AuthorConfig:window.MathJax}}else{window.MathJax={}}MathJax.isPacked=true;MathJax.version="2.7.0";MathJax.fileversion="2.7.0";MathJax.cdnVersion="2.7.0";MathJax.cdnFileVersions={};(function(d){var b=window[d];if(!b){b=window[d]={}}var e=[];var c=function(f){var g=f.constructor;if(!g){g=function(){}}for(var h in f){if(h!=="constructor"&&f.hasOwnProperty(h)){g[h]=f[h]}}return g};var a=function(){return function(){return arguments.callee.Init.call(this,arguments)}};b.Object=c({constructor:a(),Subclass:function(f,h){var g=a();g.SUPER=this;g.Init=this.Init;g.Subclass=this.Subclass;g.Augment=this.Augment;g.protoFunction=this.protoFunction;g.can=this.can;g.has=this.has;g.isa=this.isa;g.prototype=new this(e);g.prototype.constructor=g;g.Augment(f,h);return g},Init:function(f){var g=this;if(f.length===1&&f[0]===e){return g}if(!(g instanceof f.callee)){g=new f.callee(e)}return g.Init.apply(g,f)||g},Augment:function(f,g){var h;if(f!=null){for(h in f){if(f.hasOwnProperty(h)){this.protoFunction(h,f[h])}}if(f.toString!==this.prototype.toString&&f.toString!=={}.toString){this.protoFunction("toString",f.toString)}}if(g!=null){for(h in g){if(g.hasOwnProperty(h)){this[h]=g[h]}}}return this},protoFunction:function(g,f){this.prototype[g]=f;if(typeof f==="function"){f.SUPER=this.SUPER.prototype}},prototype:{Init:function(){},SUPER:function(f){return f.callee.SUPER},can:function(f){return typeof(this[f])==="function"},has:function(f){return typeof(this[f])!=="undefined"},isa:function(f){return(f instanceof Object)&&(this instanceof f)}},can:function(f){return this.prototype.can.call(this,f)},has:function(f){return this.prototype.has.call(this,f)},isa:function(g){var f=this;while(f){if(f===g){return true}else{f=f.SUPER}}return false},SimpleSUPER:c({constructor:function(f){return this.SimpleSUPER.define(f)},define:function(f){var h={};if(f!=null){for(var g in f){if(f.hasOwnProperty(g)){h[g]=this.wrap(g,f[g])}}if(f.toString!==this.prototype.toString&&f.toString!=={}.toString){h.toString=this.wrap("toString",f.toString)}}return h},wrap:function(i,h){if(typeof(h)!=="function"||!h.toString().match(/\.\s*SUPER\s*\(/)){return h}var g=function(){this.SUPER=g.SUPER[i];try{var f=h.apply(this,arguments)}catch(j){delete this.SUPER;throw j}delete this.SUPER;return f};g.toString=function(){return h.toString.apply(h,arguments)};return g}})});b.Object.isArray=Array.isArray||function(f){return Object.prototype.toString.call(f)==="[object Array]"};b.Object.Array=Array})("MathJax");(function(BASENAME){var BASE=window[BASENAME];if(!BASE){BASE=window[BASENAME]={}}var isArray=BASE.Object.isArray;var CALLBACK=function(data){var cb=function(){return arguments.callee.execute.apply(arguments.callee,arguments)};for(var id in CALLBACK.prototype){if(CALLBACK.prototype.hasOwnProperty(id)){if(typeof(data[id])!=="undefined"){cb[id]=data[id]}else{cb[id]=CALLBACK.prototype[id]}}}cb.toString=CALLBACK.prototype.toString;return cb};CALLBACK.prototype={isCallback:true,hook:function(){},data:[],object:window,execute:function(){if(!this.called||this.autoReset){this.called=!this.autoReset;return this.hook.apply(this.object,this.data.concat([].slice.call(arguments,0)))}},reset:function(){delete this.called},toString:function(){return this.hook.toString.apply(this.hook,arguments)}};var ISCALLBACK=function(f){return(typeof(f)==="function"&&f.isCallback)};var EVAL=function(code){return eval.call(window,code)};var TESTEVAL=function(){EVAL("var __TeSt_VaR__ = 1");if(window.__TeSt_VaR__){try{delete window.__TeSt_VaR__}catch(error){window.__TeSt_VaR__=null}}else{if(window.execScript){EVAL=function(code){BASE.__code=code;code="try {"+BASENAME+".__result = eval("+BASENAME+".__code)} catch(err) {"+BASENAME+".__result = err}";window.execScript(code);var result=BASE.__result;delete BASE.__result;delete BASE.__code;if(result instanceof Error){throw result}return result}}else{EVAL=function(code){BASE.__code=code;code="try {"+BASENAME+".__result = eval("+BASENAME+".__code)} catch(err) {"+BASENAME+".__result = err}";var head=(document.getElementsByTagName("head"))[0];if(!head){head=document.body}var script=document.createElement("script");script.appendChild(document.createTextNode(code));head.appendChild(script);head.removeChild(script);var result=BASE.__result;delete BASE.__result;delete BASE.__code;if(result instanceof Error){throw result}return result}}}TESTEVAL=null};var USING=function(args,i){if(arguments.length>1){if(arguments.length===2&&!(typeof arguments[0]==="function")&&arguments[0] instanceof Object&&typeof arguments[1]==="number"){args=[].slice.call(args,i)}else{args=[].slice.call(arguments,0)}}if(isArray(args)&&args.length===1){args=args[0]}if(typeof args==="function"){if(args.execute===CALLBACK.prototype.execute){return args}return CALLBACK({hook:args})}else{if(isArray(args)){if(typeof(args[0])==="string"&&args[1] instanceof Object&&typeof args[1][args[0]]==="function"){return CALLBACK({hook:args[1][args[0]],object:args[1],data:args.slice(2)})}else{if(typeof args[0]==="function"){return CALLBACK({hook:args[0],data:args.slice(1)})}else{if(typeof args[1]==="function"){return CALLBACK({hook:args[1],object:args[0],data:args.slice(2)})}}}}else{if(typeof(args)==="string"){if(TESTEVAL){TESTEVAL()}return CALLBACK({hook:EVAL,data:[args]})}else{if(args instanceof Object){return CALLBACK(args)}else{if(typeof(args)==="undefined"){return CALLBACK({})}}}}}throw Error("Can't make callback from given data")};var DELAY=function(time,callback){callback=USING(callback);callback.timeout=setTimeout(callback,time);return callback};var WAITFOR=function(callback,signal){callback=USING(callback);if(!callback.called){WAITSIGNAL(callback,signal);signal.pending++}};var WAITEXECUTE=function(){var signals=this.signal;delete this.signal;this.execute=this.oldExecute;delete this.oldExecute;var result=this.execute.apply(this,arguments);if(ISCALLBACK(result)&&!result.called){WAITSIGNAL(result,signals)}else{for(var i=0,m=signals.length;i<m;i++){signals[i].pending--;if(signals[i].pending<=0){signals[i].call()}}}};var WAITSIGNAL=function(callback,signals){if(!isArray(signals)){signals=[signals]}if(!callback.signal){callback.oldExecute=callback.execute;callback.execute=WAITEXECUTE;callback.signal=signals}else{if(signals.length===1){callback.signal.push(signals[0])}else{callback.signal=callback.signal.concat(signals)}}};var AFTER=function(callback){callback=USING(callback);callback.pending=0;for(var i=1,m=arguments.length;i<m;i++){if(arguments[i]){WAITFOR(arguments[i],callback)}}if(callback.pending===0){var result=callback();if(ISCALLBACK(result)){callback=result}}return callback};var HOOKS=MathJax.Object.Subclass({Init:function(reset){this.hooks=[];this.remove=[];this.reset=reset;this.running=false},Add:function(hook,priority){if(priority==null){priority=10}if(!ISCALLBACK(hook)){hook=USING(hook)}hook.priority=priority;var i=this.hooks.length;while(i>0&&priority<this.hooks[i-1].priority){i--}this.hooks.splice(i,0,hook);return hook},Remove:function(hook){for(var i=0,m=this.hooks.length;i<m;i++){if(this.hooks[i]===hook){if(this.running){this.remove.push(i)}else{this.hooks.splice(i,1)}return}}},Execute:function(){var callbacks=[{}];this.running=true;for(var i=0,m=this.hooks.length;i<m;i++){if(this.reset){this.hooks[i].reset()}var result=this.hooks[i].apply(window,arguments);if(ISCALLBACK(result)&&!result.called){callbacks.push(result)}}this.running=false;if(this.remove.length){this.RemovePending()}if(callbacks.length===1){return null}if(callbacks.length===2){return callbacks[1]}return AFTER.apply({},callbacks)},RemovePending:function(){this.remove=this.remove.sort();for(var i=this.remove.length-1;i>=0;i--){this.hooks.splice(i,1)}this.remove=[]}});var EXECUTEHOOKS=function(hooks,data,reset){if(!hooks){return null}if(!isArray(hooks)){hooks=[hooks]}if(!isArray(data)){data=(data==null?[]:[data])}var handler=HOOKS(reset);for(var i=0,m=hooks.length;i<m;i++){handler.Add(hooks[i])}return handler.Execute.apply(handler,data)};var QUEUE=BASE.Object.Subclass({Init:function(){this.pending=this.running=0;this.queue=[];this.Push.apply(this,arguments)},Push:function(){var callback;for(var i=0,m=arguments.length;i<m;i++){callback=USING(arguments[i]);if(callback===arguments[i]&&!callback.called){callback=USING(["wait",this,callback])}this.queue.push(callback)}if(!this.running&&!this.pending){this.Process()}return callback},Process:function(queue){while(!this.running&&!this.pending&&this.queue.length){var callback=this.queue[0];queue=this.queue.slice(1);this.queue=[];this.Suspend();var result=callback();this.Resume();if(queue.length){this.queue=queue.concat(this.queue)}if(ISCALLBACK(result)&&!result.called){WAITFOR(result,this)}}},Suspend:function(){this.running++},Resume:function(){if(this.running){this.running--}},call:function(){this.Process.apply(this,arguments)},wait:function(callback){return callback}});var SIGNAL=QUEUE.Subclass({Init:function(name){QUEUE.prototype.Init.call(this);this.name=name;this.posted=[];this.listeners=HOOKS(true);this.posting=false;this.callback=null},Post:function(message,callback,forget){callback=USING(callback);if(this.posting||this.pending){this.Push(["Post",this,message,callback,forget])}else{this.callback=callback;callback.reset();if(!forget){this.posted.push(message)}this.Suspend();this.posting=true;var result=this.listeners.Execute(message);if(ISCALLBACK(result)&&!result.called){WAITFOR(result,this)}this.Resume();this.posting=false;if(!this.pending){this.call()}}return callback},Clear:function(callback){callback=USING(callback);if(this.posting||this.pending){callback=this.Push(["Clear",this,callback])}else{this.posted=[];callback()}return callback},call:function(){this.callback(this);this.Process()},Interest:function(callback,ignorePast,priority){callback=USING(callback);this.listeners.Add(callback,priority);if(!ignorePast){for(var i=0,m=this.posted.length;i<m;i++){callback.reset();var result=callback(this.posted[i]);if(ISCALLBACK(result)&&i===this.posted.length-1){WAITFOR(result,this)}}}return callback},NoInterest:function(callback){this.listeners.Remove(callback)},MessageHook:function(msg,callback,priority){callback=USING(callback);if(!this.hooks){this.hooks={};this.Interest(["ExecuteHooks",this])}if(!this.hooks[msg]){this.hooks[msg]=HOOKS(true)}this.hooks[msg].Add(callback,priority);for(var i=0,m=this.posted.length;i<m;i++){if(this.posted[i]==msg){callback.reset();callback(this.posted[i])}}callback.msg=msg;return callback},ExecuteHooks:function(msg){var type=(isArray(msg)?msg[0]:msg);if(!this.hooks[type]){return null}return this.hooks[type].Execute(msg)},RemoveHook:function(hook){this.hooks[hook.msg].Remove(hook)}},{signals:{},find:function(name){if(!SIGNAL.signals[name]){SIGNAL.signals[name]=new SIGNAL(name)}return SIGNAL.signals[name]}});BASE.Callback=BASE.CallBack=USING;BASE.Callback.Delay=DELAY;BASE.Callback.After=AFTER;BASE.Callback.Queue=QUEUE;BASE.Callback.Signal=SIGNAL.find;BASE.Callback.Hooks=HOOKS;BASE.Callback.ExecuteHooks=EXECUTEHOOKS})("MathJax");(function(e){var a=window[e];if(!a){a=window[e]={}}var d=(navigator.vendor==="Apple Computer, Inc."&&typeof navigator.vendorSub==="undefined");var g=0;var h=function(i){if(document.styleSheets&&document.styleSheets.length>g){g=document.styleSheets.length}if(!i){i=document.head||((document.getElementsByTagName("head"))[0]);if(!i){i=document.body}}return i};var f=[];var c=function(){for(var k=0,j=f.length;k<j;k++){a.Ajax.head.removeChild(f[k])}f=[]};var b={};b[e]="";b.Contrib="https://cdn.mathjax.org/mathjax/contrib";a.Ajax={loaded:{},loading:{},loadHooks:{},timeout:15*1000,styleDelay:1,config:{root:"",path:b},params:{},STATUS:{OK:1,ERROR:-1},fileURL:function(j){var i=j.match(/^\[([-._a-z0-9]+)\]/i);if(i&&i[1] in b){j=(b[i[1]]||this.config.root)+j.substr(i[1].length+2)}return j},fileName:function(j){var i=this.config.root;if(j.substr(0,i.length)===i){j="["+e+"]"+j.substr(i.length)}else{for(var k in b){if(b.hasOwnProperty(k)&&b[k]){if(j.substr(0,b[k].length)===b[k]){j="["+k+"]"+j.substr(b[k].length);break}}}}return j},fileRev:function(j){var i=a.cdnFileVersions[j]||a.cdnVersion||"";if(i){i="?V="+i}return i},urlRev:function(i){return this.fileURL(i)+this.fileRev(i)},Require:function(k,n){n=a.Callback(n);var l;if(k instanceof Object){for(var j in k){if(k.hasOwnProperty(j)){l=j.toUpperCase();k=k[j]}}}else{l=k.split(/\./).pop().toUpperCase()}if(this.params.noContrib&&k.substr(0,9)==="[Contrib]"){n(this.STATUS.ERROR)}else{k=this.fileURL(k);if(this.loaded[k]){n(this.loaded[k])}else{var m={};m[l]=k;this.Load(m,n)}}return n},Load:function(k,m){m=a.Callback(m);var l;if(k instanceof Object){for(var j in k){if(k.hasOwnProperty(j)){l=j.toUpperCase();k=k[j]}}}else{l=k.split(/\./).pop().toUpperCase()}k=this.fileURL(k);if(this.loading[k]){this.addHook(k,m)}else{this.head=h(this.head);if(this.loader[l]){this.loader[l].call(this,k,m)}else{throw Error("Can't load files of type "+l)}}return m},LoadHook:function(l,m,k){m=a.Callback(m);if(l instanceof Object){for(var j in l){if(l.hasOwnProperty(j)){l=l[j]}}}l=this.fileURL(l);if(this.loaded[l]){m(this.loaded[l])}else{this.addHook(l,m,k)}return m},addHook:function(j,k,i){if(!this.loadHooks[j]){this.loadHooks[j]=MathJax.Callback.Hooks()}this.loadHooks[j].Add(k,i);k.file=j},removeHook:function(i){if(this.loadHooks[i.file]){this.loadHooks[i.file].Remove(i);if(!this.loadHooks[i.file].hooks.length){delete this.loadHooks[i.file]}}},Preloading:function(){for(var l=0,j=arguments.length;l<j;l++){var k=this.fileURL(arguments[l]);if(!this.loading[k]){this.loading[k]={preloaded:true}}}},loader:{JS:function(k,m){var j=this.fileName(k);var i=document.createElement("script");var l=a.Callback(["loadTimeout",this,k]);this.loading[k]={callback:m,timeout:setTimeout(l,this.timeout),status:this.STATUS.OK,script:i};this.loading[k].message=a.Message.File(j);i.onerror=l;i.type="text/javascript";i.src=k+this.fileRev(j);this.head.appendChild(i)},CSS:function(j,l){var i=this.fileName(j);var k=document.createElement("link");k.rel="stylesheet";k.type="text/css";k.href=j+this.fileRev(i);this.loading[j]={callback:l,message:a.Message.File(i),status:this.STATUS.OK};this.head.appendChild(k);this.timer.create.call(this,[this.timer.file,j],k)}},timer:{create:function(j,i){j=a.Callback(j);if(i.nodeName==="STYLE"&&i.styleSheet&&typeof(i.styleSheet.cssText)!=="undefined"){j(this.STATUS.OK)}else{if(window.chrome&&i.nodeName==="LINK"){j(this.STATUS.OK)}else{if(d){this.timer.start(this,[this.timer.checkSafari2,g++,j],this.styleDelay)}else{this.timer.start(this,[this.timer.checkLength,i,j],this.styleDelay)}}}return j},start:function(j,i,k,l){i=a.Callback(i);i.execute=this.execute;i.time=this.time;i.STATUS=j.STATUS;i.timeout=l||j.timeout;i.delay=i.total=k||0;if(k){setTimeout(i,k)}else{i()}},time:function(i){this.total+=this.delay;this.delay=Math.floor(this.delay*1.05+5);if(this.total>=this.timeout){i(this.STATUS.ERROR);return 1}return 0},file:function(j,i){if(i<0){a.Ajax.loadTimeout(j)}else{a.Ajax.loadComplete(j)}},execute:function(){this.hook.call(this.object,this,this.data[0],this.data[1])},checkSafari2:function(i,j,k){if(i.time(k)){return}if(document.styleSheets.length>j&&document.styleSheets[j].cssRules&&document.styleSheets[j].cssRules.length){k(i.STATUS.OK)}else{setTimeout(i,i.delay)}},checkLength:function(i,l,n){if(i.time(n)){return}var m=0;var j=(l.sheet||l.styleSheet);try{if((j.cssRules||j.rules||[]).length>0){m=1}}catch(k){if(k.message.match(/protected variable|restricted URI/)){m=1}else{if(k.message.match(/Security error/)){m=1}}}if(m){setTimeout(a.Callback([n,i.STATUS.OK]),0)}else{setTimeout(i,i.delay)}}},loadComplete:function(i){i=this.fileURL(i);var j=this.loading[i];if(j&&!j.preloaded){a.Message.Clear(j.message);clearTimeout(j.timeout);if(j.script){if(f.length===0){setTimeout(c,0)}f.push(j.script)}this.loaded[i]=j.status;delete this.loading[i];this.addHook(i,j.callback)}else{if(j){delete this.loading[i]}this.loaded[i]=this.STATUS.OK;j={status:this.STATUS.OK}}if(!this.loadHooks[i]){return null}return this.loadHooks[i].Execute(j.status)},loadTimeout:function(i){if(this.loading[i].timeout){clearTimeout(this.loading[i].timeout)}this.loading[i].status=this.STATUS.ERROR;this.loadError(i);this.loadComplete(i)},loadError:function(i){a.Message.Set(["LoadFailed","File failed to load: %1",i],null,2000);a.Hub.signal.Post(["file load error",i])},Styles:function(k,l){var i=this.StyleString(k);if(i===""){l=a.Callback(l);l()}else{var j=document.createElement("style");j.type="text/css";this.head=h(this.head);this.head.appendChild(j);if(j.styleSheet&&typeof(j.styleSheet.cssText)!=="undefined"){j.styleSheet.cssText=i}else{j.appendChild(document.createTextNode(i))}l=this.timer.create.call(this,l,j)}return l},StyleString:function(n){if(typeof(n)==="string"){return n}var k="",o,m;for(o in n){if(n.hasOwnProperty(o)){if(typeof n[o]==="string"){k+=o+" {"+n[o]+"}\n"}else{if(a.Object.isArray(n[o])){for(var l=0;l<n[o].length;l++){m={};m[o]=n[o][l];k+=this.StyleString(m)}}else{if(o.substr(0,6)==="@media"){k+=o+" {"+this.StyleString(n[o])+"}\n"}else{if(n[o]!=null){m=[];for(var j in n[o]){if(n[o].hasOwnProperty(j)){if(n[o][j]!=null){m[m.length]=j+": "+n[o][j]}}}k+=o+" {"+m.join("; ")+"}\n"}}}}}}return k}}})("MathJax");MathJax.HTML={Element:function(d,f,e){var g=document.createElement(d),h;if(f){if(f.hasOwnProperty("style")){var c=f.style;f.style={};for(h in c){if(c.hasOwnProperty(h)){f.style[h.replace(/-([a-z])/g,this.ucMatch)]=c[h]}}}MathJax.Hub.Insert(g,f);for(h in f){if(h==="role"||h.substr(0,5)==="aria-"){g.setAttribute(h,f[h])}}}if(e){if(!MathJax.Object.isArray(e)){e=[e]}for(var b=0,a=e.length;b<a;b++){if(MathJax.Object.isArray(e[b])){g.appendChild(this.Element(e[b][0],e[b][1],e[b][2]))}else{if(d==="script"){this.setScript(g,e[b])}else{g.appendChild(document.createTextNode(e[b]))}}}}return g},ucMatch:function(a,b){return b.toUpperCase()},addElement:function(b,a,d,c){return b.appendChild(this.Element(a,d,c))},TextNode:function(a){return document.createTextNode(a)},addText:function(a,b){return a.appendChild(this.TextNode(b))},setScript:function(a,b){if(this.setScriptBug){a.text=b}else{while(a.firstChild){a.removeChild(a.firstChild)}this.addText(a,b)}},getScript:function(a){var b=(a.text===""?a.innerHTML:a.text);return b.replace(/^\s+/,"").replace(/\s+$/,"")},Cookie:{prefix:"mjx",expires:365,Set:function(a,e){var d=[];if(e){for(var g in e){if(e.hasOwnProperty(g)){d.push(g+":"+e[g].toString().replace(/&/g,"&&"))}}}var b=this.prefix+"."+a+"="+escape(d.join("&;"));if(this.expires){var f=new Date();f.setDate(f.getDate()+this.expires);b+="; expires="+f.toGMTString()}try{document.cookie=b+"; path=/"}catch(c){}},Get:function(a,d){if(!d){d={}}var g=new RegExp("(?:^|;\\s*)"+this.prefix+"\\."+a+"=([^;]*)(?:;|$)");var f;try{f=g.exec(document.cookie)}catch(c){}if(f&&f[1]!==""){var j=unescape(f[1]).split("&;");for(var e=0,b=j.length;e<b;e++){f=j[e].match(/([^:]+):(.*)/);var h=f[2].replace(/&&/g,"&");if(h==="true"){h=true}else{if(h==="false"){h=false}else{if(h.match(/^-?(\d+(\.\d+)?|\.\d+)$/)){h=parseFloat(h)}}}d[f[1]]=h}}return d}}};MathJax.Localization={locale:"en",directory:"[MathJax]/localization",strings:{ast:{menuTitle:"asturianu"},bg:{menuTitle:"\u0431\u044A\u043B\u0433\u0430\u0440\u0441\u043A\u0438"},bcc:{menuTitle:"\u0628\u0644\u0648\u0686\u06CC"},br:{menuTitle:"brezhoneg"},ca:{menuTitle:"catal\u00E0"},cdo:{menuTitle:"M\u00ECng-d\u0115\u0324ng-ng\u1E73\u0304"},cs:{menuTitle:"\u010De\u0161tina"},da:{menuTitle:"dansk"},de:{menuTitle:"Deutsch"},diq:{menuTitle:"Zazaki"},en:{menuTitle:"English",isLoaded:true},eo:{menuTitle:"Esperanto"},es:{menuTitle:"espa\u00F1ol"},fa:{menuTitle:"\u0641\u0627\u0631\u0633\u06CC"},fi:{menuTitle:"suomi"},fr:{menuTitle:"fran\u00E7ais"},gl:{menuTitle:"galego"},he:{menuTitle:"\u05E2\u05D1\u05E8\u05D9\u05EA"},ia:{menuTitle:"interlingua"},it:{menuTitle:"italiano"},ja:{menuTitle:"\u65E5\u672C\u8A9E"},kn:{menuTitle:"\u0C95\u0CA8\u0CCD\u0CA8\u0CA1"},ko:{menuTitle:"\uD55C\uAD6D\uC5B4"},lb:{menuTitle:"L\u00EBtzebuergesch"},lki:{menuTitle:"\u0644\u06D5\u06A9\u06CC"},lt:{menuTitle:"lietuvi\u0173"},mk:{menuTitle:"\u043C\u0430\u043A\u0435\u0434\u043E\u043D\u0441\u043A\u0438"},nl:{menuTitle:"Nederlands"},oc:{menuTitle:"occitan"},pl:{menuTitle:"polski"},pt:{menuTitle:"portugus\u00EA"},"pt-br":{menuTitle:"portugu\u00EAs do Brasil"},ru:{menuTitle:"\u0440\u0443\u0441\u0441\u043A\u0438\u0439"},sco:{menuTitle:"Scots"},scn:{menuTitle:"sicilianu"},sl:{menuTitle:"sloven\u0161\u010Dina"},sv:{menuTitle:"svenska"},tr:{menuTitle:"T\u00FCrk\u00E7e"},uk:{menuTitle:"\u0443\u043A\u0440\u0430\u0457\u043D\u0441\u044C\u043A\u0430"},vi:{menuTitle:"Ti\u1EBFng Vi\u1EC7t"},"zh-hans":{menuTitle:"\u4E2D\u6587\uFF08\u7B80\u4F53\uFF09"}},pattern:/%(\d+|\{\d+\}|\{[a-z]+:\%\d+(?:\|(?:%\{\d+\}|%.|[^\}])*)+\}|.)/g,SPLIT:("axb".split(/(x)/).length===3?function(a,b){return a.split(b)}:function(c,e){var a=[],b,d=0;e.lastIndex=0;while((b=e.exec(c))){a.push(c.substr(d,b.index-d));a.push.apply(a,b.slice(1));d=b.index+b[0].length}a.push(c.substr(d));return a}),_:function(b,a){if(MathJax.Object.isArray(a)){return this.processSnippet(b,a)}return this.processString(this.lookupPhrase(b,a),[].slice.call(arguments,2))},processString:function(l,p,g){var j,e,o=MathJax.Object.isArray;for(j=0,e=p.length;j<e;j++){if(g&&o(p[j])){p[j]=this.processSnippet(g,p[j])}}var f=this.SPLIT(l,this.pattern);for(j=1,e=f.length;j<e;j+=2){var q=f[j].charAt(0);if(q>="0"&&q<="9"){f[j]=p[f[j]-1];if(typeof f[j]==="number"){f[j]=this.number(f[j])}}else{if(q==="{"){q=f[j].substr(1);if(q>="0"&&q<="9"){f[j]=p[f[j].substr(1,f[j].length-2)-1];if(typeof f[j]==="number"){f[j]=this.number(f[j])}}else{var k=f[j].match(/^\{([a-z]+):%(\d+)\|(.*)\}$/);if(k){if(k[1]==="plural"){var d=p[k[2]-1];if(typeof d==="undefined"){f[j]="???"}else{d=this.plural(d)-1;var h=k[3].replace(/(^|[^%])(%%)*%\|/g,"$1$2%\uEFEF").split(/\|/);if(d>=0&&d<h.length){f[j]=this.processString(h[d].replace(/\uEFEF/g,"|"),p,g)}else{f[j]="???"}}}else{f[j]="%"+f[j]}}}}}if(f[j]==null){f[j]="???"}}if(!g){return f.join("")}var a=[],b="";for(j=0;j<e;j++){b+=f[j];j++;if(j<e){if(o(f[j])){a.push(b);a=a.concat(f[j]);b=""}else{b+=f[j]}}}if(b!==""){a.push(b)}return a},processSnippet:function(g,e){var c=[];for(var d=0,b=e.length;d<b;d++){if(MathJax.Object.isArray(e[d])){var f=e[d];if(typeof f[1]==="string"){var h=f[0];if(!MathJax.Object.isArray(h)){h=[g,h]}var a=this.lookupPhrase(h,f[1]);c=c.concat(this.processMarkdown(a,f.slice(2),g))}else{if(MathJax.Object.isArray(f[1])){c=c.concat(this.processSnippet.apply(this,f))}else{if(f.length>=3){c.push([f[0],f[1],this.processSnippet(g,f[2])])}else{c.push(e[d])}}}}else{c.push(e[d])}}return c},markdownPattern:/(%.)|(\*{1,3})((?:%.|.)+?)\2|(`+)((?:%.|.)+?)\4|\[((?:%.|.)+?)\]\(([^\s\)]+)\)/,processMarkdown:function(b,h,d){var j=[],e;var c=b.split(this.markdownPattern);var g=c[0];for(var f=1,a=c.length;f<a;f+=8){if(c[f+1]){e=this.processString(c[f+2],h,d);if(!MathJax.Object.isArray(e)){e=[e]}e=[["b","i","i"][c[f+1].length-1],{},e];if(c[f+1].length===3){e=["b",{},e]}}else{if(c[f+3]){e=this.processString(c[f+4].replace(/^\s/,"").replace(/\s$/,""),h,d);if(!MathJax.Object.isArray(e)){e=[e]}e=["code",{},e]}else{if(c[f+5]){e=this.processString(c[f+5],h,d);if(!MathJax.Object.isArray(e)){e=[e]}e=["a",{href:this.processString(c[f+6],h),target:"_blank"},e]}else{g+=c[f];e=null}}}if(e){j=this.concatString(j,g,h,d);j.push(e);g=""}if(c[f+7]!==""){g+=c[f+7]}}j=this.concatString(j,g,h,d);return j},concatString:function(a,c,b,d){if(c!=""){c=this.processString(c,b,d);if(!MathJax.Object.isArray(c)){c=[c]}a=a.concat(c)}return a},lookupPhrase:function(f,a,d){if(!d){d="_"}if(MathJax.Object.isArray(f)){d=(f[0]||"_");f=(f[1]||"")}var c=this.loadDomain(d);if(c){MathJax.Hub.RestartAfter(c)}var b=this.strings[this.locale];if(b){if(b.domains&&d in b.domains){var e=b.domains[d];if(e.strings&&f in e.strings){a=e.strings[f]}}}return a},loadFile:function(b,d,e){e=MathJax.Callback(e);b=(d.file||b);if(!b.match(/\.js$/)){b+=".js"}if(!b.match(/^([a-z]+:|\[MathJax\])/)){var a=(this.strings[this.locale].directory||this.directory+"/"+this.locale||"[MathJax]/localization/"+this.locale);b=a+"/"+b}var c=MathJax.Ajax.Require(b,function(){d.isLoaded=true;return e()});return(c.called?null:c)},loadDomain:function(c,e){var b,a=this.strings[this.locale];if(a){if(!a.isLoaded){b=this.loadFile(this.locale,a);if(b){return MathJax.Callback.Queue(b,["loadDomain",this,c]).Push(e||{})}}if(a.domains&&c in a.domains){var d=a.domains[c];if(!d.isLoaded){b=this.loadFile(c,d);if(b){return MathJax.Callback.Queue(b).Push(e)}}}}return MathJax.Callback(e)()},Try:function(a){a=MathJax.Callback(a);a.autoReset=true;try{a()}catch(b){if(!b.restart){throw b}MathJax.Callback.After(["Try",this,a],b.restart)}},resetLocale:function(a){if(!a){return}a=a.toLowerCase();while(!this.strings[a]){var c=a.lastIndexOf("-");if(c===-1){return}a=a.substring(0,c)}var b=this.strings[a].remap;this.locale=b?b:a},setLocale:function(a){this.resetLocale(a);if(MathJax.Menu){this.loadDomain("MathMenu")}},addTranslation:function(b,e,c){var d=this.strings[b],a=false;if(!d){d=this.strings[b]={};a=true}if(!d.domains){d.domains={}}if(e){if(!d.domains[e]){d.domains[e]={}}d=d.domains[e]}MathJax.Hub.Insert(d,c);if(a&&MathJax.Menu.menu){MathJax.Menu.CreateLocaleMenu()}},setCSS:function(b){var a=this.strings[this.locale];if(a){if(a.fontFamily){b.style.fontFamily=a.fontFamily}if(a.fontDirection){b.style.direction=a.fontDirection;if(a.fontDirection==="rtl"){b.style.textAlign="right"}}}return b},fontFamily:function(){var a=this.strings[this.locale];return(a?a.fontFamily:null)},fontDirection:function(){var a=this.strings[this.locale];return(a?a.fontDirection:null)},plural:function(b){var a=this.strings[this.locale];if(a&&a.plural){return a.plural(b)}if(b==1){return 1}return 2},number:function(b){var a=this.strings[this.locale];if(a&&a.number){return a.number(b)}return b}};MathJax.Message={ready:false,log:[{}],current:null,textNodeBug:(navigator.vendor==="Apple Computer, Inc."&&typeof navigator.vendorSub==="undefined")||(window.hasOwnProperty&&window.hasOwnProperty("konqueror")),styles:{"#MathJax_Message":{position:"fixed",left:"1px",bottom:"2px","background-color":"#E6E6E6",border:"1px solid #959595",margin:"0px",padding:"2px 8px","z-index":"102",color:"black","font-size":"80%",width:"auto","white-space":"nowrap"},"#MathJax_MSIE_Frame":{position:"absolute",top:0,left:0,width:"0px","z-index":101,border:"0px",margin:"0px",padding:"0px"}},browsers:{MSIE:function(a){MathJax.Message.msieFixedPositionBug=((document.documentMode||0)<7);if(MathJax.Message.msieFixedPositionBug){MathJax.Hub.config.styles["#MathJax_Message"].position="absolute"}MathJax.Message.quirks=(document.compatMode==="BackCompat")},Chrome:function(a){MathJax.Hub.config.styles["#MathJax_Message"].bottom="1.5em";MathJax.Hub.config.styles["#MathJax_Message"].left="1em"}},Init:function(a){if(a){this.ready=true}if(!document.body||!this.ready){return false}if(this.div&&this.div.parentNode==null){this.div=document.getElementById("MathJax_Message");if(this.div){this.text=this.div.firstChild}}if(!this.div){var b=document.body;if(this.msieFixedPositionBug&&window.attachEvent){b=this.frame=this.addDiv(document.body);b.removeAttribute("id");b.style.position="absolute";b.style.border=b.style.margin=b.style.padding="0px";b.style.zIndex="101";b.style.height="0px";b=this.addDiv(b);b.id="MathJax_MSIE_Frame";window.attachEvent("onscroll",this.MoveFrame);window.attachEvent("onresize",this.MoveFrame);this.MoveFrame()}this.div=this.addDiv(b);this.div.style.display="none";this.text=this.div.appendChild(document.createTextNode(""))}return true},addDiv:function(a){var b=document.createElement("div");b.id="MathJax_Message";if(a.firstChild){a.insertBefore(b,a.firstChild)}else{a.appendChild(b)}return b},MoveFrame:function(){var a=(MathJax.Message.quirks?document.body:document.documentElement);var b=MathJax.Message.frame;b.style.left=a.scrollLeft+"px";b.style.top=a.scrollTop+"px";b.style.width=a.clientWidth+"px";b=b.firstChild;b.style.height=a.clientHeight+"px"},localize:function(a){return MathJax.Localization._(a,a)},filterText:function(a,c,b){if(MathJax.Hub.config.messageStyle==="simple"){if(b==="LoadFile"){if(!this.loading){this.loading=this.localize("Loading")+" "}a=this.loading;this.loading+="."}else{if(b==="ProcessMath"){if(!this.processing){this.processing=this.localize("Processing")+" "}a=this.processing;this.processing+="."}else{if(b==="TypesetMath"){if(!this.typesetting){this.typesetting=this.localize("Typesetting")+" "}a=this.typesetting;this.typesetting+="."}}}}return a},clearCounts:function(){delete this.loading;delete this.processing;delete this.typesetting},Set:function(c,e,b){if(e==null){e=this.log.length;this.log[e]={}}var d="";if(MathJax.Object.isArray(c)){d=c[0];if(MathJax.Object.isArray(d)){d=d[1]}try{c=MathJax.Localization._.apply(MathJax.Localization,c)}catch(a){if(!a.restart){throw a}if(!a.restart.called){if(this.log[e].restarted==null){this.log[e].restarted=0}this.log[e].restarted++;delete this.log[e].cleared;MathJax.Callback.After(["Set",this,c,e,b],a.restart);return e}}}if(this.timer){clearTimeout(this.timer);delete this.timer}this.log[e].text=c;this.log[e].filteredText=c=this.filterText(c,e,d);if(typeof(this.log[e].next)==="undefined"){this.log[e].next=this.current;if(this.current!=null){this.log[this.current].prev=e}this.current=e}if(this.current===e&&MathJax.Hub.config.messageStyle!=="none"){if(this.Init()){if(this.textNodeBug){this.div.innerHTML=c}else{this.text.nodeValue=c}this.div.style.display="";if(this.status){window.status="";delete this.status}}else{window.status=c;this.status=true}}if(this.log[e].restarted){if(this.log[e].cleared){b=0}if(--this.log[e].restarted===0){delete this.log[e].cleared}}if(b){setTimeout(MathJax.Callback(["Clear",this,e]),b)}else{if(b==0){this.Clear(e,0)}}return e},Clear:function(b,a){if(this.log[b].prev!=null){this.log[this.log[b].prev].next=this.log[b].next}if(this.log[b].next!=null){this.log[this.log[b].next].prev=this.log[b].prev}if(this.current===b){this.current=this.log[b].next;if(this.text){if(this.div.parentNode==null){this.Init()}if(this.current==null){if(this.timer){clearTimeout(this.timer);delete this.timer}if(a==null){a=600}if(a===0){this.Remove()}else{this.timer=setTimeout(MathJax.Callback(["Remove",this]),a)}}else{if(MathJax.Hub.config.messageStyle!=="none"){if(this.textNodeBug){this.div.innerHTML=this.log[this.current].filteredText}else{this.text.nodeValue=this.log[this.current].filteredText}}}if(this.status){window.status="";delete this.status}}else{if(this.status){window.status=(this.current==null?"":this.log[this.current].text)}}}delete this.log[b].next;delete this.log[b].prev;delete this.log[b].filteredText;if(this.log[b].restarted){this.log[b].cleared=true}},Remove:function(){this.text.nodeValue="";this.div.style.display="none"},File:function(a){return this.Set(["LoadFile","Loading %1",a],null,null)},Log:function(){var b=[];for(var c=1,a=this.log.length;c<a;c++){b[c]=this.log[c].text}return b.join("\n")}};MathJax.Hub={config:{root:"",config:[],styleSheets:[],styles:{".MathJax_Preview":{color:"#888"}},jax:[],extensions:[],preJax:null,postJax:null,displayAlign:"center",displayIndent:"0",preRemoveClass:"MathJax_Preview",showProcessingMessages:true,messageStyle:"normal",delayStartupUntil:"none",skipStartupTypeset:false,elements:[],positionToHash:true,showMathMenu:true,showMathMenuMSIE:true,menuSettings:{zoom:"None",CTRL:false,ALT:false,CMD:false,Shift:false,discoverable:false,zscale:"200%",renderer:null,font:"Auto",context:"MathJax",locale:null,mpContext:false,mpMouse:false,texHints:true,FastPreview:null,assistiveMML:null,inTabOrder:true,semantics:false},errorSettings:{message:["[",["MathProcessingError","Math Processing Error"],"]"],style:{color:"#CC0000","font-style":"italic"}},ignoreMMLattributes:{}},preProcessors:MathJax.Callback.Hooks(true),inputJax:{},outputJax:{order:{}},processSectionDelay:50,processUpdateTime:250,processUpdateDelay:10,signal:MathJax.Callback.Signal("Hub"),Config:function(a){this.Insert(this.config,a);if(this.config.Augment){this.Augment(this.config.Augment)}},CombineConfig:function(c,f){var b=this.config,g,e;c=c.split(/\./);for(var d=0,a=c.length;d<a;d++){g=c[d];if(!b[g]){b[g]={}}e=b;b=b[g]}e[g]=b=this.Insert(f,b);return b},Register:{PreProcessor:function(){return MathJax.Hub.preProcessors.Add.apply(MathJax.Hub.preProcessors,arguments)},MessageHook:function(){return MathJax.Hub.signal.MessageHook.apply(MathJax.Hub.signal,arguments)},StartupHook:function(){return MathJax.Hub.Startup.signal.MessageHook.apply(MathJax.Hub.Startup.signal,arguments)},LoadHook:function(){return MathJax.Ajax.LoadHook.apply(MathJax.Ajax,arguments)}},UnRegister:{PreProcessor:function(a){MathJax.Hub.preProcessors.Remove(a)},MessageHook:function(a){MathJax.Hub.signal.RemoveHook(a)},StartupHook:function(a){MathJax.Hub.Startup.signal.RemoveHook(a)},LoadHook:function(a){MathJax.Ajax.removeHook(a)}},getAllJax:function(e){var c=[],b=this.elementScripts(e);for(var d=0,a=b.length;d<a;d++){if(b[d].MathJax&&b[d].MathJax.elementJax){c.push(b[d].MathJax.elementJax)}}return c},getJaxByType:function(f,e){var c=[],b=this.elementScripts(e);for(var d=0,a=b.length;d<a;d++){if(b[d].MathJax&&b[d].MathJax.elementJax&&b[d].MathJax.elementJax.mimeType===f){c.push(b[d].MathJax.elementJax)}}return c},getJaxByInputType:function(f,e){var c=[],b=this.elementScripts(e);for(var d=0,a=b.length;d<a;d++){if(b[d].MathJax&&b[d].MathJax.elementJax&&b[d].type&&b[d].type.replace(/ *;(.|\s)*/,"")===f){c.push(b[d].MathJax.elementJax)}}return c},getJaxFor:function(a){if(typeof(a)==="string"){a=document.getElementById(a)}if(a&&a.MathJax){return a.MathJax.elementJax}if(this.isMathJaxNode(a)){if(!a.isMathJax){a=a.firstChild}while(a&&!a.jaxID){a=a.parentNode}if(a){return MathJax.OutputJax[a.jaxID].getJaxFromMath(a)}}return null},isJax:function(a){if(typeof(a)==="string"){a=document.getElementById(a)}if(this.isMathJaxNode(a)){return 1}if(a&&(a.tagName||"").toLowerCase()==="script"){if(a.MathJax){return(a.MathJax.state===MathJax.ElementJax.STATE.PROCESSED?1:-1)}if(a.type&&this.inputJax[a.type.replace(/ *;(.|\s)*/,"")]){return -1}}return 0},isMathJaxNode:function(a){return !!a&&(a.isMathJax||(a.className||"")==="MathJax_MathML")},setRenderer:function(d,c){if(!d){return}if(!MathJax.OutputJax[d]){this.config.menuSettings.renderer="";var b="[MathJax]/jax/output/"+d+"/config.js";return MathJax.Ajax.Require(b,["setRenderer",this,d,c])}else{this.config.menuSettings.renderer=d;if(c==null){c="jax/mml"}var a=this.outputJax;if(a[c]&&a[c].length){if(d!==a[c][0].id){a[c].unshift(MathJax.OutputJax[d]);return this.signal.Post(["Renderer Selected",d])}}return null}},Queue:function(){return this.queue.Push.apply(this.queue,arguments)},Typeset:function(c,d){if(!MathJax.isReady){return null}var b=this.elementCallback(c,d);if(b.count){var a=MathJax.Callback.Queue(["PreProcess",this,b.elements],["Process",this,b.elements])}return a.Push(b.callback)},PreProcess:function(e,g){var c=this.elementCallback(e,g);var b=MathJax.Callback.Queue();if(c.count){var f=(c.count===1?[c.elements]:c.elements);b.Push(["Post",this.signal,["Begin PreProcess",c.elements]]);for(var d=0,a=f.length;d<a;d++){if(f[d]){b.Push(["Execute",this.preProcessors,f[d]])}}b.Push(["Post",this.signal,["End PreProcess",c.elements]])}return b.Push(c.callback)},Process:function(a,b){return this.takeAction("Process",a,b)},Update:function(a,b){return this.takeAction("Update",a,b)},Reprocess:function(a,b){return this.takeAction("Reprocess",a,b)},Rerender:function(a,b){return this.takeAction("Rerender",a,b)},takeAction:function(g,d,h){var c=this.elementCallback(d,h);var f=c.elements;var a=MathJax.Callback.Queue(["Clear",this.signal]);var e={scripts:[],start:new Date().getTime(),i:0,j:0,jax:{},jaxIDs:[]};if(c.count){var b=["Delay",MathJax.Callback,this.processSectionDelay];if(!b[2]){b={}}a.Push(["clearCounts",MathJax.Message],["Post",this.signal,["Begin "+g,f]],["Post",this.signal,["Begin Math",f,g]],["prepareScripts",this,g,f,e],["Post",this.signal,["Begin Math Input",f,g]],["processInput",this,e],["Post",this.signal,["End Math Input",f,g]],b,["prepareOutput",this,e,"preProcess"],b,["Post",this.signal,["Begin Math Output",f,g]],["processOutput",this,e],["Post",this.signal,["End Math Output",f,g]],b,["prepareOutput",this,e,"postProcess"],b,["Post",this.signal,["End Math",f,g]],["Post",this.signal,["End "+g,f]],["clearCounts",MathJax.Message])}return a.Push(c.callback)},scriptAction:{Process:function(a){},Update:function(b){var a=b.MathJax.elementJax;if(a&&a.needsUpdate()){a.Remove(true);b.MathJax.state=a.STATE.UPDATE}else{b.MathJax.state=a.STATE.PROCESSED}},Reprocess:function(b){var a=b.MathJax.elementJax;if(a){a.Remove(true);b.MathJax.state=a.STATE.UPDATE}},Rerender:function(b){var a=b.MathJax.elementJax;if(a){a.Remove(true);b.MathJax.state=a.STATE.OUTPUT}}},prepareScripts:function(h,e,g){if(arguments.callee.disabled){return}var b=this.elementScripts(e);var f=MathJax.ElementJax.STATE;for(var d=0,a=b.length;d<a;d++){var c=b[d];if(c.type&&this.inputJax[c.type.replace(/ *;(.|\n)*/,"")]){if(c.MathJax){if(c.MathJax.elementJax&&c.MathJax.elementJax.hover){MathJax.Extension.MathEvents.Hover.ClearHover(c.MathJax.elementJax)}if(c.MathJax.state!==f.PENDING){this.scriptAction[h](c)}}if(!c.MathJax){c.MathJax={state:f.PENDING}}if(c.MathJax.error){delete c.MathJax.error}if(c.MathJax.state!==f.PROCESSED){g.scripts.push(c)}}}},checkScriptSiblings:function(a){if(a.MathJax.checked){return}var b=this.config,f=a.previousSibling;if(f&&f.nodeName==="#text"){var d,e,c=a.nextSibling;if(c&&c.nodeName!=="#text"){c=null}if(b.preJax){if(typeof(b.preJax)==="string"){b.preJax=new RegExp(b.preJax+"$")}d=f.nodeValue.match(b.preJax)}if(b.postJax&&c){if(typeof(b.postJax)==="string"){b.postJax=new RegExp("^"+b.postJax)}e=c.nodeValue.match(b.postJax)}if(d&&(!b.postJax||e)){f.nodeValue=f.nodeValue.replace(b.preJax,(d.length>1?d[1]:""));f=null}if(e&&(!b.preJax||d)){c.nodeValue=c.nodeValue.replace(b.postJax,(e.length>1?e[1]:""))}if(f&&!f.nodeValue.match(/\S/)){f=f.previousSibling}}if(b.preRemoveClass&&f&&f.className===b.preRemoveClass){a.MathJax.preview=f}a.MathJax.checked=1},processInput:function(a){var b,i=MathJax.ElementJax.STATE;var h,e,d=a.scripts.length;try{while(a.i<d){h=a.scripts[a.i];if(!h){a.i++;continue}e=h.previousSibling;if(e&&e.className==="MathJax_Error"){e.parentNode.removeChild(e)}if(!h.parentNode||!h.MathJax||h.MathJax.state===i.PROCESSED){a.i++;continue}if(!h.MathJax.elementJax||h.MathJax.state===i.UPDATE){this.checkScriptSiblings(h);var g=h.type.replace(/ *;(.|\s)*/,"");var j=this.inputJax[g];b=j.Process(h,a);if(typeof b==="function"){if(b.called){continue}this.RestartAfter(b)}b=b.Attach(h,j.id);this.saveScript(b,a,h,i);this.postInputHooks.Execute(b,j.id,h)}else{if(h.MathJax.state===i.OUTPUT){this.saveScript(h.MathJax.elementJax,a,h,i)}}a.i++;var c=new Date().getTime();if(c-a.start>this.processUpdateTime&&a.i<a.scripts.length){a.start=c;this.RestartAfter(MathJax.Callback.Delay(1))}}}catch(f){return this.processError(f,a,"Input")}if(a.scripts.length&&this.config.showProcessingMessages){MathJax.Message.Set(["ProcessMath","Processing math: %1%%",100],0)}a.start=new Date().getTime();a.i=a.j=0;return null},postInputHooks:MathJax.Callback.Hooks(true),saveScript:function(a,d,b,c){if(!this.outputJax[a.mimeType]){b.MathJax.state=c.UPDATE;throw Error("No output jax registered for "+a.mimeType)}a.outputJax=this.outputJax[a.mimeType][0].id;if(!d.jax[a.outputJax]){if(d.jaxIDs.length===0){d.jax[a.outputJax]=d.scripts}else{if(d.jaxIDs.length===1){d.jax[d.jaxIDs[0]]=d.scripts.slice(0,d.i)}d.jax[a.outputJax]=[]}d.jaxIDs.push(a.outputJax)}if(d.jaxIDs.length>1){d.jax[a.outputJax].push(b)}b.MathJax.state=c.OUTPUT},prepareOutput:function(c,f){while(c.j<c.jaxIDs.length){var e=c.jaxIDs[c.j],d=MathJax.OutputJax[e];if(d[f]){try{var a=d[f](c);if(typeof a==="function"){if(a.called){continue}this.RestartAfter(a)}}catch(b){if(!b.restart){MathJax.Message.Set(["PrepError","Error preparing %1 output (%2)",e,f],null,600);MathJax.Hub.lastPrepError=b;c.j++}return MathJax.Callback.After(["prepareOutput",this,c,f],b.restart)}}c.j++}return null},processOutput:function(h){var b,g=MathJax.ElementJax.STATE,d,a=h.scripts.length;try{while(h.i<a){d=h.scripts[h.i];if(!d||!d.parentNode||!d.MathJax||d.MathJax.error){h.i++;continue}var c=d.MathJax.elementJax;if(!c){h.i++;continue}b=MathJax.OutputJax[c.outputJax].Process(d,h);if(b!==false){d.MathJax.state=g.PROCESSED;if(d.MathJax.preview){d.MathJax.preview.innerHTML="";d.MathJax.preview.style.display="none"}this.signal.Post(["New Math",c.inputID])}h.i++;var e=new Date().getTime();if(e-h.start>this.processUpdateTime&&h.i<h.scripts.length){h.start=e;this.RestartAfter(MathJax.Callback.Delay(this.processUpdateDelay))}}}catch(f){return this.processError(f,h,"Output")}if(h.scripts.length&&this.config.showProcessingMessages){MathJax.Message.Set(["TypesetMath","Typesetting math: %1%%",100],0);MathJax.Message.Clear(0)}h.i=h.j=0;return null},processMessage:function(d,b){var a=Math.floor(d.i/(d.scripts.length)*100);var c=(b==="Output"?["TypesetMath","Typesetting math: %1%%"]:["ProcessMath","Processing math: %1%%"]);if(this.config.showProcessingMessages){MathJax.Message.Set(c.concat(a),0)}},processError:function(b,c,a){if(!b.restart){if(!this.config.errorSettings.message){throw b}this.formatError(c.scripts[c.i],b);c.i++}this.processMessage(c,a);return MathJax.Callback.After(["process"+a,this,c],b.restart)},formatError:function(b,f){var h=function(l,k,j,i){return MathJax.Localization._(l,k,j,i)};var e=h("ErrorMessage","Error: %1",f.message)+"\n";if(f.sourceURL||f.fileName){e+="\n"+h("ErrorFile","file: %1",f.sourceURL||f.fileName)}if(f.line||f.lineNumber){e+="\n"+h("ErrorLine","line: %1",f.line||f.lineNumber)}e+="\n\n"+h("ErrorTips","Debugging tips: use %1, inspect %2 in the browser console","'unpacked/MathJax.js'","'MathJax.Hub.lastError'");b.MathJax.error=MathJax.OutputJax.Error.Jax(e,b);if(b.MathJax.elementJax){b.MathJax.error.inputID=b.MathJax.elementJax.inputID}var g=this.config.errorSettings;var a=h(g.messageId,g.message);var c=MathJax.HTML.Element("span",{className:"MathJax_Error",jaxID:"Error",isMathJax:true,id:b.MathJax.error.inputID+"-Frame"},[["span",null,a]]);MathJax.Ajax.Require("[MathJax]/extensions/MathEvents.js",function(){var j=MathJax.Extension.MathEvents.Event,i=MathJax.Hub;c.oncontextmenu=j.Menu;c.onmousedown=j.Mousedown;c.onkeydown=j.Keydown;c.tabIndex=i.getTabOrder(i.getJaxFor(b))});var d=document.getElementById(c.id);if(d){d.parentNode.removeChild(d)}if(b.parentNode){b.parentNode.insertBefore(c,b)}if(b.MathJax.preview){b.MathJax.preview.innerHTML="";b.MathJax.preview.style.display="none"}this.lastError=f;this.signal.Post(["Math Processing Error",b,f])},RestartAfter:function(a){throw this.Insert(Error("restart"),{restart:MathJax.Callback(a)})},elementCallback:function(c,f){if(f==null&&(MathJax.Object.isArray(c)||typeof c==="function")){try{MathJax.Callback(c);f=c;c=null}catch(d){}}if(c==null){c=this.config.elements||[]}if(this.isHTMLCollection(c)){c=this.HTMLCollection2Array(c)}if(!MathJax.Object.isArray(c)){c=[c]}c=[].concat(c);for(var b=0,a=c.length;b<a;b++){if(typeof(c[b])==="string"){c[b]=document.getElementById(c[b])}}if(!document.body){document.body=document.getElementsByTagName("body")[0]}if(c.length==0){c.push(document.body)}if(!f){f={}}return{count:c.length,elements:(c.length===1?c[0]:c),callback:f}},elementScripts:function(e){var b=[];if(MathJax.Object.isArray(e)||this.isHTMLCollection(e)){for(var d=0,a=e.length;d<a;d++){var f=0;for(var c=0;c<d&&!f;c++){f=e[c].contains(e[d])}if(!f){b.push.apply(b,this.elementScripts(e[d]))}}return b}if(typeof(e)==="string"){e=document.getElementById(e)}if(!document.body){document.body=document.getElementsByTagName("body")[0]}if(e==null){e=document.body}if(e.tagName!=null&&e.tagName.toLowerCase()==="script"){return[e]}b=e.getElementsByTagName("script");if(this.msieHTMLCollectionBug){b=this.HTMLCollection2Array(b)}return b},isHTMLCollection:function(a){return("HTMLCollection" in window&&typeof(a)==="object"&&a instanceof HTMLCollection)},HTMLCollection2Array:function(c){if(!this.msieHTMLCollectionBug){return[].slice.call(c)}var b=[];for(var d=0,a=c.length;d<a;d++){b[d]=c[d]}return b},Insert:function(c,a){for(var b in a){if(a.hasOwnProperty(b)){if(typeof a[b]==="object"&&!(MathJax.Object.isArray(a[b]))&&(typeof c[b]==="object"||typeof c[b]==="function")){this.Insert(c[b],a[b])}else{c[b]=a[b]}}}return c},getTabOrder:function(a){return this.config.menuSettings.inTabOrder?0:-1},SplitList:("trim" in String.prototype?function(a){return a.trim().split(/\s+/)}:function(a){return a.replace(/^\s+/,"").replace(/\s+$/,"").split(/\s+/)})};MathJax.Hub.Insert(MathJax.Hub.config.styles,MathJax.Message.styles);MathJax.Hub.Insert(MathJax.Hub.config.styles,{".MathJax_Error":MathJax.Hub.config.errorSettings.style});MathJax.Extension={};MathJax.Hub.Configured=MathJax.Callback({});MathJax.Hub.Startup={script:"",queue:MathJax.Callback.Queue(),signal:MathJax.Callback.Signal("Startup"),params:{},Config:function(){this.queue.Push(["Post",this.signal,"Begin Config"]);if(MathJax.AuthorConfig&&MathJax.AuthorConfig.root){MathJax.Ajax.config.root=MathJax.AuthorConfig.root}if(this.params.locale){MathJax.Localization.resetLocale(this.params.locale);MathJax.Hub.config.menuSettings.locale=this.params.locale}if(this.params.config){var c=this.params.config.split(/,/);for(var b=0,a=c.length;b<a;b++){if(!c[b].match(/\.js$/)){c[b]+=".js"}this.queue.Push(["Require",MathJax.Ajax,this.URL("config",c[b])])}}this.queue.Push(["Config",MathJax.Hub,MathJax.AuthorConfig]);if(this.script.match(/\S/)){this.queue.Push(this.script+";\n1;")}this.queue.Push(["ConfigDelay",this],["ConfigBlocks",this],[function(d){return d.loadArray(MathJax.Hub.config.config,"config",null,true)},this],["Post",this.signal,"End Config"])},ConfigDelay:function(){var a=this.params.delayStartupUntil||MathJax.Hub.config.delayStartupUntil;if(a==="onload"){return this.onload}if(a==="configured"){return MathJax.Hub.Configured}return a},ConfigBlocks:function(){var c=document.getElementsByTagName("script");var b=MathJax.Callback.Queue();for(var d=0,a=c.length;d<a;d++){var e=String(c[d].type).replace(/ /g,"");if(e.match(/^text\/x-mathjax-config(;.*)?$/)&&!e.match(/;executed=true/)){c[d].type+=";executed=true";b.Push(c[d].innerHTML+";\n1;")}}return b.Push(function(){MathJax.Ajax.config.root=MathJax.Hub.config.root})},Cookie:function(){return this.queue.Push(["Post",this.signal,"Begin Cookie"],["Get",MathJax.HTML.Cookie,"menu",MathJax.Hub.config.menuSettings],[function(e){var d=e.menuSettings;if(d.locale){MathJax.Localization.resetLocale(d.locale)}var g=e.menuSettings.renderer,b=e.jax;if(g){var c="output/"+g;b.sort();for(var f=0,a=b.length;f<a;f++){if(b[f].substr(0,7)==="output/"){break}}if(f==a-1){b.pop()}else{while(f<a){if(b[f]===c){b.splice(f,1);break}f++}}b.unshift(c)}if(d.CHTMLpreview!=null){if(d.FastPreview==null){d.FastPreview=d.CHTMLpreview}delete d.CHTMLpreview}if(d.FastPreview&&!MathJax.Extension["fast-preview"]){MathJax.Hub.config.extensions.push("fast-preview.js")}if(e.menuSettings.assistiveMML&&!MathJax.Extension.AssistiveMML){MathJax.Hub.config.extensions.push("AssistiveMML.js")}},MathJax.Hub.config],["Post",this.signal,"End Cookie"])},Styles:function(){return this.queue.Push(["Post",this.signal,"Begin Styles"],["loadArray",this,MathJax.Hub.config.styleSheets,"config"],["Styles",MathJax.Ajax,MathJax.Hub.config.styles],["Post",this.signal,"End Styles"])},Jax:function(){var f=MathJax.Hub.config,c=MathJax.Hub.outputJax;for(var g=0,b=f.jax.length,d=0;g<b;g++){var e=f.jax[g].substr(7);if(f.jax[g].substr(0,7)==="output/"&&c.order[e]==null){c.order[e]=d;d++}}var a=MathJax.Callback.Queue();return a.Push(["Post",this.signal,"Begin Jax"],["loadArray",this,f.jax,"jax","config.js"],["Post",this.signal,"End Jax"])},Extensions:function(){var a=MathJax.Callback.Queue();return a.Push(["Post",this.signal,"Begin Extensions"],["loadArray",this,MathJax.Hub.config.extensions,"extensions"],["Post",this.signal,"End Extensions"])},Message:function(){MathJax.Message.Init(true)},Menu:function(){var b=MathJax.Hub.config.menuSettings,a=MathJax.Hub.outputJax,d;for(var c in a){if(a.hasOwnProperty(c)){if(a[c].length){d=a[c];break}}}if(d&&d.length){if(b.renderer&&b.renderer!==d[0].id){d.unshift(MathJax.OutputJax[b.renderer])}b.renderer=d[0].id}},Hash:function(){if(MathJax.Hub.config.positionToHash&&document.location.hash&&document.body&&document.body.scrollIntoView){var d=document.location.hash.substr(1);var f=document.getElementById(d);if(!f){var c=document.getElementsByTagName("a");for(var e=0,b=c.length;e<b;e++){if(c[e].name===d){f=c[e];break}}}if(f){while(!f.scrollIntoView){f=f.parentNode}f=this.HashCheck(f);if(f&&f.scrollIntoView){setTimeout(function(){f.scrollIntoView(true)},1)}}}},HashCheck:function(b){var a=MathJax.Hub.getJaxFor(b);if(a&&MathJax.OutputJax[a.outputJax].hashCheck){b=MathJax.OutputJax[a.outputJax].hashCheck(b)}return b},MenuZoom:function(){if(MathJax.Hub.config.showMathMenu){if(!MathJax.Extension.MathMenu){setTimeout(function(){MathJax.Callback.Queue(["Require",MathJax.Ajax,"[MathJax]/extensions/MathMenu.js",{}],["loadDomain",MathJax.Localization,"MathMenu"])},1000)}else{setTimeout(MathJax.Callback(["loadDomain",MathJax.Localization,"MathMenu"]),1000)}if(!MathJax.Extension.MathZoom){setTimeout(MathJax.Callback(["Require",MathJax.Ajax,"[MathJax]/extensions/MathZoom.js",{}]),2000)}}},onLoad:function(){var a=this.onload=MathJax.Callback(function(){MathJax.Hub.Startup.signal.Post("onLoad")});if(document.body&&document.readyState){if(MathJax.Hub.Browser.isMSIE){if(document.readyState==="complete"){return[a]}}else{if(document.readyState!=="loading"){return[a]}}}if(window.addEventListener){window.addEventListener("load",a,false);if(!this.params.noDOMContentEvent){window.addEventListener("DOMContentLoaded",a,false)}}else{if(window.attachEvent){window.attachEvent("onload",a)}else{window.onload=a}}return a},Typeset:function(a,b){if(MathJax.Hub.config.skipStartupTypeset){return function(){}}return this.queue.Push(["Post",this.signal,"Begin Typeset"],["Typeset",MathJax.Hub,a,b],["Post",this.signal,"End Typeset"])},URL:function(b,a){if(!a.match(/^([a-z]+:\/\/|\[|\/)/)){a="[MathJax]/"+b+"/"+a}return a},loadArray:function(b,f,c,a){if(b){if(!MathJax.Object.isArray(b)){b=[b]}if(b.length){var h=MathJax.Callback.Queue(),j={},e;for(var g=0,d=b.length;g<d;g++){e=this.URL(f,b[g]);if(c){e+="/"+c}if(a){h.Push(["Require",MathJax.Ajax,e,j])}else{h.Push(MathJax.Ajax.Require(e,j))}}return h.Push({})}}return null}};(function(d){var b=window[d],e="["+d+"]";var c=b.Hub,a=b.Ajax,f=b.Callback;var g=MathJax.Object.Subclass({JAXFILE:"jax.js",require:null,config:{},Init:function(i,h){if(arguments.length===0){return this}return(this.constructor.Subclass(i,h))()},Augment:function(k,j){var i=this.constructor,h={};if(k!=null){for(var l in k){if(k.hasOwnProperty(l)){if(typeof k[l]==="function"){i.protoFunction(l,k[l])}else{h[l]=k[l]}}}if(k.toString!==i.prototype.toString&&k.toString!=={}.toString){i.protoFunction("toString",k.toString)}}c.Insert(i.prototype,h);i.Augment(null,j);return this},Translate:function(h,i){throw Error(this.directory+"/"+this.JAXFILE+" failed to define the Translate() method")},Register:function(h){},Config:function(){this.config=c.CombineConfig(this.id,this.config);if(this.config.Augment){this.Augment(this.config.Augment)}},Startup:function(){},loadComplete:function(i){if(i==="config.js"){return a.loadComplete(this.directory+"/"+i)}else{var h=f.Queue();h.Push(c.Register.StartupHook("End Config",{}),["Post",c.Startup.signal,this.id+" Jax Config"],["Config",this],["Post",c.Startup.signal,this.id+" Jax Require"],[function(j){return MathJax.Hub.Startup.loadArray(j.require,this.directory)},this],[function(j,k){return MathJax.Hub.Startup.loadArray(j.extensions,"extensions/"+k)},this.config||{},this.id],["Post",c.Startup.signal,this.id+" Jax Startup"],["Startup",this],["Post",c.Startup.signal,this.id+" Jax Ready"]);if(this.copyTranslate){h.Push([function(j){j.preProcess=j.preTranslate;j.Process=j.Translate;j.postProcess=j.postTranslate},this.constructor.prototype])}return h.Push(["loadComplete",a,this.directory+"/"+i])}}},{id:"Jax",version:"2.7.0",directory:e+"/jax",extensionDir:e+"/extensions"});b.InputJax=g.Subclass({elementJax:"mml",sourceMenuTitle:["Original","Original Form"],copyTranslate:true,Process:function(l,q){var j=f.Queue(),o;var k=this.elementJax;if(!b.Object.isArray(k)){k=[k]}for(var n=0,h=k.length;n<h;n++){o=b.ElementJax.directory+"/"+k[n]+"/"+this.JAXFILE;if(!this.require){this.require=[]}else{if(!b.Object.isArray(this.require)){this.require=[this.require]}}this.require.push(o);j.Push(a.Require(o))}o=this.directory+"/"+this.JAXFILE;var p=j.Push(a.Require(o));if(!p.called){this.constructor.prototype.Process=function(){if(!p.called){return p}throw Error(o+" failed to load properly")}}k=c.outputJax["jax/"+k[0]];if(k){j.Push(a.Require(k[0].directory+"/"+this.JAXFILE))}return j.Push({})},needsUpdate:function(h){var i=h.SourceElement();return(h.originalText!==b.HTML.getScript(i))},Register:function(h){if(!c.inputJax){c.inputJax={}}c.inputJax[h]=this}},{id:"InputJax",version:"2.7.0",directory:g.directory+"/input",extensionDir:g.extensionDir});b.OutputJax=g.Subclass({copyTranslate:true,preProcess:function(j){var i,h=this.directory+"/"+this.JAXFILE;this.constructor.prototype.preProcess=function(k){if(!i.called){return i}throw Error(h+" failed to load properly")};i=a.Require(h);return i},Register:function(i){var h=c.outputJax;if(!h[i]){h[i]=[]}if(h[i].length&&(this.id===c.config.menuSettings.renderer||(h.order[this.id]||0)<(h.order[h[i][0].id]||0))){h[i].unshift(this)}else{h[i].push(this)}if(!this.require){this.require=[]}else{if(!b.Object.isArray(this.require)){this.require=[this.require]}}this.require.push(b.ElementJax.directory+"/"+(i.split(/\//)[1])+"/"+this.JAXFILE)},Remove:function(h){}},{id:"OutputJax",version:"2.7.0",directory:g.directory+"/output",extensionDir:g.extensionDir,fontDir:e+(b.isPacked?"":"/..")+"/fonts",imageDir:e+(b.isPacked?"":"/..")+"/images"});b.ElementJax=g.Subclass({Init:function(i,h){return this.constructor.Subclass(i,h)},inputJax:null,outputJax:null,inputID:null,originalText:"",mimeType:"",sourceMenuTitle:["MathMLcode","MathML Code"],Text:function(i,j){var h=this.SourceElement();b.HTML.setScript(h,i);h.MathJax.state=this.STATE.UPDATE;return c.Update(h,j)},Reprocess:function(i){var h=this.SourceElement();h.MathJax.state=this.STATE.UPDATE;return c.Reprocess(h,i)},Update:function(h){return this.Rerender(h)},Rerender:function(i){var h=this.SourceElement();h.MathJax.state=this.STATE.OUTPUT;return c.Process(h,i)},Remove:function(h){if(this.hover){this.hover.clear(this)}b.OutputJax[this.outputJax].Remove(this);if(!h){c.signal.Post(["Remove Math",this.inputID]);this.Detach()}},needsUpdate:function(){return b.InputJax[this.inputJax].needsUpdate(this)},SourceElement:function(){return document.getElementById(this.inputID)},Attach:function(i,j){var h=i.MathJax.elementJax;if(i.MathJax.state===this.STATE.UPDATE){h.Clone(this)}else{h=i.MathJax.elementJax=this;if(i.id){this.inputID=i.id}else{i.id=this.inputID=b.ElementJax.GetID();this.newID=1}}h.originalText=b.HTML.getScript(i);h.inputJax=j;if(h.root){h.root.inputID=h.inputID}return h},Detach:function(){var h=this.SourceElement();if(!h){return}try{delete h.MathJax}catch(i){h.MathJax=null}if(this.newID){h.id=""}},Clone:function(h){var i;for(i in this){if(!this.hasOwnProperty(i)){continue}if(typeof(h[i])==="undefined"&&i!=="newID"){delete this[i]}}for(i in h){if(!h.hasOwnProperty(i)){continue}if(typeof(this[i])==="undefined"||(this[i]!==h[i]&&i!=="inputID")){this[i]=h[i]}}}},{id:"ElementJax",version:"2.7.0",directory:g.directory+"/element",extensionDir:g.extensionDir,ID:0,STATE:{PENDING:1,PROCESSED:2,UPDATE:3,OUTPUT:4},GetID:function(){this.ID++;return"MathJax-Element-"+this.ID},Subclass:function(){var h=g.Subclass.apply(this,arguments);h.loadComplete=this.prototype.loadComplete;return h}});b.ElementJax.prototype.STATE=b.ElementJax.STATE;b.OutputJax.Error={id:"Error",version:"2.7.0",config:{},errors:0,ContextMenu:function(){return b.Extension.MathEvents.Event.ContextMenu.apply(b.Extension.MathEvents.Event,arguments)},Mousedown:function(){return b.Extension.MathEvents.Event.AltContextMenu.apply(b.Extension.MathEvents.Event,arguments)},getJaxFromMath:function(h){return(h.nextSibling.MathJax||{}).error},Jax:function(j,i){var h=MathJax.Hub.inputJax[i.type.replace(/ *;(.|\s)*/,"")];this.errors++;return{inputJax:(h||{id:"Error"}).id,outputJax:"Error",inputID:"MathJax-Error-"+this.errors,sourceMenuTitle:["ErrorMessage","Error Message"],sourceMenuFormat:"Error",originalText:MathJax.HTML.getScript(i),errorText:j}}};b.InputJax.Error={id:"Error",version:"2.7.0",config:{},sourceMenuTitle:["Original","Original Form"]}})("MathJax");(function(o){var h=window[o];if(!h){h=window[o]={}}var d=h.Hub;var s=d.Startup;var w=d.config;var g=document.head||(document.getElementsByTagName("head")[0]);if(!g){g=document.childNodes[0]}var b=(document.documentElement||document).getElementsByTagName("script");if(b.length===0&&g.namespaceURI){b=document.getElementsByTagNameNS(g.namespaceURI,"script")}var f=new RegExp("(^|/)"+o+"\\.js(\\?.*)?$");for(var q=b.length-1;q>=0;q--){if((b[q].src||"").match(f)){s.script=b[q].innerHTML;if(RegExp.$2){var t=RegExp.$2.substr(1).split(/\&/);for(var p=0,l=t.length;p<l;p++){var n=t[p].match(/(.*)=(.*)/);if(n){s.params[unescape(n[1])]=unescape(n[2])}else{s.params[t[p]]=true}}}w.root=b[q].src.replace(/(^|\/)[^\/]*(\?.*)?$/,"").replace(/^(https?:\/\/cdn.mathjax.org\/mathjax\/)(latest)/,"$1"+h.version.split(/\./).slice(0,2).join(".")+"-$2");h.Ajax.config.root=w.root;h.Ajax.params=s.params;break}}var k=navigator.userAgent;var a={isMac:(navigator.platform.substr(0,3)==="Mac"),isPC:(navigator.platform.substr(0,3)==="Win"),isMSIE:("ActiveXObject" in window&&"clipboardData" in window),isEdge:("MSGestureEvent" in window&&"chrome" in window&&window.chrome.loadTimes==null),isFirefox:(!!k.match(/Gecko\//)&&!k.match(/like Gecko/)),isSafari:(!!k.match(/ (Apple)?WebKit\//)&&!k.match(/ like iPhone /)&&(!window.chrome||window.chrome.app==null)),isChrome:("chrome" in window&&window.chrome.loadTimes!=null),isOpera:("opera" in window&&window.opera.version!=null),isKonqueror:("konqueror" in window&&navigator.vendor=="KDE"),versionAtLeast:function(y){var x=(this.version).split(".");y=(new String(y)).split(".");for(var z=0,j=y.length;z<j;z++){if(x[z]!=y[z]){return parseInt(x[z]||"0")>=parseInt(y[z])}}return true},Select:function(j){var i=j[d.Browser];if(i){return i(d.Browser)}return null}};var e=k.replace(/^Mozilla\/(\d+\.)+\d+ /,"").replace(/[a-z][-a-z0-9._: ]+\/\d+[^ ]*-[^ ]*\.([a-z][a-z])?\d+ /i,"").replace(/Gentoo |Ubuntu\/(\d+\.)*\d+ (\([^)]*\) )?/,"");d.Browser=d.Insert(d.Insert(new String("Unknown"),{version:"0.0"}),a);for(var v in a){if(a.hasOwnProperty(v)){if(a[v]&&v.substr(0,2)==="is"){v=v.slice(2);if(v==="Mac"||v==="PC"){continue}d.Browser=d.Insert(new String(v),a);var r=new RegExp(".*(Version/| Trident/.*; rv:)((?:\\d+\\.)+\\d+)|.*("+v+")"+(v=="MSIE"?" ":"/")+"((?:\\d+\\.)*\\d+)|(?:^|\\(| )([a-z][-a-z0-9._: ]+|(?:Apple)?WebKit)/((?:\\d+\\.)+\\d+)");var u=r.exec(e)||["","","","unknown","0.0"];d.Browser.name=(u[1]!=""?v:(u[3]||u[5]));d.Browser.version=u[2]||u[4]||u[6];break}}}try{d.Browser.Select({Safari:function(j){var i=parseInt((String(j.version).split("."))[0]);if(i>85){j.webkit=j.version}if(i>=538){j.version="8.0"}else{if(i>=537){j.version="7.0"}else{if(i>=536){j.version="6.0"}else{if(i>=534){j.version="5.1"}else{if(i>=533){j.version="5.0"}else{if(i>=526){j.version="4.0"}else{if(i>=525){j.version="3.1"}else{if(i>500){j.version="3.0"}else{if(i>400){j.version="2.0"}else{if(i>85){j.version="1.0"}}}}}}}}}}j.webkit=(navigator.appVersion.match(/WebKit\/(\d+)\./))[1];j.isMobile=(navigator.appVersion.match(/Mobile/i)!=null);j.noContextMenu=j.isMobile},Firefox:function(j){if((j.version==="0.0"||k.match(/Firefox/)==null)&&navigator.product==="Gecko"){var m=k.match(/[\/ ]rv:(\d+\.\d.*?)[\) ]/);if(m){j.version=m[1]}else{var i=(navigator.buildID||navigator.productSub||"0").substr(0,8);if(i>="20111220"){j.version="9.0"}else{if(i>="20111120"){j.version="8.0"}else{if(i>="20110927"){j.version="7.0"}else{if(i>="20110816"){j.version="6.0"}else{if(i>="20110621"){j.version="5.0"}else{if(i>="20110320"){j.version="4.0"}else{if(i>="20100121"){j.version="3.6"}else{if(i>="20090630"){j.version="3.5"}else{if(i>="20080617"){j.version="3.0"}else{if(i>="20061024"){j.version="2.0"}}}}}}}}}}}}j.isMobile=(navigator.appVersion.match(/Android/i)!=null||k.match(/ Fennec\//)!=null||k.match(/Mobile/)!=null)},Chrome:function(i){i.noContextMenu=i.isMobile=!!navigator.userAgent.match(/ Mobile[ \/]/)},Opera:function(i){i.version=opera.version()},Edge:function(i){i.isMobile=!!navigator.userAgent.match(/ Phone/)},MSIE:function(j){j.isMobile=!!navigator.userAgent.match(/ Phone/);j.isIE9=!!(document.documentMode&&(window.performance||window.msPerformance));MathJax.HTML.setScriptBug=!j.isIE9||document.documentMode<9;MathJax.Hub.msieHTMLCollectionBug=(document.documentMode<9);if(document.documentMode<10&&!s.params.NoMathPlayer){try{new ActiveXObject("MathPlayer.Factory.1");j.hasMathPlayer=true}catch(m){}try{if(j.hasMathPlayer){var i=document.createElement("object");i.id="mathplayer";i.classid="clsid:32F66A20-7614-11D4-BD11-00104BD3F987";g.appendChild(i);document.namespaces.add("m","http://www.w3.org/1998/Math/MathML");j.mpNamespace=true;if(document.readyState&&(document.readyState==="loading"||document.readyState==="interactive")){document.write('<?import namespace="m" implementation="#MathPlayer">');j.mpImported=true}}else{document.namespaces.add("mjx_IE_fix","http://www.w3.org/1999/xlink")}}catch(m){}}}})}catch(c){console.error(c.message)}d.Browser.Select(MathJax.Message.browsers);if(h.AuthorConfig&&typeof h.AuthorConfig.AuthorInit==="function"){h.AuthorConfig.AuthorInit()}d.queue=h.Callback.Queue();d.queue.Push(["Post",s.signal,"Begin"],["Config",s],["Cookie",s],["Styles",s],["Message",s],function(){var i=h.Callback.Queue(s.Jax(),s.Extensions());return i.Push({})},["Menu",s],s.onLoad(),function(){MathJax.isReady=true},["Typeset",s],["Hash",s],["MenuZoom",s],["Post",s.signal,"End"])})("MathJax")}};
" type="text/javascript" async></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "tex2jax": {
        "inlineMath": [["$", "$"], ["\\(", "\\)"]]
      }
    });
  </script>
</head>
<body>
<header>
<h1 class="title">Stan モデリング言語: ユーザーガイド・リファレンスマニュアル</h1>
<h2 class="author">Stan Development Team</h2>
<h2 class="author">翻訳: stan-jaチーム</h2>
<p>なお最新のpdfは<a href="https://github.com/stan-ja/stan-ja/releases">こちら</a>からダウンロードできます.</p>
<h3 class="date">Stan Version 2.9.0</h3>
</header>
<nav id="TOC">
<ul>
<li><a href="#overview">1. Overview</a><ul>
<li><a href="#stan-home-page">1.1 Stan Home Page</a></li>
<li><a href="#stanのインターフェース">1.2 Stanのインターフェース</a></li>
<li><a href="#stanのプログラム">1.3 Stanのプログラム</a></li>
<li><a href="#コンパイルとstanプログラムの実行">1.4 コンパイルとStanプログラムの実行</a></li>
<li><a href="#サンプリング">1.5 サンプリング</a></li>
<li><a href="#最適化">1.6 最適化</a></li>
<li><a href="#変分推論">1.7 変分推論</a></li>
</ul></li>
<li><a href="#ソフトウェア開発としてのモデル構築">2. ソフトウェア開発としてのモデル構築</a><ul>
<li><a href="#バージョン管理を使う">2.1. バージョン管理を使う</a></li>
<li><a href="#再現可能にする">2.2 再現可能にする</a></li>
<li><a href="#可読性を高める">2.3. 可読性を高める</a></li>
<li><a href="#データを探検する">2.4. データを探検する</a></li>
<li><a href="#トップダウンでデザインし-ボトムアップでコーディングする">2.5. トップダウンでデザインし, ボトムアップでコーディングする</a></li>
<li><a href="#シミュレートされたデータでフィットさせる">2.6. シミュレートされたデータでフィットさせる</a></li>
<li><a href="#表示することでデバッグする">2.7. 表示することでデバッグする</a></li>
<li><a href="#コメント">2.8. コメント</a></li>
</ul></li>
<li><a href="#データ型">3.データ型</a><ul>
<li><a href="#基本データ型">3.1. 基本データ型</a></li>
<li><a href="#制約付きデータ型">3.2. 制約付きデータ型</a></li>
<li><a href="#代入引数の受け渡し">3.3. 代入・引数の受け渡し</a></li>
</ul></li>
<li><a href="#コンテナ値の入れ物-array-vector-and-matrix">4. コンテナ(値の入れ物): <code>array</code>, <code>vector</code>, and <code>matrix</code></a><ul>
<li><a href="#vector-and-matrix">4.1. <code>vector</code> and <code>matrix</code></a></li>
<li><a href="#array">4.2. <code>array</code></a></li>
<li><a href="#効率についての考察">4.3. 効率についての考察</a></li>
</ul></li>
<li><a href="#multiple-indexingとrange-indexing">5. multiple indexingとrange indexing</a><ul>
<li><a href="#multiple-indexing">5.1. Multiple Indexing</a></li>
<li><a href="#range-indexを使ったアクセスslicing">5.2. range indexを使ったアクセス（slicing）</a></li>
<li><a href="#代入文の左辺でmultiple-indexingを使う">5.3. 代入文の左辺でmultiple indexingを使う</a></li>
<li><a href="#vectorやmatrixに対するmultiple-index">5.4. <code>vector</code>や<code>matrix</code>に対するmultiple index</a></li>
<li><a href="#パラメータと固定値を含んだmatrix">5.5. パラメータと固定値を含んだ<code>matrix</code></a></li>
</ul></li>
<li><a href="#回帰モデル">6. 回帰モデル</a><ul>
<li><a href="#線形回帰">6.1. 線形回帰</a></li>
<li><a href="#係数とスケールの事前分布">6.2. 係数とスケールの事前分布</a></li>
<li><a href="#ロバストノイズモデル">6.3. ロバストノイズモデル</a></li>
<li><a href="#ロジスティック回帰とプロビット回帰">6.4. ロジスティック回帰とプロビット回帰</a></li>
<li><a href="#多項ロジット回帰">6.5. 多項ロジット回帰</a></li>
<li><a href="#中央化されたベクトルへのパラメータ化">6.6. 中央化されたベクトルへのパラメータ化</a></li>
<li><a href="#順序ロジスティック回帰と順序プロビット回帰">6.7. 順序ロジスティック回帰と順序プロビット回帰</a></li>
<li><a href="#階層ロジスティック回帰">6.8. 階層ロジスティック回帰</a></li>
<li><a href="#階層事前分布">6.9. 階層事前分布</a></li>
<li><a href="#項目反応理論モデル">6.10. 項目反応理論モデル</a></li>
<li><a href="#識別可能性のための事前分布">6.11. 識別可能性のための事前分布</a></li>
<li><a href="#階層モデルにおける多変量の事前分布">6.12. 階層モデルにおける多変量の事前分布</a></li>
<li><a href="#予測-フォアキャストとバックキャスト">6.13. 予測, フォアキャストとバックキャスト</a></li>
<li><a href="#多変量の結果変数">6.14. 多変量の結果変数</a></li>
</ul></li>
<li><a href="#時系列モデル">7. 時系列モデル</a><ul>
<li><a href="#自己回帰モデル">7.1. 自己回帰モデル</a></li>
<li><a href="#時間的不等分散性のモデリング">7.2. 時間的不等分散性のモデリング</a></li>
<li><a href="#移動平均モデル">7.3. 移動平均モデル</a></li>
<li><a href="#自己回帰移動平均モデル">7.4. 自己回帰移動平均モデル</a></li>
<li><a href="#確率的ボラティリティモデル">7.5. 確率的ボラティリティモデル</a></li>
<li><a href="#隠れマルコフモデル">7.6. 隠れマルコフモデル</a></li>
</ul></li>
<li><a href="#欠測データと部分的に既知のパラメータ">8. 欠測データと部分的に既知のパラメータ</a><ul>
<li><a href="#欠測データ">8.1. 欠測データ</a></li>
<li><a href="#部分的に既知のパラメータ">8.2. 部分的に既知のパラメータ</a></li>
<li><a href="#効率性についての注意">8.3. 効率性についての注意</a></li>
<li><a href="#因子分析の負荷行列">8.4. 因子分析の負荷行列</a></li>
</ul></li>
<li><a href="#切断あるいは打ち切りデータ">9. 切断あるいは打ち切りデータ</a><ul>
<li><a href="#切断分布">9.1. 切断分布</a></li>
<li><a href="#切断データ">9.2. 切断データ</a></li>
<li><a href="#打ち切りデータ">9.3 打ち切りデータ</a></li>
</ul></li>
<li><a href="#有限混合分布">10. 有限混合分布</a><ul>
<li><a href="#潜在離散値のパラメータ化">10.1. 潜在離散値のパラメータ化</a></li>
<li><a href="#負担率パラメータを総和で消去">10.2. 負担率パラメータを総和で消去</a></li>
<li><a href="#指数の和の対数-対数軸での線形の和">10.3. 指数の和の対数: 対数軸での線形の和</a></li>
<li><a href="#混合分布のベクトル化">10.4. 混合分布のベクトル化</a></li>
<li><a href="#ゼロ過剰モデルとハードルモデル">10.5. ゼロ過剰モデルとハードルモデル</a></li>
</ul></li>
<li><a href="#測定誤差とメタアナリシス">11. 測定誤差とメタアナリシス</a><ul>
<li><a href="#ベイズ測定誤差モデル">11.1. ベイズ測定誤差モデル</a></li>
<li><a href="#メタアナリシス">11.2. メタアナリシス</a></li>
</ul></li>
<li><a href="#潜在離散パラメータ">12. 潜在離散パラメータ</a><ul>
<li><a href="#周辺化の利点">12.1 周辺化の利点</a></li>
<li><a href="#変化点モデル">12.2 変化点モデル</a></li>
<li><a href="#標識再捕獲モデル">12.3 標識再捕獲モデル</a></li>
<li><a href="#データ符号化と診断正答率のモデル">12.4 データ符号化と診断正答率のモデル</a></li>
</ul></li>
<li><a href="#まばらなデータ構造と不ぞろいなデータ構造">13. まばらなデータ構造と不ぞろいなデータ構造</a><ul>
<li><a href="#まばらなデータ構造">13.1 まばらなデータ構造</a></li>
<li><a href="#不ぞろいなデータ構造">13.2. 不ぞろいなデータ構造</a></li>
</ul></li>
<li><a href="#クラスタリングモデル">14. クラスタリングモデル</a><ul>
<li><a href="#ソフトk-means法">14.1. ソフト<span class="math inline"><em>K</em></span>-Means法</a></li>
<li><a href="#クラスタリングにおけるベイズ推定のむずかしさ">14.2. クラスタリングにおけるベイズ推定のむずかしさ</a></li>
<li><a href="#ナイーブベイズ分類法およびクラスタリング法">14.3. ナイーブベイズ分類法およびクラスタリング法</a></li>
<li><a href="#潜在ディリクレ配分法">14.4. 潜在ディリクレ配分法</a></li>
</ul></li>
<li><a href="#方向-回転-超球面">16. 方向, 回転, 超球面</a><ul>
<li><a href="#単位ベクトル">16.1. 単位ベクトル</a></li>
<li><a href="#円-球面-超球面">16.2. 円, 球面, 超球面</a></li>
<li><a href="#制約のないパラメータへの変換">16.3. 制約のないパラメータへの変換</a></li>
<li><a href="#単位ベクトルと回転">16.4. 単位ベクトルと回転</a></li>
<li><a href="#日と年の円周表現">16.5. 日と年の円周表現</a></li>
</ul></li>
<li><a href="#再パラメータ化と変数変換">17. 再パラメータ化と変数変換</a><ul>
<li><a href="#理論的かつ実践的な背景">17.1. 理論的かつ実践的な背景</a></li>
<li><a href="#再パラメータ化">17.2. 再パラメータ化</a></li>
<li><a href="#変数変換">17.3. 変数変換</a></li>
<li><a href="#変化する境界をもつvector">17.4. 変化する境界をもつ<code>vector</code></a></li>
</ul></li>
<li><a href="#自作の確率分布関数">18. 自作の確率分布関数</a><ul>
<li><a href="#例">18.1.　例</a></li>
</ul></li>
<li><a href="#ユーザー定義関数">19. ユーザー定義関数</a><ul>
<li><a href="#基本的な関数">19.1. 基本的な関数</a></li>
<li><a href="#ステートメント文としての関数">19.2. ステートメント（文）としての関数</a></li>
<li><a href="#対数確率を累積する機能にアクセスする関数">19.3. 対数確率を累積する機能にアクセスする関数</a></li>
<li><a href="#乱数生成器として振る舞う関数">19.4. 乱数生成器として振る舞う関数</a></li>
<li><a href="#ユーザー定義の確率分布関数">19.5. ユーザー定義の確率分布関数</a></li>
<li><a href="#多重定義の関数">19.6. 多重定義の関数</a></li>
<li><a href="#関数にドキュメントをつける">19.7. 関数にドキュメントをつける</a></li>
<li><a href="#関数の型の要約">19.8. 関数の型の要約</a></li>
<li><a href="#再帰関数">19.9. 再帰関数</a></li>
</ul></li>
<li><a href="#微分方程式を解く">20. 微分方程式を解く</a><ul>
<li><a href="#例単純な調和振動子">20.1. 例：単純な調和振動子</a></li>
<li><a href="#常微分方程式の系をコーディングする">20.2. 常微分方程式の系をコーディングする</a></li>
<li><a href="#測定エラーモデル">20.3. 測定エラーモデル</a></li>
<li><a href="#stiffなode">20.4. stiffなODE</a></li>
<li><a href="#odeソルバーの制御パラメータ">20.5. ODEソルバーの制御パラメータ</a></li>
</ul></li>
<li><a href="#問題のある事後分布">21. 問題のある事後分布</a><ul>
<li><a href="#回帰での予測変数の共線性">21.1. 回帰での予測変数の共線性</a></li>
<li><a href="#混合分布モデルでのラベルスイッチング">21.2. 混合分布モデルでのラベルスイッチング</a></li>
<li><a href="#混合分布モデルで成分がつぶれる">21.3. 混合分布モデルで成分がつぶれる</a></li>
<li><a href="#上下限のない密度での事後分布">21.4. 上下限のない密度での事後分布</a></li>
<li><a href="#上下限のないパラメータでの事後分布">21.5. 上下限のないパラメータでの事後分布</a></li>
<li><a href="#一様な事後分布">21.6. 一様な事後分布</a></li>
<li><a href="#問題のある事前分布によるサンプリングの難しさ">21.7. 問題のある事前分布によるサンプリングの難しさ</a></li>
</ul></li>
<li><a href="#再現性reproducibilityについて">23.再現性(Reproducibility)について</a></li>
<li><a href="#ステートメント-文">27. ステートメント (文)</a><ul>
<li><a href="#代入ステートメント">27.1. 代入ステートメント</a></li>
</ul></li>
<li><a href="#array型の演算">34. Array型の演算</a><ul>
<li><a href="#arrayから1つの値の作成">34.1 Arrayから1つの値の作成</a></li>
</ul></li>
<li><a href="#点推定">55. 点推定</a><ul>
<li><a href="#最尤推定">55.1. 最尤推定</a></li>
<li><a href="#罰則付き最尤推定">55.2. 罰則付き最尤推定</a></li>
<li><a href="#事後最頻値の推定">55.3. 事後最頻値の推定</a></li>
<li><a href="#事後平均値の推定">55.4. 事後平均値の推定</a></li>
<li><a href="#事後中央値の推定">55.5. 事後中央値の推定</a></li>
<li><a href="#推定値の誤差-バイアス偏り-分散">55.6 推定値の誤差, バイアス（偏り）, 分散</a></li>
</ul></li>
<li><a href="#ベイズデータ解析">56. ベイズデータ解析</a><ul>
<li><a href="#ベイズモデリング">56.1 ベイズモデリング</a></li>
<li><a href="#ベイズ推定">56.2 ベイズ推定</a></li>
</ul></li>
<li><a href="#マルコフ連鎖モンテカルロサンプリング">57. マルコフ連鎖モンテカルロサンプリング</a><ul>
<li><a href="#モンテカルロサンプリング">57.1. モンテカルロサンプリング</a></li>
<li><a href="#マルコフ連鎖モンテカルロサンプリング-1">57.2. マルコフ連鎖モンテカルロサンプリング</a></li>
<li><a href="#初期化と収束モニタリング">57.3. 初期化と収束モニタリング</a></li>
<li><a href="#有効サンプルサイズ">57.4. 有効サンプルサイズ</a></li>
</ul></li>
<li><a href="#ハミルトニアンモンテカルロサンプリング">60. ハミルトニアンモンテカルロサンプリング</a><ul>
<li><a href="#ハミルトニアンモンテカルロ">60.1 ハミルトニアンモンテカルロ</a></li>
<li><a href="#hmcアルゴリズムのパラメータ">60.2 HMCアルゴリズムのパラメータ</a></li>
</ul></li>
</ul>
</nav>
<h2 id="overview">1. Overview</h2>
<p>このドキュメントは統計モデリング言語であるStanのユーザーガイド兼リファレンスマニュアルです. この導入の章ではStanの全体像について紹介しますが, 残りの章ではモデルの実際のプログラミングや, Stanのモデリング言語としての詳細な解説を, コードやデータの型も含めて, 実践的な解説を行います.</p>
<h3 id="stan-home-page">1.1 Stan Home Page</h3>
<p>最新のコード, 例, マニュアル, バグレポート, 機能追加の要望など, Stanに関する情報は下記のリンクにあるStanのホームページから参照できます.</p>
<p><a href="http://mc-stan.org" class="uri">http://mc-stan.org</a></p>
<h3 id="stanのインターフェース">1.2 Stanのインターフェース</h3>
<p>Stan Projectでは３つのインターフェースをプロジェクトの一部としてサポートしています. モデリング部分やその使い方に関しては３つのインターフェースで共通していてるので, このマニュアルはその３つに共通するモデリング言語としてのマニュアルとなります. 初期化やサンプリング, チューニング方法についてはすべてのインターフェースで共通していて, また事後分布を分析する機能についてもおおむね共通しています.</p>
<p>提供されているすべてのインターフェースについて, getting-started guideやドキュメントが完全なソースコードと共に提供されています.</p>
<h4 id="cmdstan">CmdStan</h4>
<p>CmdStanはコマンドラインからStanを利用することを可能にします. ある意味でCmdStanはStanのリファレンス実装ともいえます. もともとCmdStanのドキュメントはこのドキュメントの一部でしたが, 今では独立したドキュメントとなっています. CmdStanのホームページは以下になります.</p>
<p><a href="http://mc-stan.org/cmdstan.html" class="uri">http://mc-stan.org/cmdstan.html</a></p>
<h4 id="rstan">RStan</h4>
<p>RStanはRにおけるStanのインターフェースです. R2WinBUGSやR2jagsでは, Rから外部のソフトウェアとしてWinBUGSやJAGSを呼び出していました. RStanではそうではなく, むしろRのメモリを通じたインターフェースになっています. RStanのホームページは以下になります.</p>
<p><a href="http://mc-stan.org/cmdstan.html">http://mc-stan.org/rstan.html</a></p>
<h4 id="pystan">PyStan</h4>
<p>PyStanはPythonにおけるStanのインターフェースです. 外側のStanを呼び出すというよりは, RStanと同様にPythonのメモリレベルのインターフェースです. PyStanのホームページは以下になります.</p>
<p><a href="http://mc-stan.org/pystan.html" class="uri">http://mc-stan.org/pystan.html</a></p>
<h5 id="matlabstan">MatlabStan</h5>
<p>MatlabStanはMATLABにおけるStanへのインターフェースです. RstanやPyStanとは異なり, 現状MatlabStanはCmdStanのラッパーです. MatlabStanのホームページは以下になります.</p>
<p><a href="http://mc-stan.org/matlab-stan.html" class="uri">http://mc-stan.org/matlab-stan.html</a></p>
<h5 id="stan.jl">Stan.jl</h5>
<p>Stan.jlはJuliaにおけるStanのインターフェースです. これもMatlabStanと同様に, CmdStanのラッパーです. Stan.jlのホームページは以下になります.</p>
<p><a href="http://mc-stan.org/julia-stan.html" class="uri">http://mc-stan.org/julia-stan.html</a></p>
<h5 id="statastan">StataStan</h5>
<p>StataStanはStataにおけるStanのインターフェースです. MatlabStan, Stan.jl と同様にこれもCmdStanのラッパーです. StataStanのホームページは以下になります.</p>
<p><a href="http://mc-stan.org/stata-stan.html" class="uri">http://mc-stan.org/stata-stan.html</a></p>
<h3 id="stanのプログラム">1.3 Stanのプログラム</h3>
<p>Stanのプログラムでは条件付き確率分布 <span class="math inline"><em>p</em>(<em>θ</em> ∣ <em>y</em>, <em>x</em>)</span> を通して統計モデルが定義されます. ここで<span class="math inline"><em>θ</em></span>はモデルに組み込まれる, 一連の未知の変数(例： モデルのパラメータ, 隠れ変数, 欠測データ, 将来の予測値)です. <span class="math inline"><em>y</em></span>はモデルに組み込まれる, 一連の値が得られている変数です. <span class="math inline"><em>x</em></span>はモデルに組み込まれない, 一連の説明変数と定数です（例：サイズ, ハイパーパラメータ）.</p>
<p>Stanのプログラムは, 変数の型宣言と文（ステートメント）からなります. 変数の型には整数, 実数, ベクトル, 行列はもちろん, その他の型の（多次元な）配列があり, それぞれ値を制約することもできます. 変数は, その役割に応じて, <code>data</code>・<code>transformed data</code>・<code>parameters</code>・<code>transformed parameters</code>・<code>generated quantities</code>というブロックの中で宣言されます. また制約のないローカル変数を各ブロックで定義することもできます.</p>
<p><code>transformed data</code>, <code>transformed parameters</code>, <code>generated quantities</code>ブロックは, そのブロックで宣言された変数を定義する文を含んでいます. 特別な<code>model</code>ブロックはモデルの対数尤度を定義する文で構成されています.</p>
<p>また<code>model</code>ブロックの中では, Stanが内部で持っている対数確率をインクリメントするための略記として, BUGS風のサンプリング記法を利用できます. 対数確率の変数には直接アクセスすることもでき, ユーザによる確率関数の定義や変数変換のヤコビアンの調整を可能にします.</p>
<h4 id="変数の制約">変数の制約</h4>
<p>変数の制約はStanにおいて, 特にパラメータについて重要です. Stanが効率的にサンプリングをするためには, 宣言時の制約を満たすパラメータの値が, <code>model</code>ブロックにおいて台（サポート）を持つ必要があります（言い換えると, ゼロでない事後密度をもつ必要があります）.</p>
<p><code>data</code>ブロックと<code>transformed data</code>ブロックにおける制約は, データの入力および変換のエラーチェックのために使われるだけです. <code>transformed parameters</code>ブロックにおける制約は, <code>parameters</code>ブロックにおける制約と同じように満たされなければなりません. さもなくばサンプリングが完全にランダムウォークになってしまうか, 失敗することでしょう. <code>generated quantities</code>における制約は必ず満たされる必要があり, さもなくばサンプリングが完全に停止することでしょう. なぜなら<code>generated quantities</code>ブロックが評価される時点で抽出されたサンプルを棄却するのは遅すぎるからです.</p>
<h4 id="実行順序">実行順序</h4>
<p>Stanの文は手続きとして解釈されるため, 順序が重要です. 文は最低でも変数に対する値の代入を伴います. 一連の文（と必要に応じたローカル変数の定義）によりブロックが構成されます. そしてStanもまたRやBUGSで使われている有限なfor-eachのループを提供しています.</p>
<h4 id="確率的プログラミング言語">確率的プログラミング言語</h4>
<p>Stanは手続き型の確率的プログラミング言語です. DSL（domain-specific language, ドメイン固有言語）の一種です. それは統計的な推論という特定の目的のために開発されたという意味です.</p>
<p>Stanは確率変数を正真正銘の第一級オブジェクトとして扱うという意味で確率的プログラミング言語です. Stanでは変数を確率変数として扱いますが, あるものは観測されていたり, またあるものは未知で推定の必要がある（もしくは事後の予測推論に使う必要がある）ものです. 観測された確率変数は<code>data</code>ブロックの中で宣言され, 観測されていない変数はパラメータとして宣言されます（<code>parameters</code>ブロックで宣言された変数や, それに依存する, <code>transformed parameters</code>ブロック内の変数・<code>generated quantities</code>ブロック内の変数・ローカル変数も含みます）. また観測されていない確率変数の周辺分布や同時分布からサンプリングすることもできますし, 確率変数の平均や分散を推定することもできますし, 二次的な事後の予測推論ではそれらの値をプラグインすることもできます.</p>
<p>StanはCやFortranと同様（C++, R, PythonやJavaなどと部分的に同じ）の手続き型プログラミング言語です. これは, 代入, ループ, 条件分岐, ローカル変数, オブジェクトレベルの関数や配列のようなデータ構造に基づいているという意味です. 関数型言語と比較すれば, 関数型言語では典型的には高階関数（汎関数）が許可され, しばしばリフレクション（プログラムの中でそのプログラムに含まれる型や変数/メソッドの情報を参照/操作できるようにする仕組み）が許可されています. 一方で純粋な関数型言語では代入が全く許可されていません. またオブジェクト指向言語はより一般的なデータ型を取り入れており, 動的に関数などが選ばれて実行されます.</p>
<p>Stan言語は, CやRと同様に, チャーチ＝チューリング完全 [Church (1936); Turing (1936); Hopcroft and Motwani (2006)]です. それはチューリングマシン（もしくはC）で計算可能ないかなるプログラムもStanで実装できることを意味しています（もちろんその道は険しいですが）. ちなみにチューリング完全であるために求められるのは, ループと条件分岐, そしてループの中でサイズが変更できる配列のみです.</p>
<h3 id="コンパイルとstanプログラムの実行">1.4 コンパイルとStanプログラムの実行</h3>
<p>Stanのコードはまず最初にStanのコンパイラ<code>stanc</code>によってC++の言語へと変換され, そしてそのC++はプラットフォーム依存の単独で実行可能な形式に変換されます. またStanはWindows, Mac OS X, Linuxなど様々な実行可能形式を出力することができます. <a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> Stanの実行ファイルを実行すると, まず既知の <span class="math inline"><em>y</em></span> と <span class="math inline"><em>x</em></span> を読み込み, その妥当性を評価します. そして（独立ではない）同一分布に従うサンプルの列 <span class="math inline"><em>θ</em><sup>(1)</sup>, <em>θ</em><sup>(2)</sup>, …</span> を生成します. これらは各々, 周辺分布 <span class="math inline"><em>p</em>(<em>θ</em> ∣ <em>y</em>, <em>x</em>)</span> に従います.</p>
<h3 id="サンプリング">1.5 サンプリング</h3>
<p>連続値をとるパラメータに対してStanは Hamiltonian Monte Carlo (HMC) Sampling (Duane et al., 1987; Neal, 1994, 2011) というある種のマルコフ連鎖モンテカルロ（MCMC）サンプリング(Metropolis et al., 1953)を用います. Stanは離散値をとるパラメータのサンプリングは提供していません. 観測値としては離散値を直接利用することができますが, 離散値をとるパラメータはモデルから周辺化消去されている必要があります. 10章と12章では, いかにして有限の離散値をとるパラメータを和をとってモデルから消去するか, そしてその消去がもたらすサンプリング効率の大幅な向上について議論します.</p>
<p>HMCは対数確率関数の勾配を使うことで, 定常分布への収束とパラメタの探索の効率化を促進しています. 推定すべき量のベクトル<span class="math inline"><em>θ</em></span>は仮想的な粒子の位置と解釈されます. それぞれのiterationにおいて, ランダムな運動量を生成し, （負の）対数確率関数を決めているポテンシャルエネルギー内の粒子の経路をシミュレーションします. ハミルトンの分解からこのポテンシャルの勾配が運動量の変化を与え, この運動量が位置の変化を与えることが分かります. この時間的に連続な変化は, 時間をシミュレーションが容易な離散値とみなすleapfrogアルゴリズムによって近似されます. 続けて, 様々なシミュレーションエラーを直し, マルコフ連鎖の遷移において詳細釣り合い条件を満たすようにするために, メトロポリス法の棄却ステップが適用されます (Metropolis et al., 1953; Hastings, 1970).</p>
<p>基本的なユークリッド空間上のHamiltonian Monte Carlo法には, 振る舞いに対して大きな影響を与える3つの「チューニングすべき」パラメータがあります. Stanのサンプラーはそれらを手動で設定する方法と, ユーザを介さずに自動で設定する方法の両方を提供しています.</p>
<p>1つ目のチューニングパラメータはステップサイズで, ハミルトニアンが測定される時間的な単位（すなわち, 離散化の単位）です. Stanではユーザが指定したステップサイズに設定することもできますが, 双対平均化法(Nesterov, 2009; Hoffman and Gelman, 2011, 2014)によってwarmupの段階で最適なステップサイズを推定することができます. どちらの場合もとり得るステップサイズの区間からステップサイズを抽出するために, 追加のランダム化が適用されます(Neal, 2011).</p>
<p>2つ目のチューニングパラメータはiterationごとのステップ数で, このステップ数と先ほどのステップサイズの積によりハミルトニアンに関する全シミュレーション時間が決まります. Stanはこのステップ数も特定のステップ数を指定することができますが, No-U-Turn (NUTS) sampler (Hoffman and Gelman, 2011, 2014)を使うと, このステップ数をサンプリング中に自動的に決定することができます.</p>
<p>3つ目のチューニングパラメータは仮想粒子の質量行列です. Stanはwarmupの間に対角行列となる質量行列や完全な質量行列を推定するように設定できます. また, 将来的にはユーザが指定した質量行列をサポートする予定です. 対角行列となる質量行列を推定すると, 未知パラメータ系列<span class="math inline"><em>θ</em></span>の各要素<span class="math inline"><em>θ</em><sub><em>k</em></sub></span>のスケールは正規化されます. 一方, 完全な質量行列を推定すると, スケーリングと回転の両方を考慮することができますが, <a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>行列演算のためにleapfrogの各ステップにおいて, より多くのメモリと計算を要します.</p>
<h4 id="収束のモニタリングと有効サンプルサイズ">収束のモニタリングと有効サンプルサイズ</h4>
<p>マルコフ連鎖から得られるサンプルは, その連鎖が定常分布に収束したあとに, 周辺分布 <span class="math inline"><em>p</em>(<em>θ</em> ∣ <em>y</em>, <em>x</em>)</span> から抽出されます. MCMCが収束したかどうかを判定する方法は複数ありますが, 残念ながら, そうしたテストをパスしたからといって収束が保証されるものではありません. Stanにおいておすすめの方法は, いくつか異なるパラメータの初期値を用いてランダムに初期化した複数のマルコフ連鎖を走らせ, warmup/adaptationの期間のサンプルを捨て, 各チェーンの残りのサンプルを半分に分割することで, 潜在的なスケールの逓減に関する統計量 <span class="math inline">$\hat{R}$</span> (Gelman and Rubin, 1992)を計算することです. もしその結果, 十分なサイズの有効サンプルが得られなければ, iterationの数を2倍にして再びやり直します. warmupなど全て含めてやり直しです. <a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
<p><span class="math inline"><em>M</em></span> 個の独立に抽出されたサンプルに基づいて母平均を推定する場合, その推定誤差は <span class="math inline">$\frac{1}{\sqrt{M}}$</span> に比例します. もしMCMCを使って抽出されたサンプルのように典型的にサンプル間に正の相関があれば, その推定誤差は <span class="math inline">$\frac{1}{\sqrt{\text{N\_EFF}}}$</span> に比例します. ここで<span class="math inline">N_EFF</span> は有効サンプルサイズです. したがって, 目下の推定や推論のタスクに対し十分な大きさになるまで, 有効サンプルサイズ（やその推定値）をモニターするのは標準的な習慣です.</p>
<h4 id="ベイズ推定とモンテカルロ法">ベイズ推定とモンテカルロ法</h4>
<p>Stanはフルベイズの推定をサポートするために開発されました. ベイズ推定は下記の正規化されていないベイズ則</p>
<p><br /><span class="math display"><em>p</em>(<em>θ</em> ∣ <em>y</em>, <em>x</em>) ∝ <em>p</em>(<em>y</em> ∣ <em>θ</em>, <em>x</em>)<em>p</em>(<em>θ</em>, <em>x</em>)</span><br /></p>
<p>に基づいています. これは, データ <span class="math inline"><em>y</em></span>（と定数<span class="math inline"><em>x</em></span>）が与えられたもとでのパラメータ <span class="math inline"><em>θ</em></span> の事後分布 <span class="math inline"><em>p</em>(<em>θ</em> ∣ <em>y</em>, <em>x</em>)</span> が尤度 <span class="math inline"><em>p</em>(<em>y</em> ∣ <em>θ</em>, <em>x</em>)</span> と事前分布 <span class="math inline"><em>p</em>(<em>θ</em>, <em>x</em>)</span> の積に比例することを表します.</p>
<p>Stanでは, ベイズモデリングは定係数を除いて事後確率関数のコーディングを伴います. ベイズ則から事後確率分布は定係数を除いて尤度関数と事前確率の積と等価です.</p>
<p>フルベイズ推定では, 事後分布<span class="math inline"><em>p</em>(<em>θ</em>|<em>y</em>, <em>x</em>)</span>によるモデル化において, パラメータ<span class="math inline"><em>θ</em></span>の値に関する不確実性が含まれ伝搬されてゆくことになります. このことは, 事後分布からの一連のサンプルについて推定を行うことで実現できます. その際には, 事後平均や事後確率区間等を関心のある量のためにプラグイン推定値として利用したり, 事後分布に基づいて事象の結果や未観測のデータ値を予測したりします.</p>
<h3 id="最適化">1.6 最適化</h3>
<p>Stanは最適化に基づく推論もサポートしています. 与えられた事後分布 <span class="math inline"><em>p</em>(<em>θ</em> ∣ <em>y</em>)</span> に対して, Stanは以下で定義される事後分布の最頻値 <span class="math inline"><em>θ</em><sup> * </sup></span> を見つけることができます.</p>
<p><br /><span class="math display"><em>θ</em><sup> * </sup> = argmax<sub><em>θ</em></sub><em>p</em>(<em>θ</em> ∣ <em>y</em>)</span><br /></p>
<p>ここで <span class="math inline">argmax<sub><em>v</em></sub><em>f</em>(<em>v</em>)</span> という記法は 関数 <span class="math inline"><em>f</em>(<em>v</em>)</span> を最大化する <span class="math inline"><em>v</em></span> の値を選ぶことを意味します.</p>
<p>もし事前分布が一様分布ならば, 事後分布の最頻値は, パラメータの最尤推定値（maximum likelifood estimate, MLE）に対応します. また, 事前分布が一様分布でなければ, この事後分布の最頻値はしばしばMAP（maximum a posterior）推定値と呼ばれます.</p>
<p>最適化においては, 変数の制約に由来するいかなる変換のヤコビアンも無視されます. 多くの最適化問題でより効率的に計算するには, 変数宣言時における上下限の制約を取り除いて, 代わりに台の範囲外となる無効な解を許さないように<code>model</code>ブロックで棄却するようにします.</p>
<h4 id="点推定値による推論">点推定値による推論</h4>
<p>推定値<span class="math inline"><em>θ</em><sup> * </sup></span> はいわゆる「点推定値」と呼ばれています. これは事後分布を分布と言うよりむしろ一つの点として要約することを意味します. もちろん点推定値それ自体は推定のばらつきを考慮しません. 事後予測確率<span class="math inline">$p(\tilde{y} \mid y)$</span>は, データ <span class="math inline"><em>y</em></span> が与えられた場合の事後最頻値を用いて, <span class="math inline">$p(\tilde{y} | \theta^\ast)$</span> のように作ることができます. しかし, これらは事後確率に不確実性を考慮しないため, たとえ事前分布を持っていたとしてもベイズ推定ではありません. もし, 事後分布の分散が小さく, その平均が最頻値に近ければ, 点推定の結果はフルベイズの推定結果と非常に近くなります.</p>
<h3 id="変分推論">1.7 変分推論</h3>
<p>Stanはベイズ推定を近似する変分推論もサポートしています(Jordan et al., 1999; Wainwright and Jordan, 2008). 変分推論では近似した事後分布を用いて, 事後分布の平均値と不確実性を推定することができます. 近似した事後分布は, パラメトリックな分布が真の事後分布にあてはまるように最適化して求めます. 変分推論は, 特に機械学習の分野においてベイズ推定の計算にすさまじい影響を与えてきました. 典型的には変分推論は従来のサンプリングによる推定に比べて速く, 巨大なデータにスケールしやすいという特徴があります(Hoffman et al., 2013).</p>
<p>変分推論は事後分布 <span class="math inline"><em>p</em>(<em>θ</em> ∣ <em>y</em>)</span> をシンプルなパラメトリックな分布 <span class="math inline"><em>q</em>(<em>θ</em> ∣ <em>ϕ</em>)</span> で近似します. このことは真の事後分布との以下で与えられるKullback-Leibler情報量を最小化することに対応します.</p>
<p><br /><span class="math display"><em>ϕ</em><sup> * </sup> = argmin<sub><em>ϕ</em></sub>KL[<em>q</em>(<em>θ</em> ∣ <em>ϕ</em>)‖<em>p</em>(<em>θ</em> ∣ <em>y</em>)]</span><br /></p>
<p>これはベイズ推定の問題を, 解がwell-definedな計量をもつ最適化問題に帰着できることを意味します. 変分推論では, サンプリングに比べてオーダーが異なるほど収束が速いです. 近似の精度はモデルによって異なります. 変分推論が点推定のテクニックではないことに注意すると, 結果は事後分布を近似した分布だということができます.</p>
<p>Stanでは自動微分変分推論（Automatic Differentiation Variational Inference, ADVI）というアルゴリズムが実装されています. このアルゴリズムはStanの変数変換ライブラリや自動微分に関するツールボックスを活用するようにデザインされています(Kucukelbir et al., 2015). ADVIは変分推論のアルゴリズムを導出するのに典型的に必要となるすべての数学を回避し, いかなるStanのモデルにおいても動作します.</p>
<h2 id="ソフトウェア開発としてのモデル構築">2. ソフトウェア開発としてのモデル構築</h2>
<p>Stanで統計モデルの開発を行うということはStanプログラムを書くということを意味し, つまりそれは一種のソフトウェア開発プロセスです. ソフトを開発することは大変です. とても大変です. 動くパーツとその組み合わせが非常にたくさんあるため, とても多くのことが間違った方向に向かうことがあります.</p>
<p>コンピュータプログラムを書くこと固有の複雑さによって引き起こされる諸問題を軽減するために, ソフトウェア開発の営みはデザインされています. 不幸なことに, 多くの方法論は独断的な考えかセコいテクニック, もしくはその両方に話題がそれます. 開発者への堅実で実践的なアドバイスで私達がオススメできるものは, (Hunt and Thomas, 1999)と(McConnell, 2004)です. この節では彼らのアドバイスのいくつかをまとめようとしています.</p>
<h3 id="バージョン管理を使う">2.1. バージョン管理を使う</h3>
<p>バージョン管理のソフト（SubversionやGitなど）はコーディングしはじめる前に使えるように用意しておくべきです(注1). バージョン管理を学ぶことは大きな投資に見えるかもしれません. しかし, 一つのコマンドで前に作業していたバージョンに戻ることができたり, 今のバージョンと古いバージョンの差分を得ることができますので, その価値はあります. あなたが他の人と作業を共有する必要がある時には, それが紙の上であっても, もっとよいものになります. 作業は独立に行われ, 自動的にマージされるでしょう. Stan自体がどのように開発されているかについては62章を見てください.</p>
<p>(注1): StanはSubversion (SVN)を使ってはじまりましたが, もっと色々なことができるGitに移行しました. GitはSVNができることは全部できるし, もっと色々なことができます. 対価としてはSVNより学習曲線が急なことが挙げられます. 個人用, もしくはとても小さいチームでの開発ならSVNでいいです.</p>
<h3 id="再現可能にする">2.2 再現可能にする</h3>
<p>モデルを動かす時にコマンドライン上で（もしくはRやPythonのような言語を対話的なモードで起動して）コマンドを入力するよりは, スクリプトを書いてモデルにデータを入れて, 必要な事後の解析を試して欲しいです. スクリプトはシェルスクリプトでもRでもPythonでも書けます(訳注1). そのスクリプトはどんな言語でもいいですが, 必要なものはすべて含んで自己完結しているべきです. 設定されているグローバル変数や読み込まれている他のデータに依存するべきではありません.</p>
<p>Stanにおける再現性とそのインターフェースについての完全な情報については22章を見てください.</p>
<p>(訳注1): シェルスクリプトで書く場合はCmdStanを, Rで書く場合はRStanを, Pythonで書く場合はPyStanを使うことになります.</p>
<h4 id="スクリプトはよい文書である">スクリプトはよい文書である</h4>
<p>もし1行のコードのプロジェクトならやりすぎに見えるかもしれませんが, スクリプトはコードの実行の仕方だけではなく, 何が実行されるかについて具体的なドキュメントでもあるべきです.</p>
<h4 id="ランダム化と乱数の種の保存">ランダム化と乱数の種の保存</h4>
<p>乱数を使ったランダム性は再現性を損なわせますが, MCMC法は概念として乱数を利用します. Stanのサンプラーは乱数を使ったランダムな初期化を行うだけでなく, MCMCのiterationごとにも乱数を使ってランダム化しています（例えば, ハミルトニアン・モンテカルロは各々のiterationでランダムなモーメントを生成します）.</p>
<p>コンピュータは決定論的です. 真のランダムネスは存在せず, 擬似乱数の生成があるだけです. 「種」をもとに乱数の列が生成されます. Stan では（Rのような他の言語でも）時刻や日付に基づいた種を生成する方法を使うことができます. また, 種をStanに（もしくはRに）整数として与えることもできます. Stanはあとで結果が再現するように, Stan自体のバージョン番号だけでなく, 乱数を生成するのに使った乱数の種を出力することができます(注2).</p>
<p>(注2): 再現性を保つにはコンパイラを同じにして, ハードウェアを同じにする必要もあります. なぜなら浮動小数点演算はOSやハードウェアの設定やコンパイラをまたいで振る舞いが完全に同じではないからです.</p>
<h3 id="可読性を高める">2.3. 可読性を高める</h3>
<p>他の形式のライティングのように聴衆ありきでプログラムやスクリプトを扱うことで, コードの使われ方について重要な広がりがあります. 他人がプログラムやモデルを読みたくなるかもしれないだけでなく, 開発者もあとからそれを読みたくなるでしょう. Stanのデザインのモチベーションの一つは次の諸観点からモデルがドキュメントそのものになることでした. 変数の使い方（すなわちデータとパラメータの対比）や型（すなわち分散共分散行列と制限のない行列の対比）やサイズの観点です.</p>
<p>可読性の大きな部分は一貫性です. 特に名前とレイアウトにおける一貫性です. プログラムだけではなく, そのプログラムが置かれるディレクトリやファイルの名前やレイアウトの一貫性です.</p>
<p>コードの可読性はコメントについてだけではありません（Stanのコメントの推奨や文法については2.8節を見てください）.</p>
<p>誰か他の人に助けを得るためにデバッグやデザインの問題について十分に説明しようとする時に, その問題の解決策が出てくることは驚くべきほどたくさんあります. これはメーリングリスト上でも起こりえます. 人to人の時に最もそうなります. あなた自身の問題を誰かに説明する時に解決策を見つけることはソフトウェア開発において非常に多いので, 聞いている人は「ラバーダック」と呼ばれています. なぜなら聞いている人は話に合わせてうんうんと相づちを打つだけだからです(注3).</p>
<p>(注3): 実際のラバーダックではうまくいかないことが研究によって示されています. 何らかの理由でラバーダックは実際に説明を理解できなければならないのです.</p>
<h3 id="データを探検する">2.4. データを探検する</h3>
<p>言うまでもないことですが, やみくもにデータをフィットだけさせようとしないでください. 実際に性質を理解しなければならないデータをよく見てください. もしロジスティック回帰をしているなら, それは分離可能ですか？もしマルチレベルモデリングをしているなら, そもそもの結果はレベルごとに変化していますか？もし線形回帰をしているなら, xとyの散布図を書いて線形モデルがあてはまりそうかどうか見てみましょう.</p>
<h3 id="トップダウンでデザインし-ボトムアップでコーディングする">2.5. トップダウンでデザインし, ボトムアップでコーディングする</h3>
<p>ソフトウェアのプロジェクトはだいたいいつも一つかそれ以上の意図的なユースケースからトップダウンでデザインされます. 一方, 良いソフトウェアのコーディングは典型的にはボトムアップで行われます.</p>
<p>トップダウンデザインの動機は明白です. ボトムアップ開発の動機は, すでにすっかりテスト済みのコンポーネントを使って開発するほうがはるかに容易だということです. Stanはモジュール対応もテストへの対応も組み込まれていませんが, 同じ原理の多くがあてはまります.</p>
<p>Stanの開発者自身がモデルを構築するやり方は, できるだけ単純にスタートし, そこから組み立てていきます. これは最終的に複雑なモデルを想定しているときであっても, また最終的にフィットさせたいモデルの良いアイディアを持っている場合であっても同様です. 複数の交互作用, 共分散の事前分布, またはその他の複雑な構造の階層的モデルを構築するよりも, 単純にスタートしましょう. 固定の（そしてややタイトな）事前分布の単純な回帰モデルを構築しましょう. それから交互作用やレベルの追加をおこないます. 1度にひとつずつ. 正しくなっていることを確認しましょう. それから拡張です.</p>
<h3 id="シミュレートされたデータでフィットさせる">2.6. シミュレートされたデータでフィットさせる</h3>
<p>あなたのモデルが計算上正しいことを確認するための最善の方法のひとつは, シミュレートされた（つまり偽りの）データを既知のパラメータで作成し, それからモデルがこのパラメータをデータから推定することができるかどうかを確認することです. もしだめであれば, 生のデータで正しい結果を得る希望はほとんど持てないでしょう.</p>
<p>これを行う洒落た方法がいくつかあります. そこでは周辺化された統計量に対するカイ2乗検定を行ったり, 区間検定を扱うクックら(2006)の枠組みに従うことができます.</p>
<h3 id="表示することでデバッグする">2.7. 表示することでデバッグする</h3>
<p>Stanにはステップごとのデバッガも単体テストのフレームワークもついていませんが, 昔ながらのprintfでのデバッグはサポートしています(注4).</p>
<p>Stanは1以上の文字列もしくは式を引数として持つprint文をサポートしています. Stanは命令型の言語なので, 変数はプログラムの実行中に異なる場所で異なる値を持つことができます. print文はStanのようなステップごとのデバッガを持たない言語には非常に価値があります.</p>
<p>例えば, 変数yとzの値を表示するときは以下のような文を使います.</p>
<pre><code>print(&quot;y=&quot;, y, &quot; z=&quot;, z);</code></pre>
<p>このprint文は文字列<code>&quot;y=&quot;</code>に続いて変数yの値, 文字列<code>&quot; z=&quot;</code>(頭にスペースをつけて)に続いて変数zの値を表示します.</p>
<p>それぞれのprint文の最後には改行がつきます. 改行のための具体的なアスキー文字はプラットフォーム依存です.</p>
<p>任意の式表現を使うことができます. 例えば,</p>
<pre><code>print(&quot;1+1=&quot;, 1+1);</code></pre>
<p>という文は<code>&quot;1+1=2&quot;</code>に続けて改行を表示します.</p>
<p>print文は他の命令を使うことができる場所ならどこでも使うことができますが, 何回実行されるかは記述されているブロックがどのくらいの回数評価されるかに依存します. print文の文法と評価について詳しくはセクション26.8を参照してください.</p>
<p>(注4) 「f」がついているのはタイプミスではありません. これはC言語で書式付き表示をするときに使われるprintf関数の名前にちなんだ歴史的な産物です.</p>
<h3 id="コメント">2.8. コメント</h3>
<h4 id="コードは嘘をつかない">コードは嘘をつかない</h4>
<p>機械はドキュメントに書かれたことではなくコードに書かれたことを実行します. ドキュメントは一方, 必ずしもコードと一致しません. ドキュメントがきちんとメンテナンスされていない場合, コードの進化にともなってコードのドキュメントは容易に腐ってしまいます.</p>
<p>したがって, 読めないコードのドキュメントを書くのに対して, 読めるコードを書くほうが常に好ましいです. ドキュメントを書くときにはいつも, コードをそんな風にかく方法がないか自問して, ドキュメントが不必要になるようにしましょう.</p>
<h4 id="stanのコメントスタイル">Stanのコメントスタイル</h4>
<p>StanはC++スタイルのコメントをサポートしています. 詳しくはセクション28.1をごらんください. お勧めのスタイルはコード内の短いコメントや1行以上をコメントアウトするときには行コメントにすることです. ブロックコメントは長いドキュメンテーションコメント用にとっておきます. このしきたりの理由は, ブロックコメントはブロックコメントの中に作ることができないからです.</p>
<h4 id="コメントすべきでないこと">コメントすべきでないこと</h4>
<p>コードにコメントをつける際, 使用中のプログラミング言語の基本を理解している他のプログラマのためにコメントを書いていると想定するのが通常は安全です. 別の言葉で言えば, 明白なことはコメントしてはいけません. 例えば, 以下のような, コードに何の情報も付加しないコメントを書く必要はありません.</p>
<pre><code>y ~ normal(0,1);  // yは標準正規分布に従う</code></pre>
<p>以下の例のような手動での変数変換に対するヤコビアンの調整であればコメントする価値があるかもしれません.</p>
<pre><code>exp(y) ~ normal(0,1);
// 変数変換に対するヤコビアンの調整: y = log | d/dy exp(y) |
increment_log_prob(y);</code></pre>
<p>将来このコードを読む人に共感し, その人達が統計やStanについて何を知らないか（または覚えていないか）を決めてコメントを書くのは一種の芸術です.</p>
<h4 id="コメントすべきこと">コメントすべきこと</h4>
<p><code>N</code>や<code>mu</code>, <code>sigma</code>といった一般的な変数名をつけているときには変数宣言にコメントをつけることが助けになるでしょう. 例えば, 項目反応モデルにおけるいくつかのデータ変数宣言は以下のように有用なコメントをつけるとよいでしょう.</p>
<pre><code>int&lt;lower=1&gt; N;  // 観測数
int&lt;lower=1&gt; I;  // 生徒数
int&lt;lower=1&gt; J;  // テストの問題数</code></pre>
<p>別の方法は, コメントが不要な長い変数名にすることです.</p>
<pre><code>int&lt;lower=1&gt; n_obs;
int&lt;lower=1&gt; n_students;
int&lt;lower=1&gt; n_questions;</code></pre>
<p>どちらのスタイルも合理的で, どちらを採用するかは主として好みの問題です（統計的な慣習を持つコードの読者を混乱させないように, 時にモデル独自の命名の慣習に従う必要があるというのが主な理由です）.</p>
<p>一部のコード作者は冒頭に大きなブロックコメントを置いてモデルの目的, 誰が書いたか, 著作権とライセンスの情報などなどを説明することを好みます. 以下の複数行コメントは大きなコメントブロックの従来の書き方の例です.</p>
<pre><code>/*
* Item-Response Theory PL3 Model
 * -----------------------------------------------------
 * Copyright: Joe Schmoe  &lt;joe@schmoe.com&gt;
 * Date:  19 September 2012
 * License: GPLv3
 */
data {
  // ...</code></pre>
<p>アスタリスクで始まるコメントの書き方は読者がコメントの範囲を理解するのを助けます. 一方コメントに日付やその他の変わりやすい情報を含めることの問題点は, その情報が, 簡単にコードの実情と同期しなくなってしまうことです. 誤解を招いたり間違っているコメントは, 全くコメントがないよりも悪いのです！</p>
<h2 id="データ型">3.データ型</h2>
<p>本章ではStanで変数の宣言や, 式の値に使われるデータ型について議論します. 変数の型はパラメータの宣言, データの一貫性のチェック, 関数の呼び出し, 変数に値を代入する場合のすべてにおいて重要な要素となります. Stanにおいて, すべての式と変数の宣言は, 静的に定められたデータ型と関係しています（すなわちプログラムがコンパイルされた時）. 一方, <code>vector</code>,<code>matrix</code>, <code>array</code>の大きさは動的に決定されます（プログラムが動作する際）. これは動的に変数に文字列を代入し, 後から変数に行列を代入することができる, Rのような言語とは大きく異なっています. 式は変数, 定数のような基礎要素, もしくは引数に適用される関数や演算子のような要素から構成されているでしょう. 本章では基本的なデータ型に話題を絞り, それらがどのように宣言, 代入, 使用されるかについて解説します. <code>array</code>, <code>vector</code>,<code>matrix</code>などのコンテナ型の詳細な比較については次章に譲ります.</p>
<h3 id="基本データ型">3.1. 基本データ型</h3>
<p>組み込み型関数やユーザ定義が定義した関数の引数, ローカル変数はいずれも基本データ型である必要があります. 基本データ型とは, 制約のないプリミティブ型, <code>vector</code>型, <code>matrix</code>型, およびそれらを並べた<code>array</code>型のことです.</p>
<h4 id="プリミティブ型">プリミティブ型</h4>
<p>Stanには連続値に対応した<code>real</code>と, 整数に対応した<code>int</code>の2種のプリミティブ型が用意されています.</p>
<h4 id="vector-matrix型"><code>vector</code>, <code>matrix</code>型</h4>
<p>Stanには列ベクトルに対応した<code>vector</code>, 行ベクトルに対応した<code>row_vector</code>, 行列に対応した<code>matrix</code>の3種の行列ベースのデータ型が用意されています.</p>
<h4 id="array型"><code>array</code>型</h4>
<p>array引数を宣言することで, 任意の型（次節で紹介する制約付き型も含む）を<code>array</code>型とすることができます. 以下に具体例を示します.</p>
<pre><code>real x[10];
matrix[3,3] m[6,7];</code></pre>
<p>上記のように記述すると, <code>ｘ</code>は1次元で10個の実数が含まれた値として宣言することになります. <code>m</code>は同様に, 3 × 3行列を6 × 7個並べた2次元<code>array</code>を宣言したことになります.</p>
<h3 id="制約付きデータ型">3.2. 制約付きデータ型</h3>
<p>ローカル変数以外の変数を宣言する際には, 制約をつけても構いません. この際, それぞれの制約付きデータ型は, 制約付きの基本データ型と対応しています. 制約は<code>data</code>, <code>transformed data</code>, <code>transformed parameters</code>, <code>generated quantities</code>ブロックで宣言された変数のエラーをチェックします. そのため, 制約は, <code>parameters</code>ブロック内で宣言された変数を扱う際に重要になります. ここでは, <code>parameters</code>ブロックで, 制約付き変数（宣言された制約を満たすもの）を制約のない変数（実数全体）にどのように変換するかを決めています. これは, &quot;モデルは宣言された制約を満たしているパラメータすべての値に対応 (密度がNon-zero)する必要がある&quot; という, 制約付きデータ型の最も重要な側面を示しています.</p>
<p>もし宣言されたパラメータの制約が対応よりも厳格でない場合, サンプラーやオプティマイザはさまざまな問題を抱えることになるでしょう. このような例として, プログラムが動作しない, 初期化の失敗, 過剰なMetropolis rejection, 分布の裾をサンプリングできないことに起因するサンプリングのバイアスなどが問題として挙げられます.</p>
<h4 id="上限-下限の設定">上限, 下限の設定</h4>
<p>以下のようにすべての基本データ型に制約を宣言する構文を使うことで, 変数の上限, 下限を与えることができます.</p>
<pre><code>int&lt;lower=1&gt; N;
real&lt;upper=0&gt; log_p;
vector&lt;lower=-1,upper=1&gt;[3,3] corr;</code></pre>
<h4 id="構造化ベクトル">構造化ベクトル</h4>
<p>Stanには構造化された<code>vector</code>を扱うためのデータ型が用意されています. 昇順で並べた値の<code>vector</code>に使われる<code>ordered</code>と, 昇順に並べた正の値の<code>vector</code>について使われる<code>positive_ordered</code>がそれにあたります. また, 合計すると1になる非負の値のベクトルに対するデータ型として<code>simplex</code>が, 2乗の合計が1になるベクトルに対するデータ型として<code>unit_vector</code>が用意されています. 　　</p>
<h4 id="構造化マトリクス">構造化マトリクス</h4>
<p>対称な正定値行列を表すために, <code>cov_matrix</code>が, 対角成分が1である対称な正定値行列, すなわち相関行列を表すために, <code>corr_matrix</code>が用意されています. また, コレスキー因子の型も用意されています. <code>cholesky_factor_cov</code>型は対称な正定値行列のコレスキー因子, すなわち正の対角成分を持つ下三角行列を表します. <code>cholesky_factor_corr</code>型は相関行列のコレスキー因子, すなわち正の対角成分を持つ下三角行列で, さらに各々の行が単位ベクトル（つまり2乗して合計すると1）であるような下三角行列を表すために使用できます. 因子分解, スケーリングが容易なコレスキー因子型を使うことで, すべての相関行列, 分散共分散行列を計算するよりもはるかに効率よく計算を行うことができます.</p>
<h3 id="代入引数の受け渡し">3.3. 代入・引数の受け渡し</h3>
<h4 id="代入">代入</h4>
<p>制約付けられたデータの値は基本型にマッチする制約付けられていない変数に代入されるかもしれないし, その逆の, 制約付けられていないデータの値が基本型にマッチする制約付けられた変数に代入されることもあるかもしれません. マッチングに際しては<code>array</code>の次元数, 基本型が同じであるか, 厳密に解釈されます. 制約付けは考慮されませんが, 基本データ型については考慮されます. <code>array</code>, <code>vector</code>はお互いに代入することはできません. 同様に, たとえ次元が一致していても, <code>vector</code>, <code>matrix</code>はお互いに代入することはできません. 第4章では<code>vector</code>, <code>array</code>をどのように使い分けるのが適切かについて, 詳細を示します.</p>
<h4 id="関数の呼び出し">関数の呼び出し</h4>
<p>Stanの関数に引数を渡すと, 基本型への代入のように動作します. Stanの関数は<code>array</code>の次元（<code>real a[10,10,10]</code>なら3）を含む, 引数の基本データ型だけからどの関数が呼ばれるかが決まります（<code>array</code>の大きさ, 制約は含まない）. もちろん, 関数はしばしばそれらの動作の一部として制約を確認することがあります.</p>
<h2 id="コンテナ値の入れ物-array-vector-and-matrix">4. コンテナ(値の入れ物): <code>array</code>, <code>vector</code>, and <code>matrix</code></h2>
<p>Stanには<code>array</code>, <code>vector</code>, <code>matrix</code> 3つのコンテナの型が用意されています. これら3つの型は取り換え可能ではありません. たとえ次元が一致していても, ある型の変数から別の型の変数に代入できません. Stanにおいて, 3 × 4の<code>matrix</code>は3 × 4の<code>array</code>とは全く別種類のオブジェクトなのです.</p>
<h3 id="vector-and-matrix">4.1. <code>vector</code> and <code>matrix</code></h3>
<p><code>vector</code>, <code>matrix</code>は<code>array</code>に比べると, 制限されたデータ構造になっています. <code>vector</code>は本来<code>real</code>を1次元状に集めたもので, <code>matrix</code>は本来2次元状に集めたものです.</p>
<p><code>matrix</code>型の用途は, コード内で行列を使用していることを強調するためです. Stanにおいて, <code>vector</code>,<code>matrix</code>型を使うのはおそらく以下の3つの場合のみでしょう.</p>
<ol type="1">
<li>行列演算 (行列の乗算など)</li>
<li>線形代数の関数 (固有値, 行列式など)</li>
<li>多変量関数のパラメータと出力 (多変量正規分布の引数など)</li>
</ol>
<p><code>vector</code>, <code>matrix</code>は整数値を返すことはできず, 取り扱えるのは<code>real</code>に限られています. (注1)</p>
<p>(注1):Stanにおいて, 複雑な整数行列演算や, ブール行列演算が実行されている際にはこれは変更される場合があります. これは, 整数が行列演算に適切な入力ではないためです.</p>
<h3 id="array">4.2. <code>array</code></h3>
<p>他方, <code>array</code>は, は本来, 他の種類のオブジェクトを1次元状に集めたものです. <code>array</code>には単純な<code>real</code>や<code>int</code>, <code>vector</code>, <code>matrix</code>, 別の<code>array</code>のような, あらゆるデータ型の値を格納することができます. <code>array</code>はStanにおいて唯一, 整数値の並びを格納できる型です. Stanにおいては, 離散分布などの関数が整数値を引数とします. 2次元の<code>array</code>は概念的にも現在の実装面からも, <code>array</code>を並べたものとして扱うことができます. インデックスを<code>array</code>に与えると, <code>array</code>はそのインデックスにおける値を返します. 複数のインデックスを<code>array</code>に与えると, 連鎖的にインデックスにおける値を返す動作が行われます. 例えば, <code>a</code>という2次元<code>array</code>があったとすると, <code>a[m,n]</code>という書き方は<code>a[m][n]</code>の便利な縮めた書き方にすぎません.</p>
<h3 id="効率についての考察">4.3. 効率についての考察</h3>
<p>Stanの根底にある設計の動機の1つとして計算の効率があります.</p>
<p>Stanにおいて, 行列, 線形代数の演算はEigen C++ライブラリのデータ型をベースに実装されています. このため, 行列演算や線形代数の関数を使用する際<code>vector</code>, <code>matrix</code>を型としていれば, データ型を変換する必要はありません. 他方, <code>array</code>はC++の<code>std::vector</code>クラスのインスタンスとして実装されています. (Eigenライブラリの<code>Eigen::vector</code>クラスや, Stanの<code>vector</code>と混同しないように注意しましょう). <code>array</code>はこのように実装されているため, インデックスにおける値を返すのはとても効率的です. なぜならコピーして値を返すのではなく参照を使って値を返すからです.</p>
<h4 id="matrix-vs.-2次元array"><code>matrix</code> vs. <code>2次元array</code></h4>
<p>Stanのモデルにおいて, 2次元<code>array</code>と<code>matrix</code>のどちらを使うか決めるときに, 効率について2,3個考えることがあります. 一見, 2次元<code>array</code>と<code>matrix</code>のどちらを使ってもよく思われるかもしれません. 第1に, <code>matrix</code>は2次元<code>array</code>よりもメモリの使用量が少ない点です. これは, <code>matrix</code>では, <code>array</code>の並び方は保存せず, データと2つの次元の情報だけを保存しているためです.</p>
<p>第2に, <code>matrix</code>は「列優先」の順序でデータを格納する点です. さらに, <code>matrix</code>内のすべてのデータはメモリ内で隣接することが保証されます. これは最適化されたコードを考えると大切なことです. なぜなら現代のCPUを使った算術演算を実行することよりもデータをメモリからキャッシュに持っていくことの方がはるかに時間がかかるからです. 他方, <code>array</code>はプリミティブ型の値はメモリ内で隣接することを保証しており, それ以外の場合はその値のコピーを保持します（可能な限り, 参照を使って値を返します）.</p>
<p>第3に, いずれのデータ構造もデータが保持されている順序でインデックスを移動させると最も速くアクセスできます. メモリ上の位置もアクセス速度に関係します. <code>matrix</code>は列優先であるため, 以下の順序でインデックスを移動させるのが適切です.</p>
<pre><code>  matrix[M,N] a;
  //...
  for (n in 1:N)  //列が先
    for (m in 1:M)  //行が後
      // ... a[m,n]を使った計算...</code></pre>
<p>他方, <code>array</code>は以下の例のように行優先の順序でインデックスを移動させるべきです（すなわち, 最後のインデックスが最も移動するのが速い）.</p>
<pre><code>  real a[M,N];
  // ...
  for (m in 1:M)  //行が先
    for (n in 1:N)  //列が後
      // ... a[m,n]を使った計算...</code></pre>
<p>最初に<code>a[m,n]</code>を使う際には, <code>a[m]</code>をメモリに持ってくるように書くべきです. 一般的に, <code>matrix</code>内の移動は, <code>array</code>内の移動よりも効率が良いです. これは<code>matrix</code>の<code>array</code>についても同様です. 例えば, <code>matrix</code>の2次元<code>array</code>のインデックスを移動してアクセスするのに理想的な順序は以下になります.</p>
<pre><code>  matrix[M,N] b[I,J];
  // ...
  for (i in 1:I)  //arrayなので行が先
    for (j in 1:J)  //arrayなので列が後
      for (n in 1:N)  //matrixなので列が先
        for (m in 1:M) //matrixなので行が後
          //... b[i,j,m,n] を使った計算...</code></pre>
<p><code>a</code>が<code>matrix</code>の場合, <code>a[m]</code>と表記するとその<code>matrix</code>の行<code>m</code>が抽出されますが, これは<code>matrix</code>を取り扱う上では非効率な操作です. もし複数の<code>vector</code>にインデックスでアクセスする必要があるならば, <code>vector</code>の<code>array</code>を宣言する方がはるかに良いです.</p>
<pre><code>  row_vector[N] b[M];
  // ...
  for (m in 1:M)
    //... row vector b[m]を使った計算 ...</code></pre>
<p>これは以下の<code>matrix</code>を使った例よりも圧倒的に効率的です.</p>
<pre><code>  matrix b[M,N];
  // ...
  for (m in 1:M)
    // ... row vector b[m]を使った計算  ...</code></pre>
<p>同様に, 列ベクトルの<code>array</code>に対してインデックスを移動させてアクセスする方が, <code>matrix</code>の列を抽出する<code>col</code>関数を使うよりも効率的です. 対照的に, 行列演算や線形代数の関数として行われることは何でも<code>matrix</code>が一番速いでしょう. だから, もし予測変数と係数のドット積（訳注: 要素ごとにかけて和をとったもの, 内積）の行を作成したい場合には, 以下のように記述することで,</p>
<pre><code>  matrix[N,K] x; // 予測変数（説明変数, 共変量としても知られる）
  // ...
  vector[K] beta; // 係数
  // ...
  vector[N] y_hat; // 線形予測
  // ...
  y_hat &lt;- x * beta;</code></pre>
<p>以下のように書くよりも効率よく列を作ることができます.</p>
<pre><code>  row_vector[K] x[N]; // predictors (aka covariates)
  // ...
  vector[K] beta; // coeffs
  ...
  vector[N] y_hat; // linear prediction
  ...
  for (n in 1:N)
    y_hat[n] &lt;- x[n] * beta;</code></pre>
<h4 id="列vector-vs.-1次元array">（列）<code>vector</code> vs. 1次元<code>array</code></h4>
<p>列ベクトル<code>vector</code>と行ベクトル<code>row_vector</code>と1次元<code>array</code>の間にはまったく違いがありません. . <code>Eigen:vector</code>テンプレートと, <code>C++</code>の<code>std:vector</code>テンプレートクラスは, <code>double</code>型の値のコンテナとして非常に近い形で実装されています (Stanでは<code>real</code>型). ただし, Stanにおいて整数値を格納できるのは<code>array</code>だけです.</p>
<h2 id="multiple-indexingとrange-indexing">5. multiple indexingとrange indexing</h2>
<p>Stanはコンテナ（すなわち配列・<code>vector</code>・<code>matrix</code>）に対して, 整数値のインデックスの配列または範囲のインデックスを使うことで, 複数のインデックスによるアクセスを一度に行うことができます. 以降ではこの機能を「multiple indexing」と呼びます. この機能を使うと多くのモデルをベクトル化できます. 例えば, 傾きと切片が個体によって異なる階層線形回帰モデルの尤度を考えてみましょう. Stanのコードは以下になるでしょう.</p>
<pre><code>for (n in 1:N)
  y[n] ~ normal(alpha[ii[n]] + beta[ii[n]] * x[n], sigma);</code></pre>
<p>multiple indexingを使うと, このコードは1行になり, より効率的なベクトル化されたコードになります.</p>
<pre><code>y ~ normal(alpha[ii] + beta[ii] .* x, sigma);</code></pre>
<p>後者のバージョンはローカル変数に代入するダサい方法とスピード面では等価です.</p>
<pre><code>{
  vector[N] mu;
  for (n in 1:N)
  mu[n] = alpha[ii[n]] + beta[ii[n]] * x[n];
  y ~ normal(mu, sigma);
}</code></pre>
<h3 id="multiple-indexing">5.1. Multiple Indexing</h3>
<p>整数値の配列を使ったmultiple indexingの最も単純な具体例は以下です. 省略（...）はコメントにあるように変数を定義するコードを表します.</p>
<pre><code>int c[3];
... // 定義: c == (5, 9, 7)
int idxs[4];
... // 定義: idxs == (3, 3, 1, 2)
int d[4];
d = c[idxs]; // 結果: d == (7, 7, 5, 9)</code></pre>
<p>一般的にはmultiple indexを表現した<code>c[idxs]</code>は, <code>idxs</code>を大きさ<code>K</code>の配列として, 以下のように定義されます.</p>
<pre><code>c[idxs] = ( c[idxs[1]], c[idxs[2]], ..., c[idxs[K]] )</code></pre>
<p>このように<code>c[idxs]</code>は<code>idxs</code>と同じ大きさになります. この例では大きさは<code>K</code>です. multiple indexingは多次元配列にも使うことができます. 例えば, 以下を考えましょう.</p>
<pre><code>int c[2, 3];
... // 定義: c = ((1, 3, 5), ((7, 11, 13))
int idxs[4];
... // 定義: idxs = (2, 2, 1, 2)
int d[4, 3]
d = c[idxs]; // 結果: d = ((7, 11, 13), (7, 11, 13),
             //              (1, 3, 5), (7, 11, 13))</code></pre>
<p>すなわち, <code>c[idxs]</code>のようにインデックスを1番目の位置に置くと, <code>idxs</code>が1次元配列の場合に定義したのと全く同じように振る舞います. つまり, インデックスの指す値自体が配列になっているだけで, 結果はやはり<code>c[idxs][j] == c[idxs[j]]</code>として定義されます.</p>
<p>multiple indexingは多次元配列の2番目の位置で使うこともできるでしょう. 上の例の続きとして, 1番目の位置にsingle indexで2番目の位置にmultiple indexを使う場合を考えましょう.</p>
<pre><code>int e[4];
e = c[2, idxs]; // 結果: c[2] = (7, 11, 13)
                // 結果: e = (11, 11, 7, 11)</code></pre>
<p>1番目の位置にsingle indexを使うと, 1次元の結果となります. そして, その結果に対してmultiple indexが適用されます. すなわち, <code>c[2,idxs]</code>は<code>c[2][idxs]</code>と同じものとして評価されます.</p>
<p>multiple indexingは多次元配列の1つ以上の位置に対して適用できます. 例えば, 以下を考えましょう.</p>
<pre><code>int c[2, 3];
... // 定義: c = ((1, 3, 5), (7, 11, 13))
int idxs1[3];
... // 定義: idxs1 = (2, 2, 1)
int idxs2[2];
... // 定義: idxs2 = (1, 3)
int d[3, 2];
d = c[idxs1, idxs2]; // 結果: d = ((7, 13), (7, 13), (1, 5))</code></pre>
<p>multiple indexを複数の位置で使うと, もはや<code>c[idxs1, idxs2]</code>は<code>c[idxs1][idxs2]</code>と同じではなくなります. 上のコードを実行したあとの<code>d[i, j]</code>の要素は以下で与えられるのです.</p>
<pre><code>d[i, j] == c[idxs1, idxs2][i, j] = c[idxs1[i], idxs2[j]]</code></pre>
<p>この例は一般的な場合においてmultiple indexingの動作を示したものです. <code>idxs1</code>のようなmultiple indexは, 結果（ここでは<code>c[idxs1, idxs2]</code>）に対するインデックス<code>i</code>を, アクセスの対象となる変数（ここでは<code>c</code>）におけるインデックス<code>idxs1[i]</code>に変換します. 対照的に, single indexはそのインデックスにおける値を単に返します. そして変数の次元が1つ減った結果となります.</p>
<h3 id="range-indexを使ったアクセスslicing">5.2. range indexを使ったアクセス（slicing）</h3>
<p>range indexを使ったアクセス（以降ではslicingと呼びます）は, 1次元配列の連続したスライスや2次元配列の連続した部分ブロックなどを返します. 意味的にはslicingはmultiple indexingの特殊な場合にすぎません.</p>
<h4 id="上下限を与えるインデックス">上下限を与えるインデックス</h4>
<p>例えば, インデックスに上限と下限を与える場合を考えましょう.</p>
<pre><code>int c[7];
...
int d[4];
d = c[3:6]; // 結果: d == (c[3], c[4], c[5], c[6])</code></pre>
<p>range indexである<code>3:6</code>は意味的にはmultiple indexの<code>(3, 4, 5, 6)</code>と同じように振る舞います. 実装の観点からはrange indexの方が高速かつ省メモリです. なぜなら, range indexはmultiple indexを陽に作っているわけではなく, むしろ直接的なループを使っているからです. さらにrange indexは読みやすいので, もし適用できるならばmultiple indexよりも好んで使うべきです.</p>
<h4 id="下限か上限を与えるインデックス">下限か上限を与えるインデックス</h4>
<p>下限だけ, 上限だけ与えることも可能です. <code>c[3:]</code>は<code>c[3:size(c)]</code>を略したものです. <code>c[:5]</code>は<code>c[1:5]</code>を略したものです.</p>
<h4 id="範囲全体のインデックス">範囲全体のインデックス</h4>
<p>最後に, 配列のすべての範囲をカバーするrange indexを書くこともできます. それは単にrangeの記号（:）をインデックスとして書くか, そのインデックスの位置を空にすることで実現できます. 両方の場合において, <code>c[]</code>と<code>c[:]</code>は<code>c[1:size(c)]</code>と同じであり, それはすなわち単に<code>c</code>と同じになります.</p>
<h3 id="代入文の左辺でmultiple-indexingを使う">5.3. 代入文の左辺でmultiple indexingを使う</h3>
<p>multiple indexingは代入文の左辺で使うこともできるでしょう. そこでは, 右辺でコンテナの要素を抽出するときとまったく同じ方法で動きます. 例えば, 以下を考えましょう.</p>
<pre><code>int a[3];
int c[2];
int idxs[2];
...          // 定義: a == (1, 2, 3); c == (5, 9)
             //        idxs = (3,2)
a[idxs] = c; // 結果: a == (1, 9, 5)</code></pre>
<p>上のコードの代入文の結果は次のようになります. <code>a[idxs[1]]</code>（すなわち<code>a[3]</code>）には<code>c[1]</code>（すなわち<code>5</code>）が代入され, <code>a[idxs[2]]</code>（すなわち<code>a[2]</code>）には<code>c[2]</code>（すなわち<code>9</code>）が代入されます.</p>
<p>multiple indexが多数あるときには, 同じルールが以下のように適用されます.</p>
<pre><code>int a[5, 7];
int c[2, 2];
...
a[2:3, 5:6] = c;  // 結果: a[2, 5] == c[1, 1]; a[2, 6] == c[1, 2]
                  //       a[3, 5] == c[2, 1]; a[3, 6] == c[2, 2]</code></pre>
<p>1次元の場合と同じように, 左辺でスライスやブロックもしくは何らかのかたまりを指定し, 右辺がそこへ書き込まれます.</p>
<p>左辺でmultiple indexを使用して再配置やslicingをする際にも, 一般的にsingle indexは次元を減らし, multiple indexは次元を保ちます. 例えば, 以下のように2次元配列の行の一部分に代入することもできます.</p>
<pre><code>int a[10, 13];
int c[2];
...
a[4, 2:3] = c;  // 結果: a[4, 2] == c[1]; a[4, 3] == c[2]</code></pre>
<h4 id="値ごとの代入とaliasing">値ごとの代入とaliasing</h4>
<p>代入の左辺と右辺で同じデータ構造を参照している際に, aliasingと呼ばれる問題が起きます. 例えば, 以下のコードにおける配列<code>a</code>を考えましょう.</p>
<pre><code>int a[3];
... // 定義: a == (5, 6, 7)
a[2:3] = a[1:2];
... // 結果: a == (5, 5, 6)</code></pre>
<p>代入のあとに<code>a</code>の値が<code>(5, 5, 5)</code>ではなく<code>(5, 5, 6)</code>になる理由は, Stanでは右辺の式が新しくコピーされるかのように振る舞うからです. もう一つの例として以下を考えましょう.</p>
<pre><code>int a[3];
int idxs[3];
... // 定義 idxs = (2, 1, 3)
a[idxs] = a;</code></pre>
<p>この場合, 代入の前にコピーが右辺の必要なのは明らかです. <code>a[2:3] = a[1:2]</code>という代入では以下の一連の代入を実行するのと同じと思ってしまいがちです（それは間違いです）.</p>
<pre><code>... // 定義: a = (5, 6, 7)
a[2] = a[1]; // 結果: a = (5, 5, 7)
a[3] = a[2]; // 結果: a = (5, 5, 5)!</code></pre>
<p>これは違う結果を生みます. なぜなら<code>a[2]</code>の値が使う前に変わっているからです.</p>
<h3 id="vectorやmatrixに対するmultiple-index">5.4. <code>vector</code>や<code>matrix</code>に対するmultiple index</h3>
<p>mulitple indexは<code>vector</code>や<code>matrix</code>にも使うことができますし, 同じように<code>vector</code>や<code>matrix</code>の配列にも使うことができます.</p>
<h4 id="vectorの場合"><code>vector</code>の場合</h4>
<p>multiple indexを使う場合, <code>vector</code>と<code>row_vector</code>は配列とまったく同じように振る舞います. もし<code>v</code>が<code>vector</code>ならば<code>v[3]</code>はスカラーの実数値になる一方で, <code>v[2:4]</code>は要素<code>v[2]</code>,<code>v[3]</code>,<code>v[4]</code>を含む長さ3の<code>vector</code>になります.</p>
<p>唯一少し違う点は, multiple indexを使った場合にどのような型を返すかの型推論が異なります. 例えば, 以下の最小の例を考えましょう.</p>
<pre><code>vector[5] v[3];
int idxs[7];
...
vector[7] u;
u = v[2, idxs];

real w[7];
w = v[idxs, 2];</code></pre>
<p>ポイントはsingle indexは常に次元を減らすのに対し, multiple indexは決して次元を減らさないことです. multiple indexを使った次元（とインデックスを使っていない次元）がインデックスされた式の型を決めます. 上の例では<code>v</code>が<code>vector</code>の配列なので, <code>v[2, idxs]</code>は配列の次元を減らすけれども, <code>vector</code>の次元を減らしません（スカラーになりません）. そして, <code>w</code>の型は<code>real</code>の配列になります. 両方の場合において, multiple indexの大きさ（ここでは<code>7</code>）が結果の大きさを決めています.</p>
<h4 id="matrixの場合"><code>matrix</code>の場合</h4>
<p><code>matrix</code>の場合はもう少しトリッキーです. なぜなら<code>matrix</code>は2つの次元を持つからです. しかし, 背後にある型推論のルールは同じです. multiple indexは次元を減らさないけれども, single indexは次元を減らすというルールです. 以下のコードは<code>matrix</code>に対するmultiple indexingがどのように動くか示します.</p>
<pre><code>matrix[5,7] m;
...
row_vector[3] rv;
rv = m[4, 3:5];    // 結果は 1 x 3
...
vector[4] v;
v = m[2:5, 3];     // 結果は 3 x 1
...
matrix[3, 4] m2;
m2 = m[1:3, 2:5];  // 結果は 3 x 4</code></pre>
<p>ポイントはどの位置にmultiple indexやrange indexを使っても結果に反映されている一方で, single indexを使うと1次元になることです. 上のコードのコメントで示唆した結果の次元から, 結果の型を読み取ることができるでしょう.</p>
<h4 id="matrixに1つのmultiple-indexを使う"><code>matrix</code>に1つのmultiple indexを使う</h4>
<p>もし<code>matrix</code>が1つのmultiple indexを受け取ると, 結果は<code>matrix</code>になります. したがってもし<code>m</code>が<code>matrix</code>なら<code>m[2:4]</code>は<code>matrix</code>です. 対照的に, <code>m[3]</code>のようにsingle indexを与えると結果は<code>row_vector</code>になります. すなわち, <code>m[3]</code>は<code>m[3, ]</code>や<code>m[3, 1:cols(m)]</code>と同じ結果になります.</p>
<h4 id="vectorやmatrixの配列"><code>vector</code>や<code>matrix</code>の配列</h4>
<p><code>matrix</code>や<code>vector</code>や<code>row_vector</code>の配列の場合も, 基本的なルールはまったく同じままです. 例えば, 以下の例を考えましょう.</p>
<pre><code>matrix[3, 4] m[5, 7];
...
matrix[3, 4] a[2];
a = m[1, 2:3];  // 1番目の配列の次元を減らす
a = m[3:4, 5];  // 2番目の配列の次元を減らす</code></pre>
<p>両方の代入において, multiple indexは配列の次元を減らしています. しかし, 結果は異なります. 1番目のケースでは<code>a[i] == m[1, i + 1]</code>となる一方, 2番目のケースでは<code>a[i] == m[i + 2, 5]</code>となります.</p>
<p>前の例に続けて以下を考えましょう（訳注：以降5.5節まで原文では<code>a</code>でしたが<code>m</code>に変更しました）.</p>
<pre><code>...
vector[2] b;
b = m[1, 3, 2:3, 2];</code></pre>
<p>ここでは2つの配列の次元が減って, <code>matrix</code>の列の次元も減り, 行の次元だけ残るので, 結果は<code>vector</code>になります. この場合では, <code>b[j] == m[1, 3, 1 + j, 2]</code>です.</p>
<p>この最後の例は大切なポイントを表しています. もし, 下限<code>2</code>が与えらえた<code>2:3</code>のように下限のあるインデックスがあると, 上の例で<code>1 + j</code>という式で見られるように下限引く1である<code>1</code>がインデックス<code>j</code>に加わります.</p>
<p>さらに続けて, 次を考えましょう.</p>
<pre><code>...
row_vector[3] c[2];
c = m[4:5, 3, 1, 2: ];</code></pre>
<p>ここでは2番目の配列の次元が減って1次元配列となり, <code>matrix</code>の行のインデックスが減って<code>row_vector</code>となっています. インデックスによるアクセスの結果, 値は<code>c[i, j] == a[i + 3, 3, 1, j + 1]</code>となります.</p>
<h3 id="パラメータと固定値を含んだmatrix">5.5. パラメータと固定値を含んだ<code>matrix</code></h3>
<p>3行3列の<code>matrix</code>があって, 2つの要素がゼロで残りの要素がパラメータである場合を考えてみましょう. そのような状況は固定されたパラメータを伴う問題や欠測データがある場合に起こります.</p>
<p>ある3行3列の<code>matrix</code>が<code>[1, 2]</code>と<code>[1, 3]</code>の要素はゼロだと知られているとしましょう. この場合, パラメータに対するインデックスは「融けた」データフレームやデータベースのフォーマットで表されます.</p>
<pre><code>transformed data {
  int&lt;lower=1, upper=3&gt; ii[7];
  int&lt;lower=1, upper=3&gt; jj[7];
  ii[1] = 1; jj[1] = 1;
  ii[2] = 2; jj[2] = 1; 　// [1, 2] と [1, 3] を飛ばす
  ii[3] = 3; jj[3] = 1;
  ii[4] = 2; jj[4] = 2;
  ii[5] = 3; jj[5] = 2;
  ii[6] = 2; jj[6] = 3;
  ii[7] = 3; jj[7] = 3;
}</code></pre>
<p>7つの残っているパラメータは<code>vector</code>として宣言されます.</p>
<pre><code>parameters {
  vector[7] A_raw;
}</code></pre>
<p>そして, フル行列<code>A</code>は<code>model</code>ブロックにおいてローカル変数として構築されます.</p>
<pre><code>model {
  matrix[3, 3] A;
  A[ii, jj] = A_raw;
  A[1, 2] = 0;
  A[1, 3] = 0;
}</code></pre>
<p>この状況ではこの方法はやりすぎに見えるかもしれません. しかし, より一般的な状況では, 固定値の要素を埋めるために<code>matrix</code>のサイズ・<code>vector</code>の長さ・配列<code>ii</code>と<code>jj</code>や大きさを, 固定値の値とともにデータとしてコードに書くことになるでしょう. 一握りの要素が正だとわかっている場合など, アドホックな制約がある<code>matrix</code>を構築するには, 似たテクニックが使われるでしょう.</p>
<h2 id="回帰モデル">6. 回帰モデル</h2>
<p>Stanでは, 単純な線形回帰からマルチレベルの一般化線形モデルまで, 回帰モデルを扱えます.</p>
<h3 id="線形回帰">6.1. 線形回帰</h3>
<p>以下は最も単純な線形回帰モデルで, 1つの予測変数と, 傾きと切片の係数があり, ノイズは正規分布です. このモデルは, 標準的な回帰の記法を用いて記述できます.</p>
<p><br /><span class="math display"><em>y</em><sub><em>n</em></sub> = <em>α</em> + <em>β</em><em>x</em><sub><em>n</em></sub> + <em>ϵ</em><sub><em>n</em></sub> ここで <em>ϵ</em><sub><em>n</em></sub> ∼ Normal(0, <em>σ</em>)</span><br /></p>
<p>これは, 以下のように残差を取り入れてサンプリングするのと等価です.</p>
<p><br /><span class="math display"><em>y</em><sub><em>n</em></sub> − (<em>α</em> + <em>β</em><em>X</em><sub><em>n</em></sub>) ∼ Normal(0, <em>σ</em>)</span><br /></p>
<p>さらに短くなります.</p>
<p><br /><span class="math display"><em>y</em><sub><em>n</em></sub> ∼ Normal(<em>α</em> + <em>β</em><em>X</em><sub><em>n</em></sub>, <em>σ</em>)</span><br /></p>
<p>このモデルの最後の形はStanでは以下のようにコーディングします.</p>
<pre><code>data {
  int&lt;lower=0&gt; N;
  vector[N] x;
  vector[N] y;
}
parameters {
  real alpha;
  real beta;
  real&lt;lower=0&gt; sigma;
}
model {
  y ~ normal(alpha + beta * x, sigma);
}</code></pre>
<p><code>N</code>回の観測がおこなわれ, その各回に予測変数<code>x[n]</code>と結果変数<code>y[n]</code>とがあります. 切片と傾きのパラメータは<code>alpha</code>と<code>beta</code>です. このモデルでは, スケール<code>sigma</code>の, 正規分布するノイズ項を仮定しています. また, 2つの回帰係数には非正則事前分布が設定されています.</p>
<h4 id="行列記法とベクトル化">行列記法とベクトル化</h4>
<p>前のモデルのサンプリング文はベクトル化されています.</p>
<pre><code>y ~ normal(alpha + beta * x, sigma);</code></pre>
<p>同じモデルの, ベクトル化されていないバージョンは以下のとおりです.</p>
<pre><code>for (n in 1:N)
  y[n] ~ normal(alpha + beta * x[n], sigma);</code></pre>
<p>より簡潔なことに加えて, ベクトル化された形の方がはるかに高速です. <sup>1</sup></p>
<p>Stanでは一般に, <code>normal</code>のような分布に渡す引数はベクトルにすることができます. いずれかの他の引数がベクトルまたは配列なら, すべて同じサイズでなくてはなりません. いずれかの他の引数がスカラーなら, その値はベクトルの各要素に再利用されます. 確率関数のベクトル化についてのより詳しい情報は37.5節を参照してください.</p>
<p>この書き方がうまくいく他の理由は, 行列には行列演算を行なうように, Stanの算術演算子がオーバーロードされるからです. この場合では, <code>x</code>が<code>vector</code>型で<code>beta</code>が<code>real</code>型なので, 式<code>beta * x</code>は<code>vector</code>型です. Stanはベクトル化をサポートしているので, 2つ以上の予測変数がある回帰モデルもそのまま行列の記法を用いて書くことができます.</p>
<pre><code>data {
  int&lt;lower=0&gt; N;  // データ項目の数
  int&lt;lower=0&gt; K;  // 予測変数の数
  matrix[N,K] x;   // 予測変数の行列
  vector[N] y;     // 結果変数のベクトル
}
parameters {
  real alpha;           // 切片
  vector[K] beta;       // 予測変数の係数
  real&lt;lower=0&gt; sigma;  // 誤差のスケール
}
model {
  y ~ normal(x * beta + alpha, sigma);  // 尤度
}</code></pre>
<p><code>sigma</code>の宣言には<code>lower=0</code>という制約をつけて, 値が0以上になるように制限しています. <code>model</code>ブロックには事前分布がないので, 非負の実数の非正則事前分布ということになります. より情報のある事前分布を加えることもできますが, 正則事後分布が導ける限り, 非正則事前分布でも問題はありません.</p>
<p>上のモデルでは, <code>x</code>は<span class="math inline"><em>N</em> × <em>K</em></span>行列の予測変数, <code>beta</code>は<span class="math inline"><em>K</em></span>次元ベクトルの係数なので, <code>x * beta</code>は<span class="math inline"><em>N</em></span>次元ベクトルの予測値です. <span class="math inline"><em>N</em></span>個のデータ項目のそれぞれに対応します. これら予測値は, <span class="math inline"><em>N</em></span>次元ベクトル<code>y</code>にある結果変数に対応して並んでいますので, 上のようにモデル全体を行列演算を使って書くことができます. 値が1の列を<code>x</code>に含めることにより, <code>alpha</code>パラメータをなくすこともできるでしょう.</p>
<p>上のモデルは, ループを使った下のようなモデルと統計的に等価です. 上のモデルのサンプリング文は, より効率的な, ベクトルによる方法でコーディングしたところしか違いません.</p>
<pre><code>model {
  for (n in 1:N)
    y[n] ~ normal(x[n] * beta, sigma);
}</code></pre>
<p>Stanの行列のインデックスの仕組みでは, <code>x[n]</code>は行列<code>x</code>の行<code>n</code>を取り出します. <code>beta</code>は列ベクトルなので, 積<code>x[n] * beta</code>は<code>real</code>型のスカラーです.</p>
<p><sup>1</sup> インタープリタのPythonやRとは違って, StanはC++に変換されてコンパイルされるので, ループも代入文も高速です. ベクトル化したコードがStanで速いのは次の理由によります. (a)導関数を計算するのに使われる式木を単純にできるので, 実質的な関数呼び出しを少なくできます. (b)上のモデルの<code>log(sigma)</code>のような計算は, ループ版では繰り返し実行されますが, ベクトル化すると1度だけ実行されて, その後は再利用されます.</p>
<h5 id="入力に含めた切片">入力に含めた切片</h5>
<p>以下にあるモデルの定式化では, 切片の係数<code>alpha</code>はもうありません.</p>
<pre><code>y ~ normal(x * beta, sigma);</code></pre>
<p>そのかわり, 入力行列<code>x</code>の最初の列が, 値が1の列であると仮定しています. この方法では, <code>beta[1]</code>が切片の役割を果たしているのです. もし切片が, 傾きの項とは異なる事前分布を取っているなら, 違いがはっきりするでしょう. 乗算の回数が1つ減るので, 係数の変数を明示的に別にする形式よりもやや効率的でもあります. といっても, 速度にはたいした違いはでないでしょうから, これを選ぶ理由は明確さにあるでしょう.</p>
<h3 id="係数とスケールの事前分布">6.2. 係数とスケールの事前分布</h3>
<p>この節では, 回帰の係数とスケールの事前分布をモデリングするときにどのような選択肢があるか説明します. 階層モデルにおける1変量のパラメータの事前分布は6.9節で議論し, 多変量のパラメータについては6.12節で議論します. また, モデルの識別性のために使う事前分布については6.11節で議論します.</p>
<h4 id="背景となる文献">背景となる文献</h4>
<p>スケールパラメータの事前分布の選択についての概要についてさらに知るにはGelman (2006)を参照してください. 罰則付き最尤推定値におけるスケールの事前分布の選択の概要についてはChung et al. (2013)を参照してください. 回帰係数の事前分布の選択に関する議論についてはGelman et al. (2008)を参照してください.</p>
<h4 id="非正則一様事前分布">非正則一様事前分布</h4>
<p>Stanはデフォルトでは, 宣言された制約で定まる範囲の値をすべて取りうる一様（あるいは「平坦」）事前分布をパラメータに設定します. したがって, 制約なしに宣言されたパラメータはデフォルトでは(<span class="math inline"> − ∞</span>,<span class="math inline">∞</span>)の一様事前分布が与えられます. 一方, 下限が0と宣言されたスケールパラメータでは, (0,<span class="math inline">∞</span>)の非正則一様事前分布となります. 両者の事前分布とも, 取りうる範囲全体で積分すると1になるような密度関数には定式化する方法がないという意味で非正則です.</p>
<p>Stanでは, 非正則事前分布でモデルを定式化することができますが, サンプリングあるいは最適化がうまくいくためには, 与えられたデータによって事後分布が正則にならなければなりません. そのためには通常, 必要最小限のデータ量がなくてはなりませんが, 最小量のデータを与えることは推定のはじめの一歩として有用なことがあります. あるいは, 感度分析（すなわち, 事前分布が事後分布に及ぼす影響を検討する）の基準としても同様です.</p>
<p>一様事前分布は, それが定式化された軸に依存します. 例えば, スケールパラメータ<span class="math inline"><em>σ</em> &gt; 0</span>に(0,<span class="math inline">∞</span>)という一様事前分布を与え, <span class="math inline"><em>q</em>(<em>σ</em>) = <em>c</em></span>（「密度」は正規化されていないものだけではなく, 正規化できないものにも使われるので, ここでは<span class="math inline"><em>q</em></span>を使います）とすることもできるでしょうし, 対数軸を使って, <span class="math inline">log<em>σ</em></span>に(<span class="math inline"> − ∞</span>,<span class="math inline">∞</span>)という一様事前分布を与え, <span class="math inline"><em>q</em>(log<em>σ</em>) = <em>c</em></span>とすることもできるでしょう. 対数変換に必要なヤコビアンの調整のため, これらは<span class="math inline"><em>σ</em></span>について別の事前分布となります. 変数変換とそれに必要なヤコビアンの調整についてもっと知りたいときは56.1節を参照してください.</p>
<p>Stanは, 制約付きで宣言された変数について, 制約された範囲の値を取りうる一様密度となるように, 必要なヤコビアンの調整を自動的におこないます. このヤコビアンの調整は最適化のときには行なわれませんが, これは適切な最尤推定値を生成するためです.</p>
<h4 id="正則一様事前分布-範囲の制約">正則一様事前分布: 範囲の制約</h4>
<p>上限と下限の両方を設定することで, 正則な一様事前分布に従うような変数を宣言することも可能です. 以下はその例です.</p>
<pre><code>real&lt;lower=0.1, upper=2.7&gt; sigma;</code></pre>
<p>これは<code>sigma</code>に, <span class="math inline">Uniform(0.1, 2.7)</span>という事前分布を暗黙のうちに与えるでしょう.</p>
<h5 id="制約を合わせる">制約を合わせる</h5>
<p>制約すべてについて同じことが言えますが, <code>sigma</code>の取りうるすべての値についてモデルでも同じとなっていることが重要です. 例えば, 下のコードでは<code>sigma</code>が正に制約されていますが, その一様事前分布では上下限を設定しています.</p>
<pre><code>parameters {
  real&lt;lower=0&gt; sigma;
  ...
model {
  // *** 悪い例 *** : 制約より狭い範囲しか取りません
  sigma ~ uniform(0.1, 2.7);</code></pre>
<p>このサンプリング文は<code>sigma</code>に(0.1, 2.7)の範囲を設定しています. この範囲は, 制約で宣言された範囲, すなわち(0, <span class="math inline">∞</span>)よりも狭くなっています. このようにすると, Stanプログラムは, 初期化が困難になったり, サンプリング中にハングしたり, ランダムウォークに陥ったりする可能性があります.</p>
<h5 id="上下限付近の推定値">上下限付近の推定値</h5>
<p>範囲制限のついたパラメータの境界近くに推定値があることは通常, 事前分布がモデルに合っていないことを示しています. また, サンプリングまたは最適化の際に, アンダーフローやオーバーフローといった数値的な問題を起こす可能性もあります.</p>
<h4 id="非情報正則事前分布">「非情報」正則事前分布</h4>
<p>回帰係数に<span class="math inline">Normal(0, 1000)</span>のような事前分布をつけたモデルは珍しくありません. <sup>2</sup>事前分布のスケールが, 1000のように, 推定される係数よりも数桁大きい場合は, そのような事前分布は事実上まったく何の効果も持ちません.</p>
<p>BUGSの例題(Lunn et al., 2012)は全般に, スケールについてのデフォルトの事前分布を下のようにしていますが, これは使わないようにしましょう.</p>
<p><br /><span class="math display"><em>σ</em><sup>2</sup> ∼ InvGamma(0.001, 0.001)</span><br /></p>
<p>このような事前分布は, 妥当な事後分布の値の外側に, あまりに大きく集中した確率の山があります. 正規分布につける対称的で広い事前分布とは異なり, これにより事後分布をゆがめる深刻な影響が発生する可能性があります. 例題と議論はGelman (2006)を参照してください.</p>
<p><sup>2</sup>この習慣はBUGSでは普通で, 例題(Lunn et al., 2012)のほとんどに見受けられます.</p>
<h4 id="切断事前分布">切断事前分布</h4>
<p>下限が0として宣言された変数に, 正規分布の事前分布を設定すると, Stanのモデルでは, 正則に切断された半正規分布の事前分布を設定するのと同じ効果を持ちます. 0での切断分布を指定する必要はありません. Stanで計算に必要なのは密度から割合までだけで, 確率として正規化する必要はないからです. そこで, 以下のように変数を宣言します.</p>
<pre><code>real&lt;lower=0&gt; sigma;</code></pre>
<p>そして, 事前分布を与えます.</p>
<pre><code>sigma ~ normal(0,1000);</code></pre>
<p>すると, <code>sigma</code>には半正規分布の事前分布が与えられます. 技術的には以下のようになります.</p>
<p><br /><span class="math display">$$ p(\sigma) = \frac{\mathsf{Normal}(\sigma \mid 0,1000)}{1 - \mathsf{NormalCDF}(0 \mid 0, 1000)} \propto \mathsf{Normal}(\sigma \mid 0, 1000) $$</span><br /></p>
<p>しかしStanでは, 半正規分布を正規化するのに必要な, 正規分布の累積分布関数(CDF: cumulative distribution function)の計算を避けることができます. もし, 事前分布の位置あるいはスケールがパラメータであるか, あるいは切断点がパラメータであるならば, 切断の計算をせずに済ますことはできません. その場合は, 正規分布のCDF項が定数にならないからです.</p>
<h4 id="弱情報事前分布">弱情報事前分布</h4>
<p>普通は, 推定される変数のスケールについて研究者は何らかの知識を持っているでしょう. 例えば, 成人女性の身長の母平均について切片だけのモデルを推定するなら, 答えは1から3メートルの間のどこかにあるでしょう. こうした情報から弱情報事前分布を構成できます.</p>
<p>同様に, 予測変数が標準スケール（おおよそ平均が0で, 単位分散）のロジスティック回帰なら, 係数の絶対値が5より大きくなることはあまりないでしょう. この場合, <span class="math inline">Normal(0, 5)</span>のような弱情報事前分布をそのような係数に設定するのは道理にかなっています.</p>
<p>計算機的にも統計学的にも, 推定を制御するのに弱情報事前分布は役に立ちます. 計算機的には, 解があると期待される量のまわりの曲率を増加させます. これにより, L-BFGSのような勾配に基づく方法でもハミルトニアンモンテカルロのサンプリングでも, 曲面の位置から遠く離れすぎたところに迷い込まないようになります. 統計学的には, 女性の平均身長のような問題で弱情報事前分布はより有効です. というのも, <span class="math inline">Normal(0, 1000)</span>のような非常に幅の広い事前分布では, 事前分布の確率質量の大半が, 期待される答えの範囲外にあるようにされるからです. 小さなデータセットでは, そうした事前分布が推測値を覆い隠すことがありえます.</p>
<h4 id="上下限のある事前分布">上下限のある事前分布</h4>
<p>女性の身長の例についてもう一度考えてみましょう. 正則事前分布を定式化する方法の1つは, 上下限のあるスケールに一様事前分布を設定することです. 例えば, 女性の平均身長のパラメータは, 下限が1メートルで上限が3メートルと宣言することができるでしょう. 確かに答えはこの間にあるはずです.</p>
<p>同様に, スケールパラメータの事前分布の下限に0を, 上限に, 10,000のような非常に大きな数を設定する例を見ることも珍しくありません. <sup>3</sup>これは, 幅の広い逆ガンマ分布を分散の事前分布に与えて推定するのと, おおまかに言って同じ問題をもたらします. 物理的に完全に制約があるというわけではないパラメーターは固定せずに, 情報事前分布を設定する方がよいでしょう. 女性の身長の場合なら, そのような事前分布は, メートルのスケールで<span class="math inline">Normal(2, 0.5)</span>のようになると思われます. この場合, (1,3)の区間に確率質量の95%が集中しますが, 依然としてその範囲外の値も取りえます.</p>
<p>上下限のある事前分布を使う場合は, パラメータの推定値が上下限に, あるいはそれに非常に近い値になっていないか, 当てはめた事後分布を確認すべきです. そうなっていることは, 計算上の問題が発生しているだけではなく, モデルの定式化に問題があることを示しています. そのような場合, 設定した制約がないときにパラメータが当てはまると思われるところまで範囲を広げるか, あるいは境界の値を推定させないようにする事前分布を使うべきです（6.9節を参照）.</p>
<p><sup>3</sup> これもBUGSの例題モデル(Lunn et al., 2012)でよくある戦略でした. もう1歩進めて, 数値的に0へとアンダーフローするのを防ぐために下限に0.001のような小さい数を設定することもよくありました.</p>
<h4 id="裾の重い事前分布とデフォルトの事前分布">裾の重い事前分布と「デフォルト」の事前分布</h4>
<p>外れ値に対応したいときの合理的な選択肢は, 取りうる値と期待される範囲のあたりに確率質量の大半が集中するものの, 裾にもかなりの確率質量がまだ残るような事前分布を使うことです. このような状況では通常, コーシー分布を事前分布に使うことが選択されます. そうすれば, 中央値のまわりに確率質量を集中させることができますが, 分散が無限大になるくらいに裾が重くなります.</p>
<p>とくに情報がないのであれば, 回帰係数のパラメータにはコーシー分布の事前分布がデフォルトとして非常に良い選択ですし(Gelman et al., 2008), スケールパラメータには半コーシー分布（Stanでは暗黙的にコードされます）がデフォルトとして良い選択です(Gelman, 2006).</p>
<h4 id="情報のある事前分布">情報のある事前分布</h4>
<p>理想的な場合には, 問題についての実質的な情報があって, 弱情報事前分布よりもいっそう強い事前分布を含めることができるでしょう. これは, 事前に実際に実験をして, 別のデータの事後分布として得られるかもしれませんし, メタアナリシスから得られるかもしれませんし, 単に分野の専門家から求められたというものかもしれません. より強度になったというだけで, 弱情報事前分布の長所はすべて当てはまります.</p>
<h4 id="共役性">共役性</h4>
<p>ギブズサンプリングとは異なり, 共役事前分布（すなわち, 事後分布が同族となるような事前分布）を設定する計算上の利点はStanのプログラムにはありません. <sup>4</sup>ハミルトニアンモンテカルロは, 対数密度とその導関数だけで動いており, サンプリングも最適化も共役性を使っていません.</p>
<p><sup>4</sup> BUGSとJAGSはともに, ギブズサンプリングによる共役サンプリングをサポートしています. JAGSは共役の範囲を拡張しており, GLMモジュールで利用できます. Stanとは異なり, BUGSとJAGSはともに, 共分散行列や単体のような「制約のある」多変量の量については共役事前分布を使うように制限されています.</p>
<h3 id="ロバストノイズモデル">6.3. ロバストノイズモデル</h3>
<p>線形解析の標準的な手法では, ノイズ項<span class="math inline"><em>ϵ</em></span>が正規分布するとモデリングします. Stanの視点から言うと, 正規分布のノイズは特別なものではありません. 例えば, ノイズ項にスチューデントのt分布を与えるとロバスト回帰に対応できます. Stanでコーディングするには, サンプリングの分布を以下のように変えます.</p>
<pre><code>data {
  ...
  real&lt;lower=0&gt; nu;
}
...
model {
  for (n in 1:N)
    y[n] ~ student_t(nu, alpha + beta * x[n], sigma);
}</code></pre>
<p>自由度の定数<code>nu</code>はデータとして指定します.</p>
<h3 id="ロジスティック回帰とプロビット回帰">6.4. ロジスティック回帰とプロビット回帰</h3>
<p>結果変数が2値のときは, ロジスティック回帰とプロビット回帰という近い関係にあるモデルのいずれかが使われるでしょう. 一般化線形モデルとしては両者は, (<span class="math inline"> − <em>i</em><em>n</em><em>f</em><em>t</em><em>y</em></span>,<span class="math inline"><em>i</em><em>n</em><em>f</em><em>t</em><em>y</em></span>)という線形軸の予測値を, (0,1)という確率の値へと対応させるリンク関数だけが異なります. リンク関数はそれぞれ, ロジスティック関数と標準正規累積分布関数で, ともにシグモイド関数（すなわり, 両者ともS字型）です.</p>
<p>予測変数1つと切片とからなるロジスティック回帰モデルは以下のようにコーディングされます.</p>
<pre><code>data {
  int&lt;lower=0&gt; N;
  vector[N] x;
  int&lt;lower=0,upper=1&gt; y[N];
}
parameters {
  real alpha;
  real beta;
}
model {
  y ~ bernoulli_logit(alpha + beta * x);
}</code></pre>
<p>ノイズパラメータは, 直接指定されるのではなく, ベルヌーイ分布の定式化に組み込まれています.</p>
<p>ロジスティック回帰は一般化線形モデルの1種で, 結果変数が2値で, リンク関数が対数オッズ（ロジット）関数です. これは以下のように定義されます.</p>
<p><br /><span class="math display">$$ \mathrm{logit}(\nu) = \log\left(\frac{\nu}{1 - \nu}\right) $$</span><br /></p>
<p>リンク関数の逆関数がモデル中にあります.</p>
<p><br /><span class="math display">$$ \mathrm{logit}^{-1}(u) = \frac{1}{1 + \exp(-u)} $$</span><br /></p>
<p>上のモデルの定式化では, ベルヌーイ分布をロジットでパラメータ化したバージョンを使っています. その定義は以下です.</p>
<p><br /><span class="math display">BernoulliLogit(<em>y</em> ∣ <em>α</em>) = Bernoulli(<em>y</em> ∣ <em>l</em><em>o</em><em>g</em><em>i</em><em>t</em><sup> − 1</sup>(<em>α</em>))</span><br /></p>
<p>この定式化ではまたベクトル化もおこなわれています. <code>alpha</code>と<code>beta</code>とがスカラーで<code>x</code>がベクトルなので, <code>alpha + beta * x</code>はベクトルになるからです. このベクトル化された定式化は, 下のもっと非効率なバージョンと等価です.</p>
<pre><code>for (n in 1:N)
  y[n] ~ bernoulli_logit(alpha + beta * x[n]);</code></pre>
<p>ロジットのベルヌーイ分布の部分を展開すると, モデルは等価なまま, より明示的になりますが, より非効率で, 算術的により不安定になります.</p>
<pre><code>for (n in 1:N)
  y[n] ~ bernoulli(inv_logit(alpha + beta * x[n]));</code></pre>
<p>別のリンク関数も同様に使えるでしょう. 例えば, プロビット回帰は, 正規累積分布関数を使います. これは以下のように書かれます.</p>
<p><br /><span class="math display">Φ(<em>x</em>) = ∫<sub> − ∞</sub><sup><em>x</em></sup>Normal(<em>y</em> ∣ 0, 1)<em>d</em><em>y</em></span><br /></p>
<p>標準正規累積分布関数<span class="math inline">Φ</span>は, Stanでは<code>Phi</code>関数として実装されています. Stanでプロビット回帰モデルをコーディングするには, ロジスティックモデルのサンプリング文を以下のように変えればよいでしょう.</p>
<pre><code>y[n] ~ bernoulli(Phi(alpha + beta * x[n]));</code></pre>
<p>Stanでは, 標準正規累積分布関数<span class="math inline">Φ</span>の高速な近似が<code>Phi_approx</code>関数として実装されています. 近似プロビット回帰モデルは以下のようにコーディングされるでしょう.</p>
<pre><code>y[n] ~ bernoulli(Phi_approx(alpha + beta * x[n]));</code></pre>
<h3 id="多項ロジット回帰">6.5. 多項ロジット回帰</h3>
<p>ロジスティック回帰の形式で, 結果変数が多値になるものもStanでそのままコーディングできます. 例えば, それぞれの出力の変数<span class="math inline"><em>y</em><sub><em>n</em></sub></span>について, 結果が<span class="math inline"><em>K</em></span>種類の値を取りうるとします. また, <span class="math inline"><em>y</em><sub><em>n</em></sub></span>に対応する予測変数の<span class="math inline"><em>D</em></span>次元ベクトル<span class="math inline"><em>x</em><sub><em>n</em></sub></span>があるとします. 係数の事前分布を<span class="math inline">Normal(0, 5)</span>とした多項ロジットモデルは以下のようにコーディングされます.</p>
<pre><code>data {
  int K;
  int N;
  int D;
  int y[N];
  vector[D] x[N];
}
parameters {
  matrix[K,D] beta;
}
model {
  for (k in 1:K)
    beta[k] ~ normal(0,5);
  for (n in 1:N)
    y[n] ~ categorical(softmax(beta * x[n]));
}</code></pre>
<p><code>softmax</code>関数の定義は34.11節を参照してください. 最後の行をもっと効率的にすると以下のように書けます.</p>
<pre><code>y[n] ~ categorical_logit(beta * x[n]);</code></pre>
<p><code>categorical_logit</code>分布は, カテゴリカル分布に似ていますが, パラメータがロジットスケールになります（<code>categorical_logit</code>の完全な定義は39.5節を参照してください）.</p>
<p>最初のループをもっと効率的にするには, 行列<code>beta</code>をベクトルに変換して最初のループをベクトル化するとよいでしょう.</p>
<pre><code>to_vector(beta) ~ normal(0,5);</code></pre>
<h5 id="データ宣言時の制約">データ宣言時の制約</h5>
<p>上のモデルの<code>data</code>ブロックは, サイズ<code>K</code>, <code>N</code>, <code>D</code>や結果変数の配列<code>y</code>に制約をつけず定義しています. データ宣言での制約は, データが読み込まれた時点での（あるいは変換データが定義された時点での）エラーチェックのためのもので, サンプリングが始まる前に行なわれます. データ宣言での制約はまた, モデルの作者の意図をより明示的に示すもので, 可読性が高くなります. 上のモデルの宣言をもっと厳しくするとすると以下のようになります.</p>
<pre><code>  int&lt;lower=2&gt; K;
  int&lt;lower=0&gt; N;
  int&lt;lower=1&gt; D;
  int&lt;lower=1,upper=K&gt; y[N];</code></pre>
<p>これら制約の根拠ですが, カテゴリーの数<code>K</code>は, カテゴリカル分布が使えるためには少なくとも2でなくてはなりません. データ項目の数<code>N</code>は0となりえますが, 負ではいけません. Rとは違ってStanの<code>for</code>ループは常に前進するので, <code>1:N</code>というループの範囲は, <code>N</code>が0に等しいときにはループの内部を実行しないことを保証します. 予測変数の数<code>D</code>は, <code>beta * x[n]</code>が<code>softmax()</code>に適切な引数を生成するよう, 少なくとも1でなくてはいけません. カテゴリカル分布の結果変数<code>y[n]</code>は, 離散サンプリングがうまく定義されるように<code>1</code>から<code>K</code>までになくてはなりません.</p>
<p>データ宣言の制約は任意です. 一方, <code>parameters</code>ブロックで宣言されるパラメータの制約は任意<strong>ではありません</strong>. すべてのパラメータの値が制約を確実に満たすようにすることが求められます. <code>transformed data</code>, <code>transformed parameters</code>, <code>generated quantities</code>での制約も任意です.</p>
<h4 id="識別可能性">識別可能性</h4>
<p>入力の各成分に定数を加えてもsoftmaxは不変ですので, このモデルは一般に, 係数についての適切な事前分布があるときにだけ識別されます.</p>
<p>別の選択肢は, <span class="math inline">(<em>K</em> − 1)</span>次元のベクトルを使って, そのうちのひとつを0に固定することです. 8.2節で, ベクトル中に定数とパラメータとを混在させる方法を議論します. 多項ロジットの場合は, <code>parameters</code>ブロックは, <span class="math inline">(<em>K</em> − 1)</span>次元ベクトルを使って以下のように再定義されるでしょう.</p>
<pre><code>parameters {
  matrix[K - 1, D] beta_raw;
}</code></pre>
<p>それから, モデルで使うパラメータに変換します. まず, <code>transformed data</code>ブロックを<code>parameters</code>ブロックの前に加えて, 零値からなる列ベクトルを定義します.</p>
<pre><code>transformed data {
  vector[D] zeros;
  zeros &lt;- rep_vector(0, D);
}</code></pre>
<p>つづいて, これを<code>beta_raw</code>に付け加えて, 係数行列<code>beta</code>をつくります.</p>
<pre><code>transformed parameters {
  matrix[K, D] beta;
  beta &lt;- append_col(beta_raw, zeros);
}</code></pre>
<p><code>rep_vector</code>の定義は34.7節を, <code>append_col</code>の定義は34.10節を参照してください.</p>
<p>これは, パラメータとして<span class="math inline"><em>K</em></span>次元ベクトルを使ったモデルとまったく同じというわけではありません. 事前分布が<span class="math inline">(<em>K</em> − 1)</span>ベクトルにのみ適用されるようになったからです. 実用的には, これにより最尤法の解が違うものになり, 事前分布を0のまわりに中央化したときの事後分布もやや異なります. これは回帰係数でよく起こります.</p>
<h3 id="中央化されたベクトルへのパラメータ化">6.6. 中央化されたベクトルへのパラメータ化</h3>
<p>パラメータのベクトル<span class="math inline"><em>β</em></span>を, 合計して0になる制約を満たすという意味で中央化して定義すると便利なことがよくあります.</p>
<p><br /><span class="math display">$$ \sum_{k=1}^{K}\beta_{k} = 0 $$</span><br /></p>
<p>このようなパラメータのベクトルは, 多項ロジット回帰のパラメータのベクトルを識別するのに使われたり（6.5節を参照）, IRTモデルの能力パラメータや難易度パラメータ（ただしどちらか一方）に使われることがあります（6.10節を参照）.</p>
<h4 id="k-1自由度">K-1自由度</h4>
<p>合計して0になる制約をパラメータのベクトルに課す方法は1つだけではありません. もっとも効率的なのは, 1から<span class="math inline"><em>K</em> − 1</span>番目までの合計の符号を反転させたものとして<span class="math inline"><em>K</em></span>番目の要素を定義する方法です.</p>
<pre><code>parameters {
  vector[K-1] beta_raw;
  ...
transformed parameters {
  vector[K] beta;  // 中央化
  for (k in 1:(K-1)) {
    beta[k] &lt;- beta_raw[k];
  }
  beta[K] &lt;- -sum(beta_raw);
  ...</code></pre>
<p>このパラメータ化で<code>beta_raw</code>に事前分布を設定すると, 合計して0の制約をつけないパラメータ化で<code>beta</code>に同じ事前分布を設定したものと比較して, 事後分布はわずかに異なることになります. とくに, <code>beta_raw</code>の各要素に単純な事前分布を設定すると, 制約のない<span class="math inline"><em>K</em></span>次元ベクトル<code>beta</code>の各要素に同じ事前分布を設定したのと異なる結果が得られます. たとえば, <code>beta</code>の事前分布に<span class="math inline">Normal(0, 5)</span>を設定すると, <code>beta_raw</code>に同じ事前分布を設定したものと事後分布の最頻値は異なることになるでしょう.</p>
<h4 id="単体からの変換およびスケーリング">単体からの変換およびスケーリング</h4>
<p>もう1つの方法は効率の点では劣りますが, 対称な事前分布にできます. 単体であるパラメータにオフセットをつけて, スケーリングする方法です.</p>
<pre><code>parameters {
  simplex[K] beta_raw;
  real beta_scale;
  ...
transformed parameters {
  vector[K] beta;
  beta &lt;- beta_scale * (beta_raw - 1.0 / K);
  ...</code></pre>
<p><code>beta_raw</code>は単体なので, 合計すると1になります. これにより, 要素ごとに<span class="math inline">1/<em>K</em></span>を減じると合計が0になることが保証されます（整数演算により0に丸められるのを防ぐため, <code>1 / K</code>ではなく, <code>1.0 / K</code>という式を使うことに注意しましょう）. 単体の要素の大きさには限度がありますから, <code>beta</code>が, 合計して0となるようなあらゆる値を取るために必要な自由度<span class="math inline"><em>K</em></span>を持つようにするためにはスケーリング因子が必要になります.</p>
<p>このパラメータ化では, ディリクレ分布の事前分布を<code>beta_raw</code>に設定することができます. おそらくは一様分布でしょう. そして, 通常は「縮小」のために, 別の事前分布を<code>beta_scale</code>に設定します.</p>
<h4 id="柔らかい中央化">柔らかい中央化</h4>
<p><span class="math inline"><em>β</em> ∼ Normal(0, <em>σ</em>)</span>のような事前分布を加えることは, パラメータのベクトル<span class="math inline"><em>β</em></span>について, <span class="math inline">$\sum_{k=1}^{K}\beta_{k}=0$</span>となることはないものの, それに近くなるような, 1種の柔らかい中央化を設定することになるでしょう. スカラーの定数<span class="math inline"><em>c</em></span>についての要素ごとの和<span class="math inline"><em>β</em> + <em>c</em></span>と<span class="math inline"><em>β</em></span>とが同じ尤度を生成する場合のみ, この方法でおおまかに中央化されることが保証されます（IRTモデルではおそらく, 別のベクトル<span class="math inline"><em>α</em></span>があって, <span class="math inline"><em>α</em> − <em>c</em></span>と変換されます）. これは, 対称な事前分布を得るためのまた別の方法です.</p>
<h3 id="順序ロジスティック回帰と順序プロビット回帰">6.7. 順序ロジスティック回帰と順序プロビット回帰</h3>
<p>予測変数<span class="math inline"><em>x</em><sub><em>n</em></sub> ∈ ℝ<sup><em>D</em></sup></span>に対する結果変数<span class="math inline"><em>y</em><sub><em>n</em></sub> ∈ 1, …, <em>K</em></span>の順序回帰は, 単一の係数ベクトル<span class="math inline"><em>β</em> ∈ ℝ<sup><em>D</em></sup></span>と, 切断点の数列<span class="math inline"><em>c</em> ∈ ℝ<sup><em>K</em> − 1</sup></span>により決まります. ただし<span class="math inline"><em>c</em></span>は, <span class="math inline"><em>c</em><sub><em>d</em></sub> &lt; <em>c</em><sub><em>d</em> + 1</sub></span>のように並んでいます. 線形予測子<span class="math inline"><em>x</em><sub><em>n</em></sub><em>β</em></span>が<span class="math inline"><em>c</em><sub><em>k</em> − 1</sub></span>と<span class="math inline"><em>c</em><sub><em>k</em></sub></span>の間に入るなら, 離散値の結果変数は<span class="math inline"><em>k</em></span>となります. ここでは, <span class="math inline"><em>c</em><sub>0</sub> =  − ∞</span>かつ<span class="math inline"><em>c</em><sub><em>K</em></sub> = ∞</span>と仮定されています. ノイズ項は回帰の形式によって決まります. ここでは, 回帰ロジスティック回帰と回帰プロビット回帰の例を示します.</p>
<h4 id="順序ロジスティック回帰">順序ロジスティック回帰</h4>
<p>順序ロジスティック回帰はStanでは, 切断点に<code>ordered</code>データ型を使い, 組込みの<code>ordered_logistic</code>分布によりコーディングできます.</p>
<pre><code>data {
  int&lt;lower=2&gt; K;
  int&lt;lower=0&gt; N;
  int&lt;lower=1&gt; D;
  int&lt;lower=1,upper=K&gt; y[N];
  row_vector[D] x[N];
}
parameters {
  vector[D] beta;
  ordered[K-1] c;
}
model {
  for (n in 1:N)
    y[n] ~ ordered_logistic(x[n] * beta, c);
}</code></pre>
<p>切断点<code>c</code>のベクトルは<code>ordered[K-1]</code>として宣言します. これにより, <code>c[k]</code>が<code>c[k+1]</code>よりも小さいことが保証されます.</p>
<p>切断点に独立に事前分布を割り当てれば, この制約は, 順序の制約を満たす点となるようにうまく同時事前分布を切断します. 都合の良いことに, Stanでは確率は割合で分かればよいので, 正規化項についての制約の効果を計算する必要がありません.</p>
<h5 id="順序プロビット">順序プロビット</h5>
<p>順序プロビットモデルは, 累積ロジスティック分布(<code>inv_logit</code>)を累積正規分布(<code>Phi</code>)に変えるだけで, まったく同様にコーディングできるでしょう.</p>
<pre><code>data {
  int&lt;lower=2&gt; K;
  int&lt;lower=0&gt; N;
  int&lt;lower=1&gt; D;
  int&lt;lower=1,upper=K&gt; y[N];
  row_vector[D] x[N];
}
parameters {
  vector[D] beta;
  ordered[K-1] c;
}
model {
  vector[K] theta;
  for (n in 1:N) {
    real eta;
    eta &lt;- x[n] * beta;
    theta[1] &lt;- 1 - Phi(eta - c[1]);
    for (k in 2:(K-1))
      theta[k] &lt;- Phi(eta - c[k-1]) - Phi(eta - c[k]);
    theta[K] &lt;- Phi(eta - c[K-1]);
    y[n] ~ categorical(theta);
  }
}</code></pre>
<p>ロジスティックモデルも, <code>Phi</code>を<code>inv_logit</code>に入れ替えれば, この方法でコーディングできるでしょう. ただし, softmax変換に基づく組込みのエンコーディングの方がより効率的で, 数値的により安定です. <code>Phi(eta - c[k])</code>の値を一度だけ計算して保存し, その後は保存しておいた値を再利用するようにすると, 少しだけ効率が良くなるでしょう.</p>
<h3 id="階層ロジスティック回帰">6.8. 階層ロジスティック回帰</h3>
<p>もっとも単純なマルチレベルモデルは, <span class="math inline"><em>L</em></span>だけある離散カテゴリー（あるいはレベル）にデータがグループ化されるような階層モデルです. 極端な方法は, 全データを完全にプールして, 回帰係数<span class="math inline"><em>β</em></span>のベクトルを共通のものとして推定するというものでしょう. また反対側に極端な方法は, プールはせず, 各レベル<span class="math inline"><em>l</em></span>に固有の係数ベクトル<span class="math inline"><em>β</em><sub><em>l</em></sub></span>を割り当て, 他のレベルとは別に推定するというものでしょう. 階層モデルは中間の解法で, プールの程度は, データと, プールの量についての事前分布とで決まります.</p>
<p>2値の結果変数<span class="math inline"><em>y</em><sub><em>n</em></sub> ∈ 0, 1</span>がすべて, レベル<span class="math inline"><em>l</em><em>l</em><sub><em>n</em></sub> ∈ 1, …, <em>L</em></span>と関連しているとします. 各結果変数はまた, 予測変数のベクトル<span class="math inline"><em>x</em><sub><em>n</em></sub> ∈ ℝ<sup><em>D</em></sup></span>とも関連しているでしょう. 各レベル<span class="math inline"><em>l</em></span>は, 固有の係数ベクトル<span class="math inline"><em>β</em><sub><em>l</em></sub> ∈ ℝ<sup><em>D</em></sup></span>をとります. 階層モデルでは, これもデータから推定される事前分布から係数<span class="math inline"><em>β</em><sub><em>l</em>, <em>d</em></sub> ∈ ℝ<sup><em>D</em></sup></span>が抽出されます. この, 階層的に推定される事前分布がプールの量を決定します. 各レベルのデータが非常に似ているなら, 階層事前分布の分散が小さいことを反映して強いプールが行なわれるでしょう. レベル間でデータが違っていれば, 階層事前分布の分散が大きいを反映して弱いプールが行なわれるでしょう.</p>
<p>以下のモデルは, 回帰係数に階層事前分布を設定した階層ロジスティック回帰モデルをコーディングしたものです.</p>
<pre><code>data {
  int&lt;lower=1&gt; D;
  int&lt;lower=0&gt; N;
  int&lt;lower=1&gt; L;
  int&lt;lower=0,upper=1&gt; y[N];
  int&lt;lower=1,upper=L&gt; ll[N];
  row_vector[D] x[N];
}
parameters {
  real mu[D];
  real&lt;lower=0&gt; sigma[D];
  vector[D] beta[L];
}
model {
  for (d in 1:D) {
    mu[d] ~ normal(0,100);
    for (l in 1:L)
      beta[l,d] ~ normal(mu[d],sigma[d]);
  }
  for (n in 1:N)
    y[n] ~ bernoulli(inv_logit(x[n] * beta[ll[n]]));
}</code></pre>
<p>標準偏差のパラメータ<code>sigma</code>は, 下限値0の制約をつけて宣言されているので, 暗黙の一様事前分布<span class="math inline">(0, ∞)</span>を取ります. Stanでは, 事後分布が正則である限り, 非正則の事前分布を許します. とはいっても通常は, すべてのパラメータに情報事前分布, あるいは少なくとも弱情報事前分布をつけるのが有用です. 回帰係数とスケールの事前分布についてのおすすめは6.2節を参照してください.</p>
<h5 id="モデルの最適化">モデルの最適化</h5>
<p>可能なところでは, サンプリング文をベクトル化すると, 対数確率と導関数の評価が速くなります. 高速化の理由は, ループがなくなったからではなく, 対数確率と勾配計算の下位計算がベクトル化により共有されること, 勾配計算に必要な式木のサイズが減少することによります.</p>
<p>まず最初の最適化として, <code>D</code>についての<code>for</code>ループをベクトル化します.</p>
<pre><code>  mu ~ normal(0,100);
  for (l in 1:L)
    beta[l] ~ normal(mu,sigma);</code></pre>
<p><code>beta</code>はベクトルの配列として宣言されていますので, 式<code>beta[l]</code>はベクトルを示すことになります. <code>beta</code>を行列として宣言することもできたでしょうが, ベクトルの配列（あるいは2次元配列）の方が行へのアクセスはより効率的です. 配列, ベクトル, 行列の間での効率性のトレードオフについては第4章にさらに情報があります.</p>
<p>このモデルは, ベルヌーイ分布内で逆ロジットを使用しているのを, ロジットでパラメータ化したベルヌーイ分布に置き換えることで, さらに高速化し, 算術的にもより安定させることができます.</p>
<pre><code>for (n in 1:N)
  y[n] ~ bernoulli_logit(x[n] * beta[ll[n]]);</code></pre>
<p><code>bernoulli_logit</code>の定義は38.2節を参照してください.</p>
<p>RやBUGSとは異なり, ループや, 配列へのアクセスおよび代入は, Stanでは直接C++に変換されるので高速です. ほとんどの場合, コンテナに配置したり代入したりするコストは, 対数確率と勾配の計算をベクトル化することによる効率の増加でおつりが来ます. ですので, サンプリング文をループさせていた元の定式化よりも下のバージョンの方が高速です.</p>
<pre><code>{
  vector[N] x_beta_ll;
  for (n in 1:N)
    x_beta_ll[n] &lt;- x[n] * beta[ll[n]];
  y ~ bernoulli_logit(x_beta_ll);
}</code></pre>
<p>局所変数<code>x_beta_ll</code>のため, 中括弧で, 新しいスコープを導入しています. あるいは代わりに, <code>model</code>ブロックの最初で変数を宣言しても構いません.</p>
<p>上のように局所変数への代入を使うと, モデルが読みにくくなる場合があります. そのような場合には, まず読みやすいバージョンでモデルを開発, デバッグし, 単純な定式化でデバッグし終わってはじめて最適化の作業にかかるというようにするのがおすすめです.</p>
<h3 id="階層事前分布">6.9. 階層事前分布</h3>
<p>事前分布の事前分布は「超事前分布」とも呼ばれます. 超事前分布も, 下位レベルのパラメータの事前分布と同様に扱われるべきです. すなわち, 利用可能なだけの事前分布の情報がすべて使われるべきです. 超事前分布は, ほんの少数の下位レベルのパラメータにしか適用されないことが多いので, 事後分布が正則であることと, 事前分布の裾の重さに統計的にも計算的にも過度に敏感ではないことの両方が確かかどうか注意を払う必要があります.</p>
<h4 id="階層モデルのmleで-境界の値を推定させないようにするための事前分布">階層モデルのMLEで, 境界の値を推定させないようにするための事前分布</h4>
<p>階層モデルの設定における最尤推定(MLE)の基本的な問題は, 階層事前分布の分散が小さくなって, 階層事前分布の平均のまわりに値が集まり, 全体の密度が限度なく大きくなることです. 例として, <span class="math inline"><em>x</em><sub><em>n</em></sub> ∈ ℝ<sup><em>K</em></sup></span>についての<span class="math inline"><em>y</em><sub><em>n</em></sub> ∈ ℝ</span>の単純な階層線形回帰（事前分布の平均を固定）を考えます. 定式化は以下のとおりです.</p>
<p><br /><span class="math display">$$ \begin{array}{rl}y_{n} &amp;\sim \mathsf{Normal}(x_{n}\beta, \sigma)\\ \beta_{k} &amp;\sim \mathsf{Normal}(0, \tau)\\ \tau &amp;\sim \mathsf{Cauchy}(0, 2.5) \end{array} $$</span><br /></p>
<p>この場合で, <span class="math inline"><em>τ</em> → 0</span>かつ<span class="math inline"><em>β</em><sub><em>k</em></sub> → 0</span>となるとき, 事後密度</p>
<p><br /><span class="math display"><em>p</em>(<em>β</em>, <em>τ</em>, <em>σ</em> ∣ <em>y</em>, <em>x</em>) ∝ <em>p</em>(<em>y</em> ∣ <em>x</em>, <em>β</em>, <em>τ</em>, <em>σ</em>)</span><br /></p>
<p>は限度なく大きくなります. 図21.1にNealのじょうご密度の図がありますが, これと同様の挙動を示します.</p>
<p>この場合明らかに, <span class="math inline"><em>β</em></span>, <span class="math inline"><em>τ</em></span>, <span class="math inline"><em>σ</em></span>に最尤推定値はありません. したがって, 事後の最頻値を推測に使うなら, モデルを変えなければなりません. Chung et al. (2013)が勧めているのは, 以下のように事前分布にガンマ分布を使用する方法です.</p>
<p><br /><span class="math display"><em>τ</em> ∼ Gamma(2, 1/<em>A</em>)</span><br /> （訳注: 原文では'<span class="math inline"><em>σ</em></span>'ですが, '<span class="math inline"><em>τ</em></span>'の誤りと思われます）</p>
<p><span class="math inline"><em>A</em></span>には, <span class="math inline"><em>A</em> = 10</span>のようにかなり大きな値を与えます.</p>
<h3 id="項目反応理論モデル">6.10. 項目反応理論モデル</h3>
<p>項目反応理論(Item-response theory: IRT)は, 何人かの生徒がそれぞれ, 1つ以上のテスト問題群に答えるという状況をモデリングします. このモデルは, 生徒の能力と, 問題の難易度というパラメータに基づいています. より明瞭なモデルでは, 問題の識別性と, 当て推量で正解になる確率についてのパラメータもあります. 階層IRTモデルの入門用教科書についてはGelman and Hill (2007)を参照してください. また, さまざまなIRTモデルのBUGSによるコーディングについてはCurtis (2010)を参照してください.</p>
<h4 id="欠測のあるデータ宣言">欠測のあるデータ宣言</h4>
<p>IRTモデルに渡されるデータは, 全生徒が全問題に解答するとは限らないことを考慮すると, 以下のように宣言されるでしょう.</p>
<pre><code>data {
  int&lt;lower=1&gt; J;              // 生徒の数
  int&lt;lower=1&gt; K;              // 問題の数
  int&lt;lower=1&gt; N;              // 観測の数
  int&lt;lower=1,upper=J&gt; jj[N];  // 観測nの生徒
  int&lt;lower=1,upper=K&gt; kk[N];  // 観測nの問題
  int&lt;lower=0,upper=1&gt; y[N];   // 観測nの正誤
}</code></pre>
<p>この宣言では, 生徒-問題の組み合わせが全部で<code>N</code>あり, <code>1:N</code>の各<code>n</code>が, 2値の観測値<code>y[n]</code>のインデックスで, <code>y[n]</code>は, 生徒<code>jj[n]</code>が問題<code>kk[n]</code>に正答したかどうかを示します.</p>
<p>超パラメータの事前分布は, この節の後では簡単のためハードコーディングします. とはいえ, Stanではもっと柔軟にデータとしてコーディングできます.</p>
<h4 id="pl-rasch-モデル">1PL (Rasch) モデル</h4>
<p>1PL項目反応モデルは, Raschモデルともいわれ, 問題についての1つのパラメータ(1P)を持ち, ロジスティックリンク関数(L)を使います.</p>
<p>モデルのパラメータは以下のように宣言されます.</p>
<pre><code>parameters {
  real delta;          // 平均の生徒の能力
  real alpha[J];       // 生徒jの能力 - 平均能力
  real beta[K];        // 問題kの難易度
}</code></pre>
<p>パラメータ<code>alpha[j]</code>は生徒<code>j</code>の能力の係数, <code>beta[k]</code>は問題<code>k</code>の難易度の係数です. ここで使われている非標準のパラメータ化では, 切片項<code>delta</code>も含みます. これは, 平均的な生徒の平均的な問題への反応をあらわします. <sup>5</sup>モデル自体は以下のようになります.</p>
<pre><code>model {
  alpha ~ normal(0,1);      // 情報のある, 真値の事前分布
  beta ~ normal(0,1);       // 情報のある, 真値の事前分布
  delta ~ normal(.75,1);    // 情報のある, 真値の事前分布
  for (n in 1:N)
    y[n] ~ bernoulli_logit(alpha[jj[n]] - beta[kk[n]] + delta);
}</code></pre>
<p>このモデルはロジットでパラメータ化したベルヌーイ分布を使っています.</p>
<p><br /><span class="math display">bernoulli_logit(<em>y</em> ∣ <em>α</em>) = bernoulli(<em>y</em> ∣ logit<sup> − 1</sup>(<em>α</em>))</span><br /></p>
<p>このモデルを理解する鍵は, <code>bernoulli_logit</code>分布の中の項です. 以下の式に従います.</p>
<p><br /><span class="math display">Pr[<em>y</em><sub><em>n</em></sub> = 1] = <em>l</em><em>o</em><em>g</em><em>i</em><em>t</em><sup> − 1</sup>(<em>α</em><sub><em>j</em><em>j</em>[<em>n</em>]</sub> − <em>β</em><sub><em>k</em><em>k</em>[<em>n</em>]</sub> + <em>δ</em>)</span><br /></p>
<p>このモデルでは, 事前分布を設定しないと加法的な識別可能性の問題が発生します. 例えば, <span class="math inline"><em>α</em><sub><em>j</em></sub></span>と<span class="math inline"><em>β</em><sub><em>k</em></sub></span>の両方に<span class="math inline"><em>ξ</em></span>を加えると, 予測値が同じになります. <span class="math inline"><em>α</em></span>と<span class="math inline"><em>β</em></span>に平均を0とした事前分布を設定すると, パラメータが識別できます. 識別可能性の問題と, 識別可能にする他の手法についてはGelman and Hill (2007)を参照してください.</p>
<p>テスト用に, Stanとともに配布されているIRT 1PLモデルは, Rでデータをシミュレートするために使われる実際のデータ生成過程に合うような情報のある事前分布を使っています（このシミュレーションのコードはモデルと同じディレクトリに入っています）. 実際に利用するにはほとんどの場合は現実的ではありませんが, Stanの推定が有効であることは分かります. 事前分布の裾を重くして簡単な感度分析を行なうと, 400人の生徒に100問の問題で, 25%だけランダムに欠測するとしても, 事後分布は事前分布にかなり敏感です. 実際に使用するときには, 他のパラメータと同様に事前分布も階層的にすべきです. これは次の節で記述します.</p>
<p><sup>5</sup> Gelman and Hill (2007)は<span class="math inline"><em>δ</em></span>項を, 生徒の能力の分布における位置のパラメータと等価に扱っています.</p>
<h4 id="マルチレベル2plモデル">マルチレベル2PLモデル</h4>
<p>前の節で記述した単純な1PLモデルをこの節では, 問題にどのくらいのノイズがあるかをモデリングする識別パラメータを加え, 問題の難易度と識別パラメータにマルチレベルの事前分布を加えることで一般化します. モデルのパラメータは以下のように宣言されます.</p>
<pre><code>parameters {
  real mu_beta;                // 平均の生徒の能力
  real alpha[J];               // jの能力 - 平均
  real beta[K];                // kの難易度
  real&lt;lower=0&gt; gamma[K];      // kの識別性
  real&lt;lower=0&gt; sigma_beta;    // 難易度のスケール
  real&lt;lower=0&gt; sigma_gamma;   // 識別性のスケール
}</code></pre>
<p>モデルの定義を見た後の方がパラメータはよくわかるでしょう.</p>
<pre><code>model {
  alpha ~ normal(0,1);
  beta ~ normal(0,sigma_beta);
  gamma ~ lognormal(0,sigma_gamma);
  mu_beta ~ cauchy(0,5);
  sigma_alpha ~ cauchy(0,5);
  sigma_beta ~ cauchy(0,5);
  sigma_gamma ~ cauchy(0,5);
  for (n in 1:N)
    y[n] ~ bernoulli_logit(gamma[kk[n]]
                           * (alpha[jj[n]] - (beta[kk[n]] + mu_beta)));
}</code></pre>
<p>1PLモデルに似ていますが, 追加のパラメータ<code>gamma[k]</code>で, 問題<code>k</code>の識別能力をモデリングしています. <code>gamma[k]</code>が1より大きければ, ランダムに正答になる可能性は低くなり, より正誤（反応）が能力を如実に反映するようになります. パラメータ<code>gamma[k]</code>は正値に制約されています. これは, 能力が低い生徒の方により簡単な問題はないとするものです. そのような問題を耳にしないわけではありませんが, IRTモデルが利用されるような試験ではほとんどの場合, そのような問題は除かれる傾向にあります.</p>
<p>生徒の能力<code>alpha</code>に標準正規分布の事前分布を与えるようなモデルに, ここではパラメータ化しています. これは, このパラメータの位置とスケールの両方を識別可能にするためです. そうしなければ, その両方が識別不可能になるでしょう. 識別可能性については20章でさらに議論します. このモデルでは, 難易度と識別性のパラメータ<code>beta</code>と<code>gamma</code>のスケールはパラメータとし, 階層的に定めています. <code>beta</code>と<code>gamma</code>には以下のように階層的ではない, 弱情報事前分布を与えてもよいでしょう.</p>
<pre><code>beta ~ normal(0,5);
gamma ~ lognormal(0,2);</code></pre>
<p>ポイントは, <code>alpha</code>がスケールと位置を決定し, <code>beta</code>と<code>gamma</code>は変動できるようにしていることです.</p>
<p>パラメータ<code>beta</code>には, ここでは中央化しないパラメータ化を行なっています. パラメータ<code>mu_beta</code>が<code>beta</code>の平均の位置を与えています. あるいはまた, 以下のようにもできるでしょう.</p>
<pre><code>beta ~ normal(mu_beta, sigma_beta);</code></pre>
<p>および</p>
<pre><code>y[n] ~ bernoulli_logit(gamma[kk[n]] * (alpha[jj[n]] - beta[kk[n]]));</code></pre>
<p>中央化しないパラメータ化は階層モデルではより効率的になる傾向にあります. 中央化しない再パラメータ化についてもっと知るには21.2節を参照してください.</p>
<p>切片の項<code>mu_beta</code>はそれ自身は階層的にモデリングできません. そのため, 弱情報事前分布<span class="math inline">Cauchy(0, 5)</span>を与えています. 同様に, スケールの項<code>sigma_alpha</code>および<code>sigma_beta</code>, <code>sigma_gamma</code>には半コーシー分布の事前分布を与えています. 半コーシー分布の切断は暗黙のものです. 明示的な切断は必要ありません. これは, 対数確率は割合だけ計算すればよく, スケールの変数は宣言により(0,<span class="math inline">∞</span>)に制約されているからです.</p>
<h3 id="識別可能性のための事前分布">6.11. 識別可能性のための事前分布</h3>
<h4 id="位置とスケールの固定">位置とスケールの固定</h4>
<p>（階層）事前分布の応用の1つに, パラメータ群のスケールあるいは位置, またはその両方を識別させるということがあります. 例えば, 前の節で議論したIRTモデルでは, 位置とスケールの両方に識別不能性がありました. 一様事前分布を使うと, スケールと位置の両方の項でパラメータは変動するでしょう. これが推定にもたらす問題については, 20.1節に簡単な例がありますので, 参照してください.</p>
<p>生徒の能力のような1群の係数に標準正規分布（すなわち<span class="math inline">Normal(0, 1)</span>）の事前分布をを与えると, 識別不能性は解決されます. 生徒の能力に標準正規分布の事前分布を与えると, IRTモデルは識別され, 生徒の能力のパラメータについての1群の推定値が事後分布により生成されます. その標本平均は0に近く, 標本分散は1に近い値となるでしょう. 問題の難易度と識別性のパラメータには, 広がった, 理想的には階層的な事前分布を与えるべきです. そうすると, 生徒の能力のパラメータとの相対的な位置とスケールにより, これらパラメータは識別されるでしょう.</p>
<h4 id="共線性">共線性</h4>
<p>事前分布により識別可能になる別の例として, 線形回帰における共線性の場合があります. 線形回帰では, 2つの予測変数が共線的（すなわち一方が他方の線形関数となっている）ならば, 事後分布ではそれらの係数の相関係数は1（または-1)でしょう. これは識別不能になります. 係数の事前分布を正規分布とすることで識別できるようになる場合があります. そのような事前分布を使うと, 例えば, 全く同じ値をもつ説明変数が2つある場合に（共線性があることは明らかです）, 最も尤度が高くなるような2つの係数の値は, 片方の説明変数だけを使って推定して得た値の半分の値となります.</p>
<h4 id="分離可能性">分離可能性</h4>
<p>ロジスティック回帰では, 結果変数の値が1で予測変数が正のときや, 結果変数の値が0で予測変数が負のときには, こうした予測変数の係数の最尤推定値は無限に発散します. 係数に事前分布を与えることでこうした発散を制御することができます. これにより, 推定値を0に向けて「縮小」することで, 事後分布についてモデルは識別可能になるでしょう.</p>
<p>同様の問題は非正則平坦事前分布からのサンプリングでも発生します. このときのサンプラーは非常に大きな値を抽出しようとするでしょう. 事前分布を与えることで, 事後分布は有限の値のまわりに集中し, サンプリングはうまくいくでしょう.</p>
<h3 id="階層モデルにおける多変量の事前分布">6.12. 階層モデルにおける多変量の事前分布</h3>
<p>階層回帰モデルでは（他の状況でもありますが）, いくつかの個体レベルの変数に階層事前分布を割り当てることがあります. 例えば, 複数の変動する切片や傾きを含むモデルでは, 多変量の事前分布を割り当てることでしょう.</p>
<p>例として, 人を個体レベルとして, 収入を結果変数に, 教育水準と年齢を予測変数とし, 州などの地理区分を群としましょう. 切片のほか, 教育水準と年齢の効果も州ごとに変わりうるとします. さらに, 州ごとの収入や失業水準の平均といった, 州レベルの予測変数もあるとしましょう.</p>
<h4 id="多変量回帰の例">多変量回帰の例</h4>
<p>Gelman and Hill (2007)の13章・17章では, <span class="math inline"><em>N</em></span>個体が<span class="math inline"><em>J</em></span>群にまとめられる階層モデルについて議論しています. 各個体は, 長さ<span class="math inline"><em>K</em></span>の列ベクトル<span class="math inline"><em>x</em><sub><em>n</em></sub></span>からなる予測変数を持ちます. 記法の統一のため, <span class="math inline"><em>x</em><sub><em>n</em>, 1</sub> = 1</span>として, 切片を「1に固定された予測変数」にかかる係数として扱います. 群への所属をコード化するため, 個体<span class="math inline"><em>n</em></span>は群<span class="math inline"><em>j</em><em>j</em>[<em>n</em>] ∈ 1 : <em>J</em></span>に属するとしています. 各個体<span class="math inline"><em>n</em></span>はまた, 実数値の観測結果<span class="math inline"><em>y</em><sub><em>n</em></sub></span>を持ちます.</p>
<h5 id="尤度">尤度</h5>
<p>このモデルは, 群によって切片と係数が変動する線形回帰となりますので, <span class="math inline"><em>β</em><sub><em>j</em></sub></span>は, 群<span class="math inline"><em>j</em></span>についての<span class="math inline"><em>K</em></span>次元ベクトルの係数です. 個体<span class="math inline"><em>n</em></span>についての尤度関数は以下のとおりです.</p>
<p><br /><span class="math display"><em>y</em><sub><em>n</em></sub> ∼ Normal(<em>x</em><sub><em>n</em></sub><em>β</em><sub><em>j</em><em>j</em>[<em>n</em>]</sub>, <em>σ</em>) すべての<em>n</em> ∈ 1 : <em>N</em>について</span><br /></p>
<h5 id="係数の事前分布">係数の事前分布</h5>
<p>GelmanとHillは, 係数ベクトル<span class="math inline"><em>β</em><sub><em>j</em></sub></span>を, 平均ベクトル<span class="math inline"><em>μ</em></span>と共分散行列<span class="math inline">Σ</span>を持つ多変量分布から抽出されるとモデリングしています.</p>
<p><br /><span class="math display"><em>β</em><sub><em>j</em></sub> ∼ MultiNormal(<em>μ</em>, Σ) すべての<em>j</em> ∈ 1 : <em>J</em>について</span><br /></p>
<p>以下, GelmanとHillのフルモデルについて議論しますが, このモデルでは<span class="math inline"><em>μ</em></span>をモデリングするのに群レベルの予測変数を使っています. ここでは, <span class="math inline"><em>μ</em></span>は単純なベクトルのパラメータと仮定します.</p>
<h5 id="超パラメータ">超パラメータ</h5>
<p>階層モデリングでは, 群レベルの平均ベクトル<span class="math inline"><em>μ</em></span>と共分散行列<span class="math inline">Σ</span>自身に事前分布を与えなくてはなりません. 群レベルの平均ベクトルには, 独立した係数に合理的な弱情報事前分布を与えることができます.</p>
<p><br /><span class="math display"><em>μ</em><sub><em>j</em></sub> ∼ Normal(0, 5)</span><br /></p>
<p>もちろん, 係数<span class="math inline"><em>β</em><sub><em>j</em>, <em>k</em></sub></span>の期待値について知識があれば, その情報を<span class="math inline"><em>μ</em><sub><em>k</em></sub></span>の事前分布に取り入れることができます.</p>
<p>共分散行列の事前分布についてGelmanとHillは, 対角成分の標準偏差のスケールを分離した逆Wishart分布を使うことを提案しています. それを選ぶ理由は主として, 多変量の尤度関数に対して共役であり, Gibbsサンプリングが単純になるという利便性にあります.</p>
<p>Stanでは, 多変量の事前分布に共役の制限はありませんし, 実際のところやや違った手法をお勧めしています. GelmanとHillと同様, 事前分布をスケールと行列に分解しますが, 実際の変数のスケールと相関行列に基づいた, もっと自然な方法でできます. 具体的には以下のように定義します.</p>
<p><br /><span class="math display">Σ = <em>d</em><em>i</em><em>a</em><em>g</em>_<em>m</em><em>a</em><em>t</em><em>r</em><em>i</em><em>x</em>(<em>τ</em>)Ω<em>d</em><em>i</em><em>a</em><em>g</em>_<em>m</em><em>a</em><em>t</em><em>r</em><em>i</em><em>x</em>(<em>τ</em>)</span><br /></p>
<p>ここで<span class="math inline">Ω</span>は相関行列で, <span class="math inline"><em>τ</em></span>は係数のスケールのベクトルです.</p>
<p>スケールのベクトル<span class="math inline"><em>τ</em></span>の要素には, スケールの事前分布として合理的ならどのようなものでも与えることができますが, 下の, 小さいスケールの半コーシー分布のような弱情報のものをお勧めします.</p>
<p><br /><span class="math display"><em>τ</em><sub><em>k</em></sub> ∼ Cauchy(0, 2.5) すべての<em>k</em> ∈ 1 : <em>J</em>について, かつ<em>τ</em><sub><em>k</em></sub> &gt; 0</span><br /></p>
<p>事前平均については, 群間の係数の変動のスケールについて情報があるなら, <span class="math inline"><em>τ</em></span>の事前分布に取り入れるべきでしょう. 交換可能な係数が多数あるときは, <span class="math inline"><em>τ</em></span>自体の要素自体に（おそらく切片は除いて）階層事前分布が与えらえるでしょう.</p>
<p>最後に, 相関行列<span class="math inline">Ω</span>には, 形状パラメータ<span class="math inline"><em>ν</em> &gt; 1</span>のLKJ事前分布を与えることをお勧めします.</p>
<p><br /><span class="math display">Ω ∼ LKJcorr(<em>ν</em>)</span><br /></p>
<p>LKJ相関分布は51.1節で定義していますが, モデリングの基本的なアイデアは, <span class="math inline"><em>ν</em></span>が大きくなるにつれ, 事前分布が単位相関行列のまわりに集中するようになる（すなわち<span class="math inline"><em>β</em><sub><em>j</em></sub></span>の要素間の相関を低くする）ということです. <span class="math inline"><em>ν</em> = 1</span>のとき, LKJ相関分布は相関行列に対して一様分布（訳注: 原文では'identity distribution'ですが, 'uniform distribution'の誤りと思われます）を設定するのと等価になります. したがって, LKJ事前分布は, パラメータ<span class="math inline"><em>β</em><sub><em>j</em></sub></span>間の相関について期待する量を制御するのに用いることがあります.</p>
<h5 id="事前平均についての群レベルの予測変数">事前平均についての群レベルの予測変数</h5>
<p>GelmanとHillのモデルを完成させるため, 各群<span class="math inline"><em>j</em> ∈ 1 : <em>J</em></span>には, 群レベルの予測変数<span class="math inline"><em>u</em><sub><em>j</em></sub></span>の<span class="math inline"><em>L</em></span>次元の行ベクトルを与えるとします. すると<span class="math inline"><em>β</em><sub><em>j</em></sub></span>の事前平均は, <span class="math inline"><em>L</em></span>次元の係数ベクトル<span class="math inline"><em>g</em><em>a</em><em>m</em><em>m</em><em>a</em></span>を使って, それ自体を回帰としてモデリングできます. 群レベルの係数の事前分布は以下のようになります.</p>
<p><br /><span class="math display"><em>β</em><sub><em>j</em></sub> ∼ MultiNormal(<em>u</em><sub><em>j</em></sub><em>γ</em>, Σ)</span><br /></p>
<p>群レベルの係数<span class="math inline"><em>g</em><em>a</em><em>m</em><em>m</em><em>a</em></span>には, それ自体に弱情報事前分布を独立に与えてもよいでしょう.</p>
<p><br /><span class="math display"><em>γ</em><sub><em>l</em></sub> ∼ Normal(0, 5)</span><br /></p>
<p>いつもと同様に, 群レベルの平均についての情報は事前分布に取り入れるべきでしょう.</p>
<h5 id="stanでのモデルのコーディング">Stanでのモデルのコーディング</h5>
<p>群レベルの係数についての多変量の事前分布と, 群レベルの事前平均を与えた完全な階層モデルのStanのコードは以下のように定義されます.</p>
<pre><code>data {
  int&lt;lower=0&gt; N;              // 個体の数
  int&lt;lower=1&gt; K;              // 個体の予測変数の数
  int&lt;lower=1&gt; J;              // 群の数
  int&lt;lower=1&gt; L;              // 群の予測変数の数
  int&lt;lower=1,upper=J&gt; jj[N];  // 個体が属している群
  matrix[N,K] x;               // 個体の予測変数
  row_vector[L] u[J];          // 群の予測変数
  vector[N] y;                 // 結果変数
}
parameters {
  corr_matrix[K] Omega;        // 事前分布の相関
  vector&lt;lower=0&gt;[K] tau;      // 事前分布のスケール
  matrix[L,K] gamma;           // 群の係数
  vector[K] beta[J];           // 群ごとの個体の係数
  real&lt;lower=0&gt; sigma;         // 予測誤差のスケール
}
model {
  tau ~ cauchy(0,2.5);
  Omega ~ lkj_corr(2);
  to_vector(gamma) ~ normal(0, 5);
  {
    row_vector[K] u_gamma[J];
    for (j in 1:J)
      u_gamma[j] &lt;- u[j] * gamma;
    beta ~ multi_normal(u_gamma, quad_form_diag(Omega, tau));
  }
  {
    vector[N] x_beta_jj;
    for (n in 1:N)
      x_beta_jj[n] &lt;- x[n] * beta[jj[n]];
    y ~ normal(x_beta_jj, sigma);
  }
}</code></pre>
<p>超事前分布の共分散行列は, 2次形式でコード中で暗黙に定義されています. これは, 相関行列<code>Omega</code>とスケールのベクトル<code>tau</code>の方が出力を調べる目的にはより自然だからです. <code>Sigma</code>を出力するためには, 変換パラメータとして定義します. 関数<code>quad_form_diag</code>は, <code>quad_form_diag(Sigma,tau)</code>が<code>diag_matrix(tau) * Sigma * diag_matrix(tau)</code>と等価になるように定義されています. ここで, <code>diag_matrix_(tau)</code>は, 対角成分が<code>tau</code>となり, それ以外が0の行列を返します. ただし, <code>quad_form_diag</code>を使うバージョンの方が高速のはずです. 特別な行列演算についてさらに知るには34.2節を参照してください.</p>
<h5 id="ベクトル化による最適化">ベクトル化による最適化</h5>
<p>上のStanプログラムのコードでは, 予測変数<code>x[n]</code>と群レベルの係数<code>beta[jj[n]]</code>を使って, <code>x_beta_jj[n]</code>という平均を記述するベクトルを組み立てていました. <code>x_beta_jj</code>が局所変数として宣言できるように, その周りに中括弧でブロックを定義しました. このベクトル化したコードは, より単純な, ベクトル化していない次のコードと等価です.</p>
<pre><code>for (n in 1:N)
  y[n] ~ normal(x[n] * beta[jj[n]], sigma);</code></pre>
<p>上にあるStanプログラムのコードではまた, 結果変数の平均と多変量正規分布の平均とについてベクトル（の配列）を組み立てています. 解いて微分する必要のある線形系の数を減らすことで, 非常に顕著な高速化が可能になります.</p>
<pre><code>{
  matrix[K,K] Sigma_beta;
  Sigma_beta &lt;- quad_form_diag(Omega, tau);
  for (j in 1:J)
    beta[j] ~ multi_normal((u[j] * gamma)', Sigma_beta);
}</code></pre>
<p>この例では, 共分散行列<code>Sigma_beta</code>は, 二次形式の計算を<span class="math inline"><em>J</em></span>回繰り返さなくても良いように局所変数として定義されています. このベクトル化は, 次の節のコレスキー因子による最適化と組み合わせることもできます.</p>
<h5 id="コレスキー分解による最適化">コレスキー分解による最適化</h5>
<p>多変量正規密度と, 相関行列に対するLKJ事前分布はともに, 行列パラメータを因子分解する必要があります. これは, 前の節のようにベクトル化すると, 各密度について1度だけ行われることが保証されます. 効率性の点でも数値的な安定性の点でももっと良い解法は, 中央化しないパラメータ化の多変量版を使って, 相関行列のコレスキー因子により直接的にモデルをパラメータ化することです. 前の節のモデルについて, プログラム中の完全な行列の事前分布の部分を, 等価なコレスキー分解した事前分布に置き換えると以下のようになります.</p>
<pre><code>parameters {
  matrix[K,J] z;
  cholesky_factor_corr[K] L_Omega;
  ...
transformed parameters {
  matrix[J,K] beta;
  beta &lt;- u * gamma + (diag_pre_multiply(tau,L_Omega) * z)';
}
model {
  to_vector(z) ~ normal(0,1);
  L_Omega ~ lkj_corr_cholesky(2);
  ...</code></pre>
<p>新しいパラメータ<code>L_Omega</code>は, 元の相関行列<code>Omega</code>のコレスキー因子です. つまり, 以下のようになっています.</p>
<pre><code>Omega = L_Omega * L_Omega'</code></pre>
<p>事前分布のスケールのベクトル<code>tau</code>は変化ありません. さらに, コレスキー因子にスケールを左から掛ける(<strong>pre-multiplying</strong>)ことで, 最終的な共分散行列のコレスキー因子が生成されます.</p>
<pre><code>Sigma_beta
= quad_form_diag(Omega,tau)
= diag_pre_multiply(tau,L_Omega) * diag_pre_multiply(tau,L_Omega)'</code></pre>
<p>ここで, <code>a</code>を対角成分とした対角行列を作って左から掛ける複合演算は以下のように定義されています.</p>
<pre><code>diag_pre_multiply(a,b) = diag_matrix(a) * b</code></pre>
<p>新しい変数<code>z</code>は行列として宣言されており, 独立な標準正規分布の事前分布を与えられています. ここで, <code>to_vector</code>は行列をベクトルにする演算で, ベクトル化された引数に1変量の正規密度を与えるのに使うことができます. 共分散行列のコレスキー因子に<code>z</code>を掛けて, 平均<code>(u*gamma)'</code>を加えることで, 元のモデルと同じ分布となる<code>beta</code>が生成されます.</p>
<p>データ宣言は省略しますが, 前と同じモデルを最適化したものは以下のようになります.</p>
<pre><code>parameters {
  matrix[K,J] z;
  cholesky_factor_corr[K] L_Omega;
  vector&lt;lower=0&gt;[K] tau;      // 事前分布のスケール
  matrix[L,K] gamma;           // 群の係数
  real&lt;lower=0&gt; sigma;         // 予測誤差のスケール
}
transformed parameters {
  matrix[J,K] beta;
  beta &lt;- u * gamma + (diag_pre_multiply(tau,L_Omega) * z)';
}
model {
  vector[N] x_beta_jj;
  for (n in 1:N)
    x_beta_jj[n] &lt;- x[n] * beta[jj[n]]';
  y ~ normal(x_beta_jj, sigma);

  tau ~ cauchy(0,2.5);
  to_vector(z) ~ normal(0,1);
  L_Omega ~ lkj_corr_cholesky(2);
  to_vector(gamma) ~ normal(0,5);
}</code></pre>
<h3 id="予測-フォアキャストとバックキャスト">6.13. 予測, フォアキャストとバックキャスト</h3>
<p>Stanのモデルは, モデル中の任意の未知量の値を「予測」するのに使うことができます. 予測が将来についてなら「フォアキャスト」と呼ばれますし, 気候の再現や宇宙論であるように, 過去についてなら「バックキャスト」と呼ばれることがあります（「フォア」の対義語についての書き手の感覚により, 「アフトキャスト」や「ハインドキャスト」, 「アンテキャスト」とも呼ばれます）.</p>
<h4 id="予測のプログラミング">予測のプログラミング</h4>
<p>以下の線形回帰は単純な例ですが, まさに最初の例と同じ設定になっています. すなわち, <code>N</code>個の観測値<code>y</code>と, 長さ<code>N</code>の予測変数のベクトル<code>x</code>を使って, 係数<code>beta</code>を推定します. このモデルのパラメータと, 観測値のモデルは前とまったく同じです.</p>
<p>予測のためには, 予測の数<code>N_new</code>とその予測変数の行列<code>x_new</code>を与える必要があります. 予測値自身はパラメータ<code>y_new</code>としてモデリングされています. 新しい結果変数のベクトル<code>y_new</code>と予測変数の行列<code>x_new</code>がありますが, 予測値のモデル文は観測値とまったく同じです.</p>
<pre><code>data {
  int&lt;lower=1&gt; K;
  int&lt;lower=0&gt; N;
  matrix[N,K] x;
  vector[N] y;
  int&lt;lower=0&gt; N_new;
  matrix[N_new, K] x_new;
}
parameters {
  vector[K] beta;
  real&lt;lower=0&gt; sigma;

  vector[N_new] y_new;                  // 予測値
}
model {
  y ~ normal(x * beta, sigma);          // 観測モデル

  y_new ~ normal(x_new * beta, sigma);  // 予測モデル
}</code></pre>
<h4 id="生成量としての予測">生成量としての予測</h4>
<p>可能であれば, <code>generated quantities</code>ブロックを使うのが予測値を生成するのに最も効率的な方法です. これにより正則なモンテカルロ（マルコフ連鎖モンテカルロではありません）の推定値が得られます. これはiterationごとに, はるかに効率的にサンプルを作ることができます.</p>
<pre><code>...上のデータ...

parameters {
  vector[K] beta;
  real&lt;lower=0&gt; sigma;
}
model {
  y ~ normal(x * beta, sigma);
}
generated quantities {
  vector[N_new] y_new;
  for (n in 1:N_new)
    y_new[n] &lt;- normal_rng(x_new[n] * beta, sigma);
}</code></pre>
<p>ここでは, データは前と同じですが, パラメータ<code>y_new</code>が新しく生成量(generated quantity)として宣言されています. また, 予測モデルは<code>model</code>から取り除かれ, 正規分布から擬似乱数を使って抽出されるようになっています.</p>
<h3 id="多変量の結果変数">6.14. 多変量の結果変数</h3>
<p>ほとんどの回帰の設定では, 1変量の観測値（スカラー値や論理値, カテゴリカル値, 順序値, 計数値など）をモデリングします. 多項回帰も, カテゴリカル回帰の繰り返しにすぎません. これとは対照的に, 各観測値が多変量の場合の回帰をこの節では議論します. 複数の結果変数を回帰の設定で関連づけるために, それらの誤差項に共分散構造を与えます.</p>
<p>この節では2つの場合について考えます. 連続の多変量の量についての見かけ上無関係な回帰と, 論理値の多変量の量についての多変量プロビット回帰です.</p>
<h4 id="見かけ上無関係な回帰">見かけ上無関係な回帰</h4>
<p>最初に考えるモデルは, 計量経済学の「見かけ上無関係な」回帰(seemingly unrelated regression: SUR)です. いくつかの線形回帰が予測変数を共有し, 独立な誤差ではなく共分散誤差構造を使います(Zellner, 1962; Greene, 2011).</p>
<p>このモデルは回帰として簡単に書けます.</p>
<p><br /><span class="math display">$$ \begin{array}{rl} y_{n} &amp;= x_{n}\beta + \epsilon_{n}\\ \epsilon_{n} &amp;\sim \mathsf{MultiNormal}(0, \Sigma) \end{array} $$</span><br /></p>
<p>ここで, <span class="math inline"><em>x</em><sub><em>n</em></sub></span>は<span class="math inline"><em>J</em></span>次元の予測変数の行ベクトル（<span class="math inline"><em>x</em></span>は<span class="math inline">(<em>N</em> × <em>J</em>)</span>行列です）, <span class="math inline"><em>y</em><sub><em>n</em></sub></span>は<span class="math inline"><em>K</em></span>次元の観測値のベクトル, <span class="math inline"><em>β</em></span>は回帰係数の<span class="math inline">(<em>K</em> × <em>J</em>)</span>行列（ベクトル<span class="math inline"><em>β</em><sub><em>k</em></sub></span>は, 結果変数のベクトルの<span class="math inline"><em>k</em></span>番目の要素に関する係数を収容します）, <span class="math inline">Σ</span>は誤差を決める共分散行列です. いつものように切片は, 1の値の列として<span class="math inline"><em>x</em></span>の中に含めることができます.</p>
<p>基本的なStanのコードは単純です（とはいえ, 相関に対してLKJ事前分布を使用してもっと最適化したコードをその後で示します）.</p>
<pre><code>data {
  int&lt;lower=1&gt; K;
  int&lt;lower=1&gt; J;
  int&lt;lower=0&gt; N;
  vector[J] x[N];
  vector[K] y[N];
}
parameters {
  matrix[K,J] beta;
  cov_matrix[K] Sigma;
}
model {
  vector[K] mu[N];
  for (n in 1:N)
    mu[n] &lt;- beta * x[n];
  y ~ multi_normal(mu, Sigma);
}</code></pre>
<p>効率性のため, 平均のベクトルの配列を前もって計算し, 同じ共分散行列は共有して, 多変量正規分布はベクトル化しています.</p>
<p>6.12節での助言にしたがって, 回帰係数の事前分布には正規分布の弱情報事前分布を, 相関にはLKJ事前分布を, 標準偏差には半コーシー分布の事前分布を設定しましょう. 共分散構造は, 効率性と算術的な安定性のため, コレスキー因子によりパラメータ化されます.</p>
<pre><code>...
parameters {
  matrix[K,J] beta;
  cholesky_factor_corr[K] L_Omega;
  vector&lt;lower=0&gt;[K] L_sigma;
}
model {
  vector[K] mu[N];
  matrix[K,K] L_Sigma;

  for (n in 1:N)
    mu[n] &lt;- beta * x[n];

  L_Sigma &lt;- diag_pre_multiply(L_sigma, L_Omega);

  to_vector(beta) ~ normal(0, 5);
  L_Omega ~ lkj_corr_cholesky(4);
  L_sigma ~ cauchy(0, 2.5);

  y ~ multi_normal_cholesky(mu, L_Sigma);
}</code></pre>
<p>共分散行列のコレスキー因子はここでは局所変数として再構築され, 相関行列のコレスキー因子によりスケーリングされてモデルで使われています. 行列<code>beta</code>をベクトルに変換することにより1度にまとめて回帰係数に事前分布を設定しています.</p>
<p>必要なら, <code>generated quantities</code>ブロックで, コレスキー因子から完全な相関または共分散行列を再構築してもよいでしょう.</p>
<h4 id="多変量プロビット回帰">多変量プロビット回帰</h4>
<p>多変量プロビットモデルは, 見かけ上無関係な回帰の結果変数に階段関数を適用することにより一連の論理値の変数を生成します.</p>
<p>観測値<span class="math inline"><em>y</em><sub><em>n</em></sub></span>は, 論理値（0を偽, 1を真とコーディングします）からなる<span class="math inline"><em>D</em></span>次元ベクトルです. <span class="math inline"><em>y</em><sub><em>n</em></sub></span>の値は, 見かけ上無関係な回帰モデル（前の節を参照）から抽出される潜在変数<span class="math inline"><em>z</em><sub><em>n</em></sub></span>によって決まります.</p>
<p><br /><span class="math display">$$ \begin{array}{rl} z_{n} &amp;= x_{n}\beta + \epsilon_{n}\\ \epsilon_{n} &amp;\sim \mathsf{MultiNormal}(0, \Sigma) \end{array} $$</span><br /></p>
<p>次に, 階段関数を適用して, 以下で定義される要素からなる論理値の<span class="math inline"><em>D</em></span>次元ベクトル<span class="math inline"><em>y</em><sub><em>n</em></sub></span>を生成します. （訳注: 原文では'<span class="math inline"><em>K</em></span>-vector <span class="math inline"><em>z</em><sub><em>n</em></sub></span>とありますが, 誤りと思われるので上のように修正しています. 以下の式でも<span class="math inline"><em>k</em></span>を<span class="math inline"><em>d</em></span>に修正しています. ）</p>
<p><br /><span class="math display"><em>y</em><sub><em>n</em>, <em>d</em></sub> = <em>I</em>(<em>z</em><sub><em>n</em>, <em>d</em></sub> &gt; 0)</span><br /></p>
<p>ここで, I()は指示関数で, 引数が真なら1の, そうでなければ0の値をとります.</p>
<p>見かけ上無関係な回帰モデルの場合とは異なり, ここでは共分散行列<span class="math inline">Σ</span>は単位標準偏差をとります（すなわち, 相関行列になります）. 通常のプロビット回帰やロジスティック回帰と同様に, スケールが変動するとモデル（スケールではなく, 0についての切断点によってのみ定義されています）が識別不能になります（Greene (2011)を参照）.</p>
<p>多変量プロビットモデルはStanでは, Albert and Chib (1993)によって導入されたトリックを使ってコーディングできます. この方法では, 基礎となる連続値のベクトル<span class="math inline"><em>y</em><sub><em>n</em></sub></span>が切断パラメータとしてコーディングされます. Stanでのこのモデルのコーディングの鍵は, 対応する<span class="math inline"><em>y</em></span>の値が0か1かにもとづいて, 潜在ベクトル<span class="math inline"><em>z</em></span>を2つに分けて宣言することです. さもないと, このモデルは前の節の見かけ上無関係な回帰モデルと同じになってしまいます.</p>
<p>最初に, 整数の2次元配列についての合計関数を導入します. これは, <span class="math inline"><em>y</em></span>の中にどれだけの1が合計してあるのかを計算するのに役立ちます.</p>
<pre><code>functions {
  int sum(int[,] a) {
    int s;
    s &lt;- 0;
    for (i in 1:size(a))
      s &lt;- s + sum(a[i]);
    return s;
  }
}</code></pre>
<p>この関数はちょっとしたことですが, Stanには組み込まれていませんし, モデリングに集中できるように独自の関数として定義しておくと, このモデルの残りの部分をわかりやすくします.</p>
<p>データ宣言ブロックは, 見かけ上無関係な回帰ととてもよく似ていますが, 観測値<code>y</code>は今回は, 0か1かに制約された整数です.</p>
<pre><code>data {
  int&lt;lower=1&gt; K;
  int&lt;lower=1&gt; D;
  int&lt;lower=0&gt; N;
  int&lt;lower=0,upper=1&gt; y[N,D];
  vector[K] x[N];
}</code></pre>
<p>データの宣言の後には<code>transformed data</code>ブロックがあります. これは, データの配列<code>y</code>を真か偽かの成分に分けるというただ1つの目的のために必要になったものです. これにより, <code>transformed parameters</code>ブロックで<code>z</code>が容易に再構築できるようにインデックスの状態を把握します.</p>
<pre><code>transformed data {
  int&lt;lower=0&gt; N_pos;
  int&lt;lower=1,upper=N&gt; n_pos[sum(y)];
  int&lt;lower=1,upper=D&gt; d_pos[size(n_pos)];
  int&lt;lower=0&gt; N_neg;
  int&lt;lower=1,upper=N&gt; n_neg[(N * D) - size(n_pos)];
  int&lt;lower=1,upper=D&gt; d_neg[size(n_neg)];

  N_pos &lt;- size(n_pos);
  N_neg &lt;- size(n_neg);
  {
    int i;
    int j;
    i &lt;- 1;
    j &lt;- 1;
    for (n in 1:N) {
      for (d in 1:D) {
        if (y[n,d] == 1) {
          n_pos[i] &lt;- n;
          d_pos[i] &lt;- d;
          i &lt;- i + 1;
        } else {
          n_neg[j] &lt;- n;
          d_neg[j] &lt;- d;
          j &lt;- j + 1;
        }
      }
    }
  }
}</code></pre>
<p>変数<code>N_pos</code>と<code>N_neg</code>には, 観測値<code>y</code>の真(1)の数と偽(0)の数とが入ります. そのループではそれから, 真と偽の値についての一連のインデックスで4つの配列を埋めます.</p>
<p>パラメータは以下のように宣言されます.</p>
<pre><code>parameters {
  matrix[D,K] beta;
  cholesky_factor_corr[D] L_Omega;
  vector&lt;lower=0&gt;[N_pos] z_pos;
  vector&lt;upper=0&gt;[N_neg] z_neg;
}</code></pre>
<p>ここには, 回帰係数<code>beta</code>と, 相関行列のコレスキー因子<code>L_Omega</code>が含まれます. 共分散行列は単位スケール（すなわち相関行列です. 上を参照）なので, 今回はスケーリングはありません.</p>
<p>パラメータ宣言の重要なところは, 潜在実数変数<code>z</code>が真のみの成分と, 偽のみの成分とに分解されていることです. そのサイズは, 便利なことに<code>transformed data</code>ブロックで計算されています. <code>transformed data</code>ブロックの実際の仕事は, <code>transformed parameters</code>ブロックで<code>z</code>を再構築できるようにすることでした.</p>
<pre><code>transformed parameters {
  vector[D] z[N];
  for (n in 1:N_pos)
    z[n_pos[n], d_pos[n]] &lt;- z_pos[n];
  for (n in 1:N_neg)
    z[n_neg[n], d_neg[n]] &lt;- z_neg[n];
}</code></pre>
<p>ここまで来るとモデルは単純で, 見かけ上無関係な回帰とかなりよく似ています.</p>
<pre><code>model {
  L_Omega ~ lkj_corr_cholesky(4);
  to_vector(beta) ~ normal(0, 5);
  {
    vector[D] beta_x[N];
    for (n in 1:N)
      beta_x[n] &lt;- beta * x[n];
    z ~ multi_normal_cholesky(beta_x, L_Omega);
  }
}</code></pre>
<p>モデルがこのように単純になったのは, <code>z</code>についてのAlbertとChib式の制約のおかげです.</p>
<p>最後に, 相関行列自体が必要なら, <code>generated quantities</code>ブロックで戻すことができます.</p>
<pre><code>generated quantities {
  corr_matrix[D] Omega;
  Omega &lt;- multiply_lower_tri_self_transpose(L_Omega);
}</code></pre>
<p>もちろん, 前の節の見かけ上無関係な回帰でも同じことができるでしょう.</p>
<h2 id="時系列モデル">7. 時系列モデル</h2>
<p>時系列データは, 時間軸に沿って得られるデータです. この章では2種類の時系列モデルを紹介します. 1つは, 自己回帰および移動平均モデルといった回帰に似たモデル, もう1つは隠れマルコフモデルです.</p>
<p>15章ではガウス過程を紹介しますが, これも時系列（と空間）データに使えるでしょう.</p>
<h3 id="自己回帰モデル">7.1. 自己回帰モデル</h3>
<p>正規ノイズの1次自己回帰モデル(AR(1))では, 各点<span class="math inline"><em>y</em><sub><em>n</em></sub></span>は次式のように生成される系列<span class="math inline"><em>y</em></span>にまとめられます.</p>
<p><br /><span class="math display"><em>y</em><sub><em>n</em></sub> ∼ Normal(<em>α</em> + <em>β</em><em>y</em><sub><em>n</em> − 1</sub>, <em>σ</em>)</span><br /></p>
<p>すなわち, <span class="math inline"><em>y</em><sub><em>n</em></sub></span>の期待値は<span class="math inline"><em>α</em> + <em>β</em><em>y</em><sub><em>n</em> − 1</sub></span>で, ノイズの大きさは<span class="math inline"><em>σ</em></span>です.</p>
<h4 id="ar1モデル">AR(1)モデル</h4>
<p>傾き（<span class="math inline"><em>β</em></span>）, 切片（<span class="math inline"><em>α</em></span>）, ノイズのスケール（<span class="math inline"><em>σ</em></span>）のパラメータに非正則平坦一様分布を設定するなら, AR(1)モデルのStanプログラムは以下のようになります.</p>
<pre><code>data {
  int&lt;lower=0&gt; N;
  vector[N] y;
}
parameters {
  real alpha;
  real beta;
  real&lt;lower=0&gt; sigma;
}
model {
  for (n in 2:N)
    y[n] ~ normal(alpha + beta * y[n-1], sigma);
}</code></pre>
<p>最初の観測データ点<code>y[1]</code>はここではモデル化されていません. これは条件となるものが何もないからです. そのかわり, <code>y[1]</code>は<code>y[2]</code>の条件となっています. このモデルではまた<code>sigma</code>に非正則事前分布を使っていますが, もし<code>y</code>の時間的変化のスケールに情報があるのなら, 問題なく情報事前分布を加えることができます. あるいは, <code>y</code>のスケールに大ざっぱな知識があるのであれば弱情報事前分布で推定を良くすることもできます.</p>
<h5 id="スライシングで効率よく">スライシングで効率よく</h5>
<p>おそらく少し読みにくくはなりますが, 上のモデルをとても効率良く記述する方法がベクトルのスライシングです. 上のモデルは1行で書けます.</p>
<pre><code>model {
  tail(y, N - 1) ~ normal(alpha + beta * head(y, N - 1), sigma);
}</code></pre>
<p><code>tail</code>演算は, <code>y</code>の末尾から<code>N - 1</code>個の要素を取り出すもので, <code>head</code>演算は最初の<code>N - 1</code>個を取り出すものです. <code>head</code>の要素に<code>beta</code>を掛けるのにはベクトル計算を使っています.</p>
<h4 id="ar1モデルの拡張">AR(1)モデルの拡張</h4>
<p>回帰係数とノイズのスケールには, さまざまな別の分布族の正則事前分布を設定することもできるでしょう. 正規分布ノイズのモデルは, スチューデントのt分布はじめ, 上下限のない分布に変えることができます. 複数の観測系列があるようなら, このモデルは階層的にもできるでしょう.</p>
<p>強制的に定常AR(1)過程として推定するなら, 傾きの係数<code>beta</code>には以下のように上下限に制約をつけてもよいかもしれません.</p>
<pre><code>real&lt;lower=-1,upper=1&gt; beta;</code></pre>
<p>実際には, こうした制約はおすすめしません. データが定常でないなら, モデルのあてはめのときにそれがわかるというのが最善です. <code>beta</code>の値が零近辺に集まるような事前分布を設定することで, 定常パラメーターが推定しやすくなるでしょう.</p>
<h4 id="ar2モデル">AR(2)モデル</h4>
<p>このモデルの次数を拡張するのも簡単です. 例えば, 2次の係数<code>gamma</code>を入れて, 以下のモデル文のようにAR(2)モデルをコーディングできるでしょう.</p>
<pre><code>for (n in 3:N)
  y[n] ~ normal(alpha + beta*y[n-1] + gamma*y[n-2], sigma);</code></pre>
<h4 id="arkモデル">AR(<span class="math inline"><em>K</em></span>)モデル</h4>
<p>次数自体もデータとして与えるような一般モデルは, 係数を配列に入れ, 線形予測子をループ内で計算させるようにしてコーディングできます.</p>
<pre><code>data {
  int&lt;lower=0&gt; K;
  int&lt;lower=0&gt; N;
  real y[N];
}
parameters {
  real alpha;
  real beta[K];
  real sigma;
}
model {
  for (n in (K+1):N) {
    real mu;
    mu &lt;- alpha;
    for (k in 1:K)
      mu &lt;- mu + beta[k] * y[n-k];
    y[n] ~ normal(mu, sigma);
  }
}</code></pre>
<h4 id="arch1モデル">ARCH(1)モデル</h4>
<p>計量経済学と財政学の時系列モデルでは不等分散を仮定するのが普通です（すなわち, 系列を定義するノイズ項のスケールが時間的に変化してもよいとします）. そのようなモデルで最も単純なのがARCH（AugoRegressive Conditional Heteroscedasticity, 自己回帰条件付き不等分散）モデルです(Engle, 1982). 自己回帰モデルAR(1)では, 系列の平均が時間的に変化しますが, ノイズ項は固定されたままです. ARCH(1)モデルでは, これとは異なり, ノイズ項のスケールが時間的に変化する一方で平均項は固定されたままです. もちろん, 平均もスケールも時間的に変化するとモデルを定義することもできるでしょう. 計量経済学の文献には多種多様な時系列モデリングの選択肢があります.</p>
<p>ARCH(1)モデルは典型的には以下の一連の式で示されます. ここで, <span class="math inline"><em>r</em><sub><em>t</em></sub></span>は時点<span class="math inline"><em>t</em></span>における収益の観測値, <span class="math inline"><em>μ</em></span>, <span class="math inline"><em>α</em><sub>0</sub></span>, <span class="math inline"><em>α</em><sub>1</sub></span>は未知の回帰係数パラメーターです.</p>
<p><br /><span class="math display">$$\begin{array}{rl}r_{t} &amp;= \mu + a_{t} \\ a_{t} &amp;= \sigma_{t} \epsilon_{t} \\ \epsilon_{t} &amp;\sim \mathsf{Normal}(0, 1) \\ \sigma_{t}^{2} &amp;= \alpha_{0} + \alpha_{1}a_{t-1}^{2}\end{array}$$</span><br /></p>
<p>ノイズ項<span class="math inline"><em>σ</em><sub><em>t</em></sub><sup>2</sup></span>が正であることを保証するため, <span class="math inline"><em>α</em><sub>0</sub>, <em>α</em><sub>1</sub> &gt; 0</span>と, スケール係数は正に制約されています. 時系列の定常性を保証するため, <span class="math inline"><em>α</em><sub>1</sub> &lt; 1</span>と, 傾きは1未満に制約されています. <sup>1</sup>ARCH(1)モデルはStanでは以下のようにそのままコーディングできます.</p>
<pre><code>data {
  int&lt;lower=0&gt; T;   // 時点の数
  real r[T];        // 時点tにおける収益
}
parameters {
  real mu;                       // 平均収益
  real&lt;lower=0&gt; alpha0;          // 誤差の切片
  real&lt;lower=0,upper=1&gt; alpha1;  // 誤差の傾き
}
model {
  for (t in 2:T)
    r[t] ~ normal(mu, sqrt(alpha0 + alpha1 * pow(r[t-1] - mu,2)));
}</code></pre>
<p>このモデルのループは, 時点<span class="math inline"><em>t</em> = 1</span>における収益をモデル化しないように定義されています. 次節のモデルで, <span class="math inline"><em>t</em> = 1</span>における収益をモデル化する方法をお見せします. このモデルは, ベクトル化してより効率的にすることができません. 次節のモデルでベクトル化の例を紹介します.</p>
<p><sup>1</sup>実際には, この制約を外してみて, 非定常な係数の組み合わせの方がデータによく当てはまるかどうかを試すのが有用なこともあります. あるいはまた, 当てはめから外れているトレンドがあるなら明らかに非定常でしょうから, モデルにトレンド項を加えることもあります.</p>
<h3 id="時間的不等分散性のモデリング">7.2. 時間的不等分散性のモデリング</h3>
<p>一揃いの変数について, 分散がすべて同じなら, 等分散ということなります. 一方, 分散がすべて同じというわけでないなら, 不等分散ということになります. 不等分散の時系列モデルでは, ノイズ項が時間的に変化してもよいとします.</p>
<h4 id="garch11モデル">GARCH(1,1)モデル</h4>
<p>基本的なGARCH（Generalized AutoRegressive Conditional Heteroscedasticity, 一般化自己回帰条件付き不等分散）モデルであるGARCH(1,1)はARCH(1)モデルを拡張したもので, 一期前の時点<span class="math inline"><em>t</em> − 1</span>での収益の平均との差の2乗を, 時点<span class="math inline"><em>t</em></span>のボラティリティの予測変数に含みます.</p>
<p><br /><span class="math display"><em>σ</em><sub><em>t</em></sub><sup>2</sup> = <em>α</em><sub>0</sub> + <em>α</em><sub>1</sub><em>a</em><sub><em>t</em> − 1</sub><sup>2</sup> + <em>β</em><sub>1</sub><em>σ</em><sub><em>t</em> − 1</sub><sup>2</sup></span><br /></p>
<p>スケール項が正であることと時系列が定常であることを保証するため, 係数については, <span class="math inline"><em>α</em><sub>0</sub>, <em>α</em><sub>1</sub>, <em>β</em><sub>1</sub> &gt; 0</span>, かつ傾きについて<span class="math inline"><em>α</em><sub>1</sub> + <em>β</em><sub>1</sub> &lt; 1</span>をすべて満たさなくてなりません.</p>
<pre><code>data {
  int&lt;lower=0&gt; T;
  real r[T];
  real&lt;lower=0&gt; sigma1;
}
parameters {
  real mu;
  real&lt;lower=0&gt; alpha0;
  real&lt;lower=0,upper=1&gt; alpha1;
  real&lt;lower=0,upper=(1-alpha1)&gt; beta1;
}
transformed parameters {
  real&lt;lower=0&gt; sigma[T];
  sigma[1] &lt;- sigma1;
  for (t in 2:T)
    sigma[t] &lt;- sqrt(alpha0
                     + alpha1 * pow(r[t-1] - mu, 2)
                     + beta1 * pow(sigma[t-1], 2));
}
model {
  r ~ normal(mu,sigma);
}</code></pre>
<p>ボラティリティ回帰の再帰的定義の最初を決めるために, <span class="math inline"><em>t</em> = 1</span>におけるノイズのスケールとして, 非負の値を持つ<code>sigma1</code>をデータ宣言に含めます.</p>
<p>制約はそのままパラメータ宣言でコーディングされています. この宣言は, <code>alpha1</code>の値が<code>beta1</code>に依存するという制約があるので, この順序どおりにする必要があります.</p>
<p>非負値の配列である変換パラメータ(transformed parameter)<code>sigma</code>は各時点のスケールの値を格納するのに使われます. これらの値の定義は<code>transformed parameters</code>ブロックにあり, 回帰もここで定義されるようにしました. 切片<code>alpha0</code>, 1期前の収益と平均との差の2乗に対する傾き<code>alpha1</code>, 1期前のノイズスケールの2乗に対する傾き<code>beta1</code>がここにあります. 最後に, Stanでは正規分布には（分散パラメータではなく）スケール（偏差）パラメータが必要なので, 回帰全体を<code>sqrt</code>関数の中に入れています.</p>
<p><code>transformed parameters</code>ブロックに回帰を置くことにより, モデルは, ベクトル化されたサンプリング文1行にまで減りました. <code>r</code>と<code>sigma</code>の長さは<code>T</code>ですので, すべてのデータが直接モデル化されています.</p>
<h3 id="移動平均モデル">7.3. 移動平均モデル</h3>
<p>移動平均モデルは, 過去の誤差を将来の結果の予測変数として使います. 次数<span class="math inline"><em>Q</em></span>の移動平均モデルMA(<span class="math inline"><em>Q</em></span>)には, 全体的な平均パラメータ<span class="math inline"><em>μ</em></span>と, 過去の誤差項についての回帰係数<span class="math inline"><em>θ</em><sub><em>q</em></sub></span>があります. 時点<span class="math inline"><em>t</em></span>における誤差を<span class="math inline"><em>ϵ</em><sub><em>t</em></sub></span>として, 結果<span class="math inline"><em>y</em><sub><em>t</em></sub></span>についてのモデルは次のように定義されます.</p>
<p><br /><span class="math display"><em>y</em><sub><em>t</em></sub> = <em>μ</em> + <em>θ</em><sub>1</sub><em>ϵ</em><sub><em>t</em> − 1</sub> + … + <em>θ</em><sub><em>Q</em></sub><em>ϵ</em><sub><em>t</em> − <em>Q</em></sub> + <em>ϵ</em><sub><em>t</em></sub></span><br /></p>
<p>結果<span class="math inline"><em>y</em><sub><em>t</em></sub></span>についての誤差項<span class="math inline"><em>ϵ</em><sub><em>t</em></sub></span>は正規分布としてモデル化されています.</p>
<p><br /><span class="math display"><em>ϵ</em><sub><em>t</em></sub> ∼ Normal(0, <em>σ</em>)</span><br /></p>
<p>正則なベイズモデルでは, <span class="math inline"><em>μ</em></span>, <span class="math inline"><em>θ</em></span>, <span class="math inline"><em>σ</em></span>にはすべて事前分布を与える必要があります.</p>
<h4 id="ma2の例">MA(2)の例</h4>
<p>MA(2)モデルはStanでは以下のようにコーディングできます.</p>
<pre><code>data {
  int&lt;lower=3&gt; T;  // 観測値の数
  vector[T] y;     // 時点Tにおける観測値
}
parameters {
  real mu;              // 平均
  real&lt;lower=0&gt; sigma;  // 誤差のスケール
  vector[2] theta;      // ラグの係数
}
transformed parameters {
  vector[T] epsilon;    // 誤差項
  epsilon[1] &lt;- y[1] - mu;
  epsilon[2] &lt;- y[2] - mu - theta[1] * epsilon[1];
  for (t in 3:T)
   epsilon[t] &lt;- ( y[t] - mu
                   - theta[1] * epsilon[t - 1]
                   - theta[2] * epsilon[t - 2] );
}
model {
  mu ~ cauchy(0,2.5);
  theta ~ cauchy(0,2.5);
  sigma ~ cauchy(0,2.5);
  for (t in 3:T)
    y[t] ~ normal(mu
                  + theta[1] * epsilon[t - 1]
                  + theta[2] * epsilon[t - 2],
                  sigma);
}</code></pre>
<p>誤差項<span class="math inline"><em>ϵ</em><sub><em>t</em></sub></span>は, 観測値とパラメータを使って変換パラメータ(transformed parameter)として定義されています. （尤度を定義する）サンプリング文の定義もこれと同じ定義を使っていますが, <span class="math inline"><em>n</em> &gt; <em>Q</em></span>の<span class="math inline"><em>y</em><sub><em>n</em></sub></span>にのみ適用可能です. この例では, パラメータにはコーシー事前分布（<span class="math inline"><em>σ</em></span>には半コーシー分布）を与えています. もっとも, ほかの事前分布も同じくらい簡単に使えます.</p>
<p>modelブロック中のサンプリング文をベクトル化すると, このモデルはもっと速くできるでしょう. ループの代わりにドット乗算を使って<span class="math inline"><em>ϵ</em><sub><em>t</em></sub></span>の計算をベクトル化しても高速化できるでしょう.</p>
<h4 id="ベクトル化したmaqモデル">ベクトル化したMA(<span class="math inline"><em>Q</em></span>)モデル</h4>
<p>サンプリング確率をベクトル化した一般的なMA(<span class="math inline"><em>Q</em></span>)モデルは以下のように定義できるでしょう.</p>
<pre><code>data {
  int&lt;lower=0&gt; Q;  // 過去のノイズ項の数
  int&lt;lower=3&gt; T;  // 観測値の数
  vector[T] y;     // 時点tにおける観測値
}
parameters {
  real mu;              // 平均
  real&lt;lower=0&gt; sigma;  // 誤差のスケール
  vector[Q] theta;      // 誤差の係数, ラグ -t
}
transformed parameters {
  vector[T] epsilon;    // 時点tにおける誤差
  for (t in 1:T) {
    epsilon[t] &lt;- y[t] - mu;
    for (q in 1:min(t - 1, Q))
      epsilon[t] &lt;- epsilon[t] - theta[q] * epsilon[t - q];
  }
}
model {
  vector[T] eta;
  mu ~ cauchy(0, 2.5);
  theta ~ cauchy(0, 2.5);
  sigma ~ cauchy(0, 2.5);
  for (t in 1:T) {
    eta[t] &lt;- mu;
    for (q in 1:min(t - 1, Q))
      eta[t] &lt;- eta[t] + theta[q] * epsilon[t - q];
  }
  y ~ normal(eta, sigma);
}</code></pre>
<p>ここではすべてのデータがモデル化されています. 不足する項は単に, 誤差項の計算の際に回帰から除かれます. 両方のモデルともとても速く収束し, 収束した連鎖はよく混ざっています. ベクトル化したモデルの方がちょっとだけ速いのですが, これは繰り返しあたりについてのことで, 収束についてではありません. というのも両者は同じモデルだからです.</p>
<h3 id="自己回帰移動平均モデル">7.4. 自己回帰移動平均モデル</h3>
<p>ARMA（AutoRegressive Moving-Average, 自己回帰移動平均）モデルは, 自己回帰モデルと移動平均モデルの予測変数を結合させたものです. 履歴が1状態のARMA(1,1)モデルは, Stanでは以下のようにコーディングできます.</p>
<pre><code>data {
  int&lt;lower=1&gt; T;           // 観測値の数
  real y[T];                // 観測結果
}
parameters {
  real mu;                  // 平均の係数
  real phi;                 // 自己回帰の係数
  real theta;               // 移動平均の係数
  real&lt;lower=0&gt; sigma;      // 誤差のスケール
}
model {
  vector[T] nu;             // 時点tでの予測値
  vector[T] err;            // 時点tでの誤差
  nu[1] &lt;- mu + phi * mu;   // err[0] == 0と仮定
  err[1] &lt;- y[1] - nu[1];
  for (t in 2:T) {
    nu[t] &lt;- mu + phi * y[t-1] + theta * err[t-1];
    err[t] &lt;- y[t] - nu[t];
  }
  mu ~ normal(0,10);        // 事前分布
  phi ~ normal(0,2);
  theta ~ normal(0,2);
  sigma ~ cauchy(0,5);
  err ~ normal(0,sigma);    // 尤度
}</code></pre>
<p>データは他の時系列回帰と同様に宣言されており, パラメータはコード中に説明があります.</p>
<p>modelブロックでは, 局所変数のベクトル<code>nu</code>が予測値を, <code>err</code>が誤差を格納しています. これらの計算は, 前節で記述した移動平均モデルの誤差と同様です.</p>
<p>定常過程にするため, 弱情報事前分布を設定しています. 尤度は誤差項だけを含み, この例では効率的にベクトル化されています.</p>
<p>このようなモデルでは, 計算した誤差項を調べるのが必要なことがよくあります. Stanでは, 変換パラメータ(transformed parameter)として<code>err</code>を宣言することで簡単に対応できるでしょう. その場合も, 上のモデルでの定義と同様です. <code>nu</code>は局所変数のままでもよいのですが, ここでは<code>transformed parameters</code>ブロックに移しましょう.</p>
<p>Wayne Foltaは, 局所変数のベクトルを使わないモデルのコーディングを提案してくれました. 以下がそれです.</p>
<pre><code>model {
  real err;
  mu ~ normal(0,10);
  phi ~ normal(0,2);
  theta ~ normal(0,2);
  sigma ~ cauchy(0,5);
  err &lt;- y[1] - mu + phi * mu;
  err ~ normal(0,sigma);
  for (t in 2:T) {
    err &lt;- y[t] - (mu + phi * y[t-1] + theta * err);
    err ~ normal(0,sigma);
  }
}</code></pre>
<p>このARMAモデルのアプローチは, Stanではどのように局所変数（この場合は<code>err</code>）を再利用できるか示す良い例となっています. Foltaのアプローチは, 2つ以上の誤差項を局所変数に格納し, ループ内で再び代入することで, 高次の移動平均モデルにも拡張できるでしょう.</p>
<p>両方のコーディングとも大変高速です. 元のコーディングは正規分布をベクトル化できるという利点がありますが, 使用メモリがやや多くなります. 中間点は, <code>err</code>だけをベクトル化することでしょう.</p>
<h4 id="識別可能性と定常性">識別可能性と定常性</h4>
<p>MA部分の固有多項式の解が単位円の中にあるなら, MAおよびARMAモデルは識別可能ではありません. その場合, 以下の制約をつける必要があります. <sup>2</sup></p>
<pre><code>real&lt;lower = -1, upper = 1&gt; theta;</code></pre>
<p>このモデルから生成される合成データを用いて, 上の制約をつけずにモデルを走らせると, [-1,1]の範囲外に(<code>theta</code>, <code>phi</code>)の最頻値ができることがあります. これは事後分布の多峰性問題となり, またNUTSのtreedepthが非常に大きく（10を超えることもままあります）なります. 制約をつけることにより, 事後分布がより正確になり, treedepthが劇的に減少します. そのため, シミュレーションがかなり速くなります（典型的には1桁を大きく超えます）.</p>
<p>さらに, プロセスが本当に非定常なら別ですが, そうは考えられないのなら, 定常性を保証するため以下の制約をつける価値があります.</p>
<pre><code>read&lt;lower = -1, upper = 1&gt; phi;</code></pre>
<p><sup>2</sup>この小節は, Jonathan GilliganのGitHubのコメントを少し編集したものです. https://github.com/stan-dev/stan/issues/1617#issuecomment-160249142 を参照してください.</p>
<h3 id="確率的ボラティリティモデル">7.5. 確率的ボラティリティモデル</h3>
<p>確率的ボラティリティモデルは, 離散時間の潜在確率過程に従って, 証券購入のオプションのような資産収益のボラティリティ（すなわち分散）を扱います(Kim et al., 1998). データは, <span class="math inline"><em>T</em></span>個の等間隔時点における原資産に対する平均修正（すなわち中央化）収益<span class="math inline"><em>y</em><sub><em>t</em></sub></span>からなります. Kimらは, 以下の回帰に似た式を使って典型的な確率ボラティリティモデルを定式化しています. ここで, 潜在パラメータ<span class="math inline"><em>h</em><sub><em>t</em></sub></span>は対数ボラティリティを, パラメータ<span class="math inline"><em>μ</em></span>は平均対数ボラティリティを, <span class="math inline"><em>ϕ</em></span>はボラティリティ項の持続性をしめします. 変数<span class="math inline"><em>ϵ</em><sub><em>t</em></sub></span>は時点<span class="math inline"><em>t</em></span>における資産収益に対するホワイトノイズショック（すなわち乗法的誤差）で, <span class="math inline"><em>δ</em><sub><em>t</em></sub></span>は時点<span class="math inline"><em>t</em></span>におけるボラティリティに対するショックを表します.</p>
<p><br /><span class="math display">$$\begin{array}{rl}y_{t} &amp;= \epsilon_{t}\exp(h_{t}/2) \\ h_{t+1} &amp;= \mu + \phi(h_{t}-\mu)+\delta_{t}\sigma \\ h_{1} &amp;\sim \mathsf{Normal}\left(\mu, \frac{\sigma}{\sqrt{1-\phi^{2}}}\right) \\ \epsilon_{t} &amp;\sim \mathsf{Normal}(0, 1), \delta_{t} \sim \mathsf{Normal}(0, 1)\end{array}$$</span><br /></p>
<p>最初の行を変形すると, <span class="math inline"><em>ϵ</em><sub><em>t</em></sub> = <em>y</em><sub><em>t</em></sub>exp( − <em>h</em><sub><em>t</em></sub>/2)</span>となり, <span class="math inline"><em>y</em><sub><em>t</em></sub></span>のサンプリング分布は以下のように書けます.</p>
<p><br /><span class="math display"><em>y</em><sub><em>t</em></sub> ∼ Normal(0, exp(<em>h</em><sub><em>t</em></sub>/2))</span><br /></p>
<p><span class="math inline"><em>h</em><sub><em>t</em> + 1</sub></span>についての再帰式には, <span class="math inline"><em>δ</em><sub><em>t</em></sub></span>のスケーリングとサンプリングを組み合わせることができて, 次のサンプリング分布が得られます.</p>
<p><br /><span class="math display"><em>h</em><sub><em>t</em> + 1</sub> ∼ Normal(<em>μ</em> + <em>ϕ</em>(<em>h</em><sub><em>t</em></sub> − <em>μ</em>), <em>σ</em>)</span><br /></p>
<p>この定式化はそのままコーディングできて, 以下のStanモデルになります.</p>
<pre><code>data {
  int&lt;lower=0&gt; T;  // 時点(等間隔)の数
  vector[T] y;     // 時点tにおける平均修正収益
}
parameters {
  real mu;                     // 平均対数ボラティリティ
  real&lt;lower=-1,upper=1&gt; phi;  // ボラティリティの持続性
  real&lt;lower=0&gt; sigma;         // ホワイトノイズショックのスケール
  vector[T] h;                 // 時点tにおける対数ボラティリティ
}
model {
  phi ~ uniform(-1,1);
  sigma ~ cauchy(0,5);
  mu ~ cauchy(0,10);
  h[1] ~ normal(mu, sigma / sqrt(1 - phi * phi));
  for (t in 2:T)
    h[t] ~ normal(mu + phi * (h[t - 1] -  mu), sigma);
  for (t in 1:T)
    y[t] ~ normal(0, exp(h[t] / 2));
}</code></pre>
<p>Kimらの定式化と比較すると, Stanのモデルではパラメータ<span class="math inline"><em>ϕ</em></span>, <span class="math inline"><em>σ</em></span>, <span class="math inline"><em>μ</em></span>に事前分布を与えています. ショック項<span class="math inline"><em>ϵ</em><sub><em>t</em></sub></span>と<span class="math inline"><em>δ</em><sub><em>t</em></sub></span>はモデル中には明示的には現れないことに注意してください. とはいえ, generated quantitiesブロックで効率的に計算することは可能でしょう.</p>
<p>このような確率的ボラティリティモデルの事後分布で事後分散が大きくなるのは普通のことです. 例えば, <span class="math inline"><em>μ</em> =  − 1.02</span>, <span class="math inline"><em>ϕ</em> = 0.95</span>, <span class="math inline"><em>σ</em> = 0.25</span>として上のモデルで500データ点のシミュレーションを行なうと, 95%事後区間は<span class="math inline"><em>μ</em></span>が(-1.23,-0.54), <span class="math inline"><em>ϕ</em></span>が(0.82,0.98), <span class="math inline"><em>σ</em></span>が(0.16,0.38)となります.</p>
<p>このモデルで生成される, 秒あたりの有効サンプルを1桁以上高速化するのは比較的単純です. まず, 収益<span class="math inline"><em>y</em></span>についてのサンプリング文は簡単にベクトル化できます.</p>
<pre><code>y ~ normal(0, exp(h / 2));</code></pre>
<p>これにより繰り返しは高速化されますが, 有効サンブルサイズは変化しません. 根本的なパラメータ化と対数確率関数は変わっていないからです. 標準化ボラティリティによる再パラメータ化と, それからリスケーリングにより, 連鎖の混ざり具合は改善されます. これには, <code>h</code>の代わりに, 標準化パラメータ<code>h_std</code>を宣言する必要があります.</p>
<pre><code>parameters {
  ...
  vector[T] h_std; // 時点tにおける標準化対数ボラティリティ</code></pre>
<p>この時, 元の<code>h</code>の値は<code>transformed parameters</code>ブロックで定義します.</p>
<pre><code>transformed parameters {
  vector[T] h;            // 時点tにおける対数ボラティリティ
  h &lt;- h_std * sigma;     // h ~ normal(0,sigma)とした
  h[1] &lt;- h[1] / sqrt(1 - phi * phi);  // h[1]をリスケール
  h &lt;- h + mu;
  for (t in 2:T)
    h[t] &lt;- h[t] + phi * (h[t-1] - mu);
}</code></pre>
<p>最初の代入では, <code>h_std</code>を<span class="math inline">Normal(0, <em>σ</em>)</span>という分布になるようにリスケールして, これを一時的に<code>h</code>に代入しています. 2番目の代入では, <code>h[1]</code>の事前分布が<code>h[2]</code>から<code>h[T]</code>までの事前分布とは異なるように<code>h[1]</code>をリスケールしています. その次の代入では<code>mu</code>にオフセットをつけており, これにより<code>h[2]</code>から<code>h[T]</code>までが<span class="math inline">Normal(<em>μ</em>, <em>σ</em>)</span>という分布になります. この移動は<code>h[1]</code>のリスケーリングの後に行なう必要があることに注意してください. 最後のループは, <code>h[2]</code>から<code>h[T]</code>までが<code>phi</code>と<code>mu</code>と比較して適切にモデル化されるように移動平均に加算します.</p>
<p>最後の改良として, <code>h[1]</code>のサンプリング文と, <code>h[2]</code>から<code>h[T]</code>のサンプリングのループを, 1行のベクトル化した標準正規分布のサンプリング文に置き換えます.</p>
<pre><code>model { ...
  h_std ~ normal(0,1);</code></pre>
<p>元のモデルでは, 数百から時には数千回の繰り返しが収束に必要となることがありますが, 再パラメータ化したモデルでは数十回の繰り返しで信頼できる収束に至ります. 連鎖の混ざり具合も劇的に改善され, 繰り返しあたりの有効サンプルサイズも多くなります. 最後に, 各繰り返しの時間は元のモデルのおおよそ4分の1になります.</p>
<h3 id="隠れマルコフモデル">7.6. 隠れマルコフモデル</h3>
<p>隠れマルコフモデル（HMM: Hidden Markov Model）は, <span class="math inline"><em>T</em></span>個の出力変数<span class="math inline"><em>y</em><sub><em>t</em></sub></span>からなる系列を, これに並行する, 潜在カテゴリカル状態変数<span class="math inline"><em>z</em><sub><em>t</em></sub> ∈ {1, …, <em>K</em>}</span>の系列を条件として生成します. 与えられた<span class="math inline"><em>z</em><sub><em>t</em> − 1</sub></span>について, <span class="math inline"><em>z</em><sub><em>t</em></sub></span>が他の変数と条件付き独立であるように, この「隠れ」状態変数はマルコフ連鎖を形成すると仮定します. このマルコフ連鎖は推移行列<span class="math inline"><em>θ</em></span>によりパラメータ化されます. ここで, <span class="math inline"><em>θ</em><sub><em>k</em></sub></span>はK次元単体（<span class="math inline"><em>k</em> ∈ {1, …, <em>K</em>}</span>です. 状態<span class="math inline"><em>z</em><sub><em>t</em> − 1</sub></span>から状態<span class="math inline"><em>z</em><sub><em>t</em></sub></span>への推移確率は次式のようになります.</p>
<p><br /><span class="math display"><em>z</em><sub><em>t</em></sub> ∼ Categorical(<em>θ</em><sub><em>z</em>[<em>t</em> − 1]</sub>)</span><br /></p>
<p>時点<span class="math inline"><em>t</em></span>における出力<span class="math inline"><em>y</em><sub><em>t</em></sub></span>は潜在状態<span class="math inline"><em>z</em><sub><em>t</em></sub></span>に基づいて条件付き独立に生成されます.</p>
<p>この節では, 出力<span class="math inline"><em>y</em><sub><em>t</em></sub> ∈ {1, …, <em>V</em>}</span>に単純なカテゴリカルモデルを適用してHMMを記述します. 潜在状態<span class="math inline"><em>k</em></span>についてのカテゴリカル分布はV次元単体<span class="math inline"><em>ϕ</em><sub><em>k</em></sub></span>によりパラメータ化されます. 時点<span class="math inline"><em>t</em></span>における出力の観測値<span class="math inline"><em>y</em><sub><em>t</em></sub></span>は, 時点<span class="math inline"><em>t</em></span>における隠れ状態のインジケータ<span class="math inline"><em>z</em><sub><em>t</em></sub></span>に基づいて生成されます.</p>
<p><br /><span class="math display"><em>y</em><sub><em>t</em></sub> ∼ Categorical(<em>ϕ</em><sub><em>z</em>[<em>t</em>]</sub>)</span><br /></p>
<p>つまり, 混合成分のインジケータが潜在マルコフ連鎖を形成するような離散混合分布モデルをHMMは形成しています.</p>
<h4 id="教師付きパラメーター推定">教師付きパラメーター推定</h4>
<p>隠れ状態が既知の状況では, パラメータ<span class="math inline"><em>θ</em></span>および<span class="math inline"><em>ϕ</em></span>の当てはめのため, 以下のナイーブモデルを使うことができます. <sup>3</sup></p>
<pre><code>data {
  int&lt;lower=1&gt; K;  // カテゴリーの数
  int&lt;lower=1&gt; V;  // 単語(word)の数
  int&lt;lower=0&gt; T;  // 時点の数
  int&lt;lower=1,upper=V&gt; w[T]; // 単語(word)
  int&lt;lower=1,upper=K&gt; z[T]; // カテゴリー
  vector&lt;lower=0&gt;[K] alpha;  // 推移(transit)確率の事前確率
  vector&lt;lower=0&gt;[V] beta;   // 単語vを出力(emit)する確率の事前確率
}
parameters {
  simplex[K] theta[K];  // 推移(transit)確率
  simplex[V] phi[K];    // 単語vを出力(emit)する確率
}
model {
  for (k in 1:K)
    theta[k] ~ dirichlet(alpha);
  for (k in 1:K)
    phi[k] ~ dirichlet(beta);
  for (t in 1:T)
    w[t] ~ categorical(phi[z[t]]);
  for (t in 2:T)
    z[t] ~ categorical(theta[z[t - 1]]);
}</code></pre>
<p><span class="math inline"><em>θ</em><sub><em>k</em></sub></span>と<span class="math inline"><em>ϕ</em><sub><em>k</em></sub></span>には明示的なディリクレ事前分布を与えています. この2文を除くと, 有効な単体全体についての一様事前分布が暗黙のうちに使われることでしょう.</p>
<p><sup>3</sup>このプログラムは, Stan例題モデルのリポジトリで入手できます. https://github.com/stan-dev/example-models/tree/master/misc/gaussian-process を参照してください.</p>
<h4 id="開始状態と終了状態の確率">開始状態と終了状態の確率</h4>
<p>動くことは動きますが, 上のようにHMMを記述しても完全ではありません. 開始状態<span class="math inline"><em>z</em><sub>1</sub></span>がモデル化されていないからです（添字は2から<span class="math inline"><em>T</em></span>までです）. データを長い過程の部分系列と考えるなら, <span class="math inline"><em>z</em><sub>1</sub></span>の確率は, マルコフ連鎖の定常状態確率に合わせるべきでしょう. この場合, データには明確な終了がなく, 系列が<span class="math inline"><em>z</em><sub><em>T</em></sub></span>で終了する確率をモデル化する必要はありません.</p>
<p>もうひとつ別の概念として, HMMは有限長の系列のモデルとして考えることもできます. 例えば, 自然言語の文には明確な開始の分布（通常は大文字）と, 終了の分布（通常は何らかの句読点）とがあります. 文の境界をモデル化する最も単純な方法は, 新しい潜在状態<span class="math inline"><em>K</em> + 1</span>を加え, パラメータのベクトル<span class="math inline"><em>θ</em><sub><em>K</em> + 1</sub></span>によりカテゴリカル分布から最初の状態を生成し, 状態<span class="math inline"><em>K</em> + 1</span>への推移が文の終了時にのみ起こり, それ以外では起こらないように推移を制限することです.</p>
<h4 id="十分統計量の計算">十分統計量の計算</h4>
<p>上に紹介したナイーブなHMM推定モデルは, カテゴリカル分布についてのループを単一の多項分布に置き換えることで劇的に速くすることができます. <sup>4</sup>データは前もって宣言しますが, ここでは<code>transformed data</code>ブロックで, 推移行列と出力行列を推定する十分統計量を計算することにします.</p>
<pre><code>transformed data {
  int&lt;lower=0&gt; trans[K,K];
  int&lt;lower=0&gt; emit[K,V];
  for (k1 in 1:K)
    for (k2 in 1:K)
      trans[k1,k2] &lt;- 0;
  for (t in 2:T)
    trans[z[t - 1], z[t]] &lt;- 1 + trans[z[t - 1], z[t]];
  for (k in 1:K)
    for (v in 1:V)
      emit[k,v] &lt;- 0;
  for (t in 1:T)
    emit[z[t], w[t]] &lt;- 1 + emit[z[t], w[t]];
}</code></pre>
<p>入力についてのループに基づくモデルの尤度成分は以下のように多項分布に置き換わります.</p>
<pre><code>model {
  ...
  for (k in 1:K)
    trans[k] ~ multinomial(theta[k]);
  for (k in 1:K)
    emit[k] ~ multinomial(phi[k]);
}</code></pre>
<p>正規出力確率で連続値のHMMも, 同様に十分統計量を計算することで高速化できるでしょう.</p>
<p><sup>4</sup>このモデルはStan例題モデルリポジトリにあります. http://mc-stan.org/documentation を参照してください.</p>
<h4 id="解析的事後分布">解析的事後分布</h4>
<p>ディリクレ-多項HMMでは, ディリクレ分布が多項分布の共益事前分布なので, 事後分布は解析的に計算できます. 以下の例<sup>5</sup>では, Stanのモデルがどのように事後分布を解析的に定義できるかを示します. Stan言語ではこれが可能です. というのは, データが与えられたもとで, パラメータの条件付き確率に比例する量をモデルが定義しさえすればよいからです. これには, （正規化されていない）同時確率か, （正規化されていない）条件付き確率か, あるいはその間の量を定義すればよいのです.</p>
<p>このモデルは, 前のモデルと同じデータとパラメータを用いますが, <code>transformed data</code>ブロックで事後ディリクレパラメータを計算するようにしています.</p>
<pre><code>transformed data {
  vector&lt;lower=0&gt;[K] alpha_post[K];
  vector&lt;lower=0&gt;[V] beta_post[K];
  for (k in 1:K)
    alpha_post[k] &lt;- alpha;
  for (t in 2:T)
    alpha_post[z[t-1],z[t]] &lt;- alpha_post[z[t-1],z[t]] + 1;
  for (k in 1:K)
    beta_post[k] &lt;- beta;
  for (t in 1:T)
    beta_post[z[t],w[t]] &lt;- beta_post[z[t],w[t]] + 1;
}</code></pre>
<p>事後分布は以下のように解析的に書くことができます.</p>
<pre><code>model {
  for (k in 1:K)
    theta[k] ~ dirichlet(alpha_post[k]);
  for (k in 1:K)
    phi[k] ~ dirichlet(beta_post[k]);
}</code></pre>
<p><sup>5</sup>このプログラムはStan例題モデルリポジトリにあります. http://mc-stan.org/documentation を参照してください.</p>
<h4 id="半教師付き推定">半教師付き推定</h4>
<p>潜在状態が既知であるデータがまったくない完全に教師なしの方法でもHMMは推定ができます. その結果の事後分布は極端に多峰になるのが典型的です. 中間の解法は, 半教師付き推定を使うことです. これは, 教師付きデータと教師なしデータとの組み合わせに基づいています. この推定戦略をStanで実装するには, 未知の状態系列に対する出力系列の確率の計算が必要です. これは周辺化の問題で, HMMでは, 前向きアルゴリズムと呼ばれる方法で計算されます.</p>
<p>Stanでは, 前向きアルゴリズムは以下のようにコーディングされます. <sup>6</sup>まず, 教師なしデータのためデータ変数を2つ追加で宣言します.</p>
<pre><code>data {
  ...
  int&lt;lower=1&gt; T_unsup;  // 教師なしアイテムの数
  int&lt;lower=1,upper=V&gt; u[T_unsup]; // 教師なし単語(word)
  ...</code></pre>
<p>教師付きデータのモデルは変更ありません. 教師なしデータは, 以下のように前向きアルゴリズムをStanで実装して扱います.</p>
<pre><code>model {
  ...
  {
    real acc[K];
    real gamma[T_unsup,K];
    for (k in 1:K)
      gamma[1,k] &lt;- log(phi[k,u[1]]);
    for (t in 2:T_unsup) {
      for (k in 1:K) {
        for (j in 1:K)
          acc[j] &lt;- gamma[t-1,j] + log(theta[j,k]) + log(phi[k,u[t]]);
        gamma[t,k] &lt;- log_sum_exp(acc);
      }
    }
    increment_log_prob(log_sum_exp(gamma[T_unsup]));
  }</code></pre>
<p>前向きの値<code>gamma[t,k]</code>は, 時点<code>t</code>までの入力<code>u[1],...,u[t]</code>と, 潜在状態が時点<code>t</code>で<code>k</code>に等しいことの対数周辺確率であるとして定義されます. このとき, その前の潜在状態は周辺化消去されます. <code>gamma</code>の最初の行（<code>gamma[1,k]</code>）は, 潜在状態<code>k</code>が最初の出力<code>u[1]</code>を生成する対数確率で初期化されています. 前と同じように, 最初の潜在状態の確率自体はモデル化されません. 以降の時点<code>t</code>と出力<code>j</code>について<code>acc[j]</code>の値が設定されます. <code>acc[j]</code>の値は次の3つの対数確率の和です. それは時点<code>t-1</code>の潜在状態が<code>j</code>である対数確率, 時点<code>t-1</code>に状態<code>j</code>だったのが時点<code>t</code>に状態<code>k</code>に推移する対数確率, 出力<code>u[t]</code>が状態<code>k</code>から生成される対数確率です. <code>log_sum_exp</code>演算は単に, 状態<code>j</code>の各事前確率を掛け合わせるだけです. ただし, 対数スケールにおいて算術的に安定な方法で行います.</p>
<p>中括弧で, 局所変数<code>acc</code>と<code>gamma</code>のスコープを示しています. もっと前に宣言することもできたのですが, 使用される場所の近くに宣言を置いておく方がわかりやすいでしょう.</p>
<h4 id="予測推論">予測推論</h4>
<p>推移パラメータ<span class="math inline"><em>θ</em><sub><em>k</em>, <em>k</em>′</sub></span>および出力パラメータ<span class="math inline"><em>ϕ</em><sub><em>k</em>, <em>v</em></sub></span>, それに観測系列<span class="math inline"><em>u</em><sub>1</sub>, …, <em>u</em><sub><em>T</em></sub> ∈ 1, …, <em>V</em></span>が与えられたとき, 観測された出力<span class="math inline"><em>u</em></span>を生成した可能性が最も高い状態系列は, Viterbi（動的プログラミング）アルゴリズムにより計算されます.</p>
<p>ViterbiアルゴリズムはStanでは, 以下のように<code>generated quantities</code>ブロックでコーディングできます. この場合の予測値は, 観測値の配列<code>u[1], ..., u[T_unsup]</code>のもとでの最も可能性の高い状態系列<code>y_star[1], ..., y_star[T_unsup]</code>です. この系列は, 推移確率<code>theta</code>と出力確率<code>phi</code>により決定されるので, 事後のサンプルによって異なることがあります.</p>
<pre><code>generated quantities {
  int&lt;lower=1,upper=K&gt; y_star[T_unsup];
  real log_p_y_star;
  {
    int back_ptr[T_unsup,K];
    real best_logp[T_unsup,K];
    real best_total_logp;
    for (k in 1:K)
      best_logp[1,K] &lt;- log(phi[k,u[1]]);
    for (t in 2:T_unsup) {
      for (k in 1:K) {
        best_logp[t,k] &lt;- negative_infinity();
        for (j in 1:K) {
          real logp;
          logp &lt;- best_logp[t-1,j]
                  + log(theta[j,k]) + log(phi[k,u[t]]);
          if (logp &gt; best_logp[t,k]) {
            back_ptr[t,k] &lt;- j;
            best_logp[t,k] &lt;- logp;
          }
        }
      }
    }
    log_p_y_star &lt;- max(best_logp[T_unsup]);
    for (k in 1:K)
      if (best_logp[T_unsup,k] == log_p_y_star)
        y_star[T_unsup] &lt;- k;
    for (t in 1:(T_unsup - 1))
      y_star[T_unsup - t] &lt;- back_ptr[T_unsup - t + 1,
                                      y_star[T_unsup - t + 1]];
  }
}</code></pre>
<p>中括弧のブロックで, 3つの変数<code>back_ptr</code>, <code>best_logp</code>, <code>best_total_logp</code>を局所変数にしています. そのため, これらは出力には含まれません. 変数<code>y_star</code>は, 入力系列<code>u</code>のもとで最も高い確率を持つラベル系列を保持します. 中間量は, 前向きアルゴリズムでは合計確率でしたが, それとは異なり, 時点<code>t</code>までの系列についての, 時点<code>t</code>で最終の出力カテゴリーが<code>k</code>である確率の最大値<code>best_logp[t,k]</code>からなります. また, リンク元へのバックポインタがあり, 最終の時点<code>t</code>での対数確率の最大値からバックポインタをたどることで, 最も可能性の高い状態系列が得られます.</p>
<p>この推論は, 半教師付きモデルへの当てはめに使えるのと同様に, 教師なし出力<code>u</code>でも使えます. 上のコードは, 教師なしの当てはめと同じモデルファイルでも使うことができます. これは, モデルをトレーニングするのに半教師付き法でデータを使うという, ベイズの推定法です. これは「ズル」ではありません. <code>u</code>についての元の状態は決して観測されないからです. 他のパラメータすべてを使って推定されるだけです.</p>
<p>もし出力<code>u</code>が半教師付き推定では使われず, 単に予測のもととなるだけなら, カット演算を使ってBUGSモデリング言語で記述したものと結果は同じです. すなわち, モデルは<code>u</code>とは独立に当てはめられており, そして, <code>u</code>を生成したものとして最も可能性の高い状態を見つけるのにパラメータが使われています.</p>
<h2 id="欠測データと部分的に既知のパラメータ">8. 欠測データと部分的に既知のパラメータ</h2>
<p>ベイズ推定では欠測データに対してごく一般的な手法を使えます. すなわち, 欠測データの項目すべてが, 事後分布で推定されるパラメータとして表されます(Gelman et al., 2013). 欠測データが明示的にモデル化されないのであれば, たいていの回帰モデルの予測変数でそうですが, 結果は, 欠測している予測変数を表すパラメータに非正則事前分布を設定することになります.</p>
<p>観測データと欠測データを配列に混ぜる方法はStanに取り入れるのは難しいことがあります. これは一部には, 離散の未知量をモデル化する方法がStanではトリッキーなことがあることによりますし, また一部には, ほかのいくつかの統計言語（例えば, RやBUGS）とは違ってStanでは, 観測量と未知量はモデル中の別の場所で定義する必要があることによります. そのため, Stanプログラムでは, データ構造中にある観測部分と欠測部分とを組み合わせて一緒にするコードを含める必要がある場合があります. 例はこの章でこの後から紹介します.</p>
<h3 id="欠測データ">8.1. 欠測データ</h3>
<p>Stanでは, <code>data</code>と<code>transformed data</code>ブロックで宣言された変数を既知量として, <code>parameters</code>ブロックで宣言された変数を未知量として扱います.</p>
<p>正規分布に従う観測値に欠測データを含む例<sup>1</sup>は以下のようにコーディングできるでしょう.</p>
<pre><code>data {
  int&lt;lower=0&gt; N_obs;
  int&lt;lower=0&gt; N_mis;
  real y_obs[N_obs];
}
parameters {
  real mu;
  real&lt;lower=0&gt; sigma;
  real y_mis[N_mis];
}
model {
  for (n in 1:N_obs)
    y_obs[n] ~ normal(mu,sigma);
  for (n in 1:N_mis)
    y_mis[n] ~ normal(mu,sigma);
}</code></pre>
<p>観測データ点と欠測データ点の数は, 非負の整数の変数<code>N_obs</code>と<code>N_mis</code>にデータとしてコーディングされています. 観測データは, <code>array</code>型のデータの変数<code>y_obs</code>として与えられています. 欠測データは<code>array</code>型のパラメータ<code>y_mis</code>としてコーディングされています. 通常のパラメータは推定されるので, 位置<code>mu</code>とスケール<code>sigma</code>もパラメータとしてコーディングされています. このモデルをもっとうまく書く方法はベクトル化することです. そうすると本体は次のようになります.</p>
<pre><code>  y_obs ~ normal(mu,sigma);
  y_mis ~ normal(mu,sigma);</code></pre>
<p>このモデルには, 観測データと欠測データにそれぞれ1つのループがあります. これはモデル指定としていささか冗長ですが, そのおかげで, Stanでの欠測データ問題としては, 次の節で記述するもっと一般的なテクニックよりもはるかに効率的なサンプリングが可能になります.</p>
<p><sup>1</sup>もっと意味のある推定の例は, 予測変数を使った観測された観測値と欠測の観測値の回帰を含むものでしょう. 観測された観測値に対応する予測変数も欠測の観測値に対応する予測変数も既知で, <code>data</code>ブロックで指定されているものです.</p>
<h3 id="部分的に既知のパラメータ">8.2. 部分的に既知のパラメータ</h3>
<p>多変量の確率関数で, 一部の結果やパラメータだけが観測されているといったような状況では, 既知量（データ）と未知量（パラメータ）とを混ぜたベクトルを作る必要があります. Stanでこれをおこなうのは, <code>transformed parameters</code>ブロックでベクトルあるいは配列をつくり, それに代入することで可能です.</p>
<p>以下の例は, 2変数の共分散行列を含むもので, 分散は既知ですが, 共分散は既知ではありません.</p>
<pre><code>data {
  int&lt;lower=0&gt; N;
  vector[2] y[N];
  real&lt;lower=0&gt; var1;     real&lt;lower=0&gt; var2;
}
transformed data {
  real&lt;upper=0&gt; min_cov;
  real&lt;lower=0&gt; max_cov;
  max_cov &lt;- sqrt(var1 * var2);
  min_cov &lt;- -max_cov;
}
parameters {
  vector[2] mu;
  real&lt;lower=min_cov,upper=max_cov&gt; cov;
}
transformed parameters {
  matrix[2,2] sigma;
  sigma[1,1] &lt;- var1;     sigma[1,2] &lt;- cov;
  sigma[2,1] &lt;- cov;      sigma[2,2] &lt;- var2;
}
model {
  for (n in 1:N)
    y[n] ~ multi_normal(mu,sigma);
}</code></pre>
<p>分散は, 変数<code>var1</code>および<code>var2</code>にデータとして定義されていますが, 共分散は, 変数<code>cov</code>にパラメータとして定義されています. 2×2共分散行列<code>sigma</code>は変換パラメータ(transformed parameter)として定義されています. 分散は, 2つの対角要素に代入され, 共分散は, 2つの非対角要素に代入されます.</p>
<p>共分散の宣言につけた制約により, 結果の共分散行列<code>sigma</code>が正定値であることが保証されます. 分散の積の, 正負の平方根が上下限となりますが, この値は<code>transformed data</code>として定義されるので, 計算されるのは1回だけです.</p>
<h3 id="効率性についての注意">8.3. 効率性についての注意</h3>
<p>はじめの節の欠測データの例は, 次の節の, 部分的に既知のパラメータの例の方法と同じように, データとパラメータとを混ぜた配列としてプログラミングすることもできるでしょう. 正しく動くでしょうが, 計算には無駄が多くなります. <code>parameters</code>あるいは<code>transformed parameters</code>ブロックで宣言される各パラメータは自動微分の変数を使います. 単純なデータの変数と比較すると, 記憶容量と勾配計算の時間の点でこれはより高くつきます. さらに, そのコピーが余分な容量と余分な時間を使います.</p>
<h3 id="因子分析の負荷行列">8.4. 因子分析の負荷行列</h3>
<p>Rick Farouniは, Stan users groupで, 単位対角の共分散行列についてCholesky因子をつくる方法を尋ねました. これは, ベイズ因子分析(Aguilar and West 2000)で使われます. 対角下の要素をパラメータとして宣言し, それから変換パラメータ(tranformed parameter)とした行列全体に値を埋めることで, これを行なうことができます.</p>
<pre><code>data {
  int&lt;lower=2&gt; K;
}
transformed data {
  int&lt;lower=1&gt; K_choose_2;
  K_choose_2 &lt;- (K * (K - 1)) / 2;
}
parameters {
  vector[K_choose_2] L_lower;
}
transformed parameters {
  cholesky_factor_cov[K] L;
  for (k in 1:K)
    L[k,k] &lt;- 1;
  {
    int i;
    for (m in 2:K) {
      for (n in 1:(m - 1)) {
        L[m,n] &lt;- L_lower[i];
        L[n,m] &lt;- 0;
        i &lt;- i + 1;
      }
    }
  }
}</code></pre>
<p>事前分布は, <code>L_lower</code>に直接置くのが最も便利です. もうひとつの方法は, Cholesky因子<code>L</code>全体への事前分布でしょう. <code>L_lower</code>から<code>L</code>への変換では値は変わらないので, （訳注: 確率変数の変換に伴う）ヤコビアンの調整を必要としないからです（必要としないにもかかわらず, パーサから警告が出ます. これはパーサがコード解析をして変換が線形であると推測するほど十分に賢くないためです. ）. 共分散行列<code>L * L'</code>全体に事前分布を置くのは, Jacobian調整が必要となるでしょうから, まったく便利ではないでしょう. 正確な調整は, 共分散行列を扱う56.1節の小節で示します.</p>
<h2 id="切断あるいは打ち切りデータ">9. 切断あるいは打ち切りデータ</h2>
<p>測定値に切断や打ち切りがあるデータは, Stanでは以下のそれぞれの確率モデルのようにコーディングできます.</p>
<h3 id="切断分布">9.1. 切断分布</h3>
<p>Stanで切断を使える分布は, 1変量分布で, 対応する対数累積分布関数(cumulative distribution function: CDF)と対数相補累積分布関数(complementary cumulative distribution function: ccdf)があるものに限られています. 切断分布とcdf, ccdfについてもっと知るには, 26.3節の切断分布の項を参照してください.</p>
<h3 id="切断データ">9.2. 切断データ</h3>
<p>切断データとは, ある下限より上か, 上限より下か, あるいは上下限の間にある測定値しか報告されないようなデータです.</p>
<p>切断データはStanでは切断分布を使ってモデリングできるでしょう. 例えば, 切断データ<span class="math inline"><em>y</em><sub><em>n</em></sub></span>は, <span class="math inline"><em>y</em><sub><em>n</em></sub> &lt; 300</span>となるように切断点<span class="math inline"><em>U</em> = 300</span>が上限となっているとします. Stanではこのデータは, 観測値が切断正規分布に従うとして以下のようにモデリングできます.</p>
<pre><code>data {
  int&lt;lower=0&gt; N;
  real U;
  real&lt;upper=U&gt; y[N];
}
parameters {
  real mu;
  real&lt;lower=0&gt; sigma;
}
model {
  for (n in 1:N)
    y[n] ~ normal(mu,sigma) T[,U];
}</code></pre>
<p>このモデルでは, 上限<code>U</code>をデータとして宣言し, この制約にあてはまるように<code>y</code>のデータに制約をつけています. これは, サンプリングが始まる前, データがモデルに読み込まれたときにチェックされます.</p>
<p>このモデルは, スケールと位置のパラメータに暗黙に非正則平坦事前分布を使っています. これらには, サンプリング文（~）を使って<code>model</code>ブロック中で事前分布を与えることもできます.</p>
<h4 id="制約と範囲外の値">制約と範囲外の値</h4>
<p>切断分布では, サンプリングされた変量が切断の範囲外の値になったときには, 確率は0になります. したがって対数確率は<span class="math inline"> − ∞</span>と評価されるでしょう. 例えば, 変量<code>y</code>が次の文でサンプリングされているとします.</p>
<pre><code>for (n in 1:N)
  y[n] ~ normal(mu,sigma) T[L,U];</code></pre>
<p>このとき, <code>y[n]</code>の値が<code>L</code>の値より小さいか, <code>U</code>の値よりも大きければ, サンプリング文は確率0の推定値を生成します. ユーザー定義の切断では, このように切断範囲外では0とするように明示的に処理しなければいけません.</p>
<p>変数が切断範囲外にそれるのを防ぐには, 適切な制約が必要です. 例えば, <code>y</code>が上のモデルのパラメータならば, <code>L</code>と<code>U</code>の値の間になるように宣言で制約をつけるべきです.</p>
<pre><code>parameters {
  real&lt;lower=L,upper=U&gt; y[N];
  ...</code></pre>
<p>上のモデルで, <code>L</code>と<code>U</code>がパラメータで<code>y</code>がデータなら, 適切に制約をつけて, データがその間にあり, <code>L</code>の値が<code>U</code>の値よりも小さくなるようにしなくてはなりません（もし両者が等しければ, パラメータの範囲は1点にまで小さくなり, サンプラーで使われているハミルトニアン力学は破綻します）. 以下の宣言は, 境界がうまく設定されることを保証します.</p>
<pre><code>parameters {
  real&lt;upper=min(y)&gt; L; // L &lt; y[n]
  real&lt;lower=fmax(L,max(y))&gt; U; // L &lt; U; y[n] &lt; U</code></pre>
<p>実数の組み合わせについては, <code>max</code>ではなく<code>fmax</code>関数を使うことに注意してください.</p>
<h4 id="未知の切断点">未知の切断点</h4>
<p>切断点が未知のときは, パラメータとして扱うことでこれを推定することができます. 既知の切断点に関する前の節のモデルから, 変数宣言を少し再構成することで, これが可能になります.</p>
<pre><code>data {
  int&lt;lower=1&gt; N;
  real y[N];
}
parameters {
  real&lt;upper = min(y)&gt; L;
  real&lt;lower = max(y)&gt; U;
  real mu;
  real&lt;lower=0&gt; sigma;
}
model {
  L ~ ...;
  U ~ ...;
  for (n in 1:N)
    y[n] ~ normal(mu,sigma) T[L,U];
}</code></pre>
<p>ここでは, 下方の切断点<code>L</code>が, <code>y</code>の最小値以下と宣言されています. また, 上方の切断点<code>U</code>は<code>y</code>の最大値以上と宣言されています. この宣言はデータに依存していますが, データが両切断点の間にあるという制約しか課していません. ただし, <code>N</code>が<code>int&lt;lower=1&gt;</code>という型で宣言されていますので, 少なくとも1つのデータ点があるはずです. <code>L</code>が<code>U</code>よりも小さいという制約は, 空ではないデータによって間接的に保証されるのです.</p>
<p>下限<code>L</code>と上限<code>U</code>の事前分布については省略していますが, このモデルにおいて<code>L</code>が<code>min(y)</code>のまわりに強く集中したり<code>U</code>が<code>max(y)</code>のまわりに強く集中したりしないように, 情報のある事前分布を設定すべきでしょう.</p>
<h3 id="打ち切りデータ">9.3 打ち切りデータ</h3>
<p>打ち切りデータとは, 点の値が大きすぎたり, 小さすぎたり, あるいはその両方の場合で, 値が不明になるようなデータです. 切断データと異なるのは, 打ち切られたデータ点の数が分かっていることです. 教科書に載っている例としては, 135kgより重い値が報告されない家庭用の秤があります.</p>
<h4 id="打ち切りとなる値の推定">打ち切りとなる値の推定</h4>
<p>打ち切りデータをモデリングする方法の1つは, 打ち切りデータを, 打ち切られる範囲のあるという制約をつけた欠測データとして扱うことです. Stanでは配列や行列に未知の値を許しませんから, 打ち切りとなる値は明示的に表さなくてはなりません. 以下は, 右側打ち切りの場合です.</p>
<pre><code>data {
  int&lt;lower=0&gt; N_obs;
  int&lt;lower=0&gt; N_cens;
  real y_obs[N_obs];
  real&lt;lower=max(y_obs)&gt; U;
}
parameters {
  real&lt;lower=U&gt; y_cens[N_cens];
  real mu;
  real&lt;lower=0&gt; sigma;
}
model {
  y_obs ~ normal(mu,sigma);
  y_cens ~ normal(mu,sigma);
}</code></pre>
<p>打ち切りデータの配列<code>y_cens</code>はパラメータとして宣言されていますので, 位置とスケールのパラメータ, <code>mu</code>および<code>sigma</code>とともにサンプリングされます. 打ち切りデータの配列<code>y_cens</code>は, <code>real&lt;lower=U&gt;</code>という型の値と宣言されていますので, 打ち切りデータに補完される値はすべて<code>U</code>よりも大きくなります. 打ち切りを補完したデータでは, モデル中の最後のサンプリング文を実行すると, 位置とスケールのパラメータに影響が出ます.</p>
<h4 id="打ち切りとなる値の積分消去">打ち切りとなる値の積分消去</h4>
<p>位置とスケールを推定するときに打ち切りとなる値を無視するのは良くありませんが, 補完値が必要な場合だけではありません. 補完せずに, 打ち切りとなる値を積分消去することもできます. 打ち切られた各データ点は以下の確率を持ちます.</p>
<p><br /><span class="math display">$$\Pr[y&gt;U]=\int_{U}^{\infty}\mathsf{Normal}(y \mid \mu, \sigma)dy = 1 - \Phi\left(\frac{y - \mu}{\sigma}\right)$$</span><br /></p>
<p>ここで, <span class="math inline">Φ()</span>は標準正規分布の累積分布関数です. <span class="math inline"><em>M</em></span>個の打ち切り観測があるとき, 合計の確率は対数軸で以下のようになります.</p>
<p><br /><span class="math display">$$\log\prod_{m=1}^{M}\Pr[y_{m}&gt;U] = \log\left(1 - \Phi\left(\frac{y - \mu}{\sigma}\right)\right)^{M} = M \mathsf{normal\_ccdf\_log}(y, \mu, \sigma)$$</span><br /></p>
<p>ここで, <span class="math inline">normal_ccdf_log</span>は相補CDFの対数です（Stanに実装されている各分布には<code>&lt;distr&gt;_ccdf_log</code>があります）.</p>
<p>以下の右側打ち切りモデルでは, 打ち切り点が既知であることを仮定しており, データとして宣言しています.</p>
<pre><code>data {
  int&lt;lower=0&gt; N_obs;
  int&lt;lower=0&gt; N_cens;
  real y_obs[N_obs];
  real&lt;lower=max(y_obs)&gt; U;
}
parameters {
  real mu;
  real&lt;lower=0&gt; sigma;
}
model {
  y_obs ~ normal(mu,sigma);
  increment_log_prob(N_cens * normal_ccdf_log(U,mu,sigma));
}</code></pre>
<p><code>y_obs</code>の観測値については, 打ち切りのない, 正規分布のサンプリングを行なうモデルが使われています. 打ち切りデータの項目については, 正規分布の対数相補累積分布（訳注: 原文では'log cumulative normal probability'ですが, モデルにあわせて修正しました）の確率を計算して, それを対数確率に直接加算されています.</p>
<p>左側打ち切りデータでは, 相補CDFの代わりにCDF(<code>normal_cdf_log</code>)を使わなくてはなりません. もし打ち切り点の変数(<code>L</code>)が未知なら, その宣言は<code>data</code>から<code>parameters</code>ブロックに移動すべきです.</p>
<pre><code>data {
  int&lt;lower=0&gt; N_obs;
  int&lt;lower=0&gt; N_cens;
  real y_obs[N_obs];
}
parameters {
  real&lt;upper=min(y_obs)&gt; L;
  real mu;
  real&lt;lower=0&gt; sigma;
}
model {
  L ~ normal(mu,sigma);
  y_obs ~ normal(mu,sigma);
  increment_log_prob(N_cens * normal_cdf_log(L,mu,sigma));
}</code></pre>
<h2 id="有限混合分布">10. 有限混合分布</h2>
<p>結果変数が有限混合分布となるモデルは, 結果変数が複数の分布のうちのひとつから抽出され, どの分布から抽出されるかは, 混合させるカテゴリカルな分布により制御されると仮定します. 混合モデルは通常, 多峰の分布となり, その山は混合成分の最頻値に近くなります. 混合分布モデルはいくつかの方法でパラメータ化できます. 以下の節でそれを記述します.</p>
<h3 id="潜在離散値のパラメータ化">10.1. 潜在離散値のパラメータ化</h3>
<p>混合分布モデルをパラメータ化する方法の1つは, 結果変数を負担する混合成分を示す潜在カテゴリカル変数を使うことです. 例えば, <span class="math inline"><em>K</em></span>個の正規分布があり, その位置は<span class="math inline"><em>μ</em><sub><em>k</em></sub> ∈ ℛ</span>, スケールは<span class="math inline"><em>σ</em><sub><em>k</em></sub> ∈ (0, ∞)</span>とします. ここで, これらを割合<span class="math inline"><em>θ</em></span>で混合させるとします. <span class="math inline"><em>θ</em><sub><em>k</em></sub> ≥ 0</span>かつ<span class="math inline">$\sum_{k=1}^{K}\theta_{k}=1$</span>, すなわち<span class="math inline"><em>θ</em></span>は<span class="math inline"><em>K</em></span>次元単体です. 各結果変数<span class="math inline"><em>y</em><sub><em>n</em></sub></span>には潜在変数<span class="math inline"><em>z</em><sub><em>n</em></sub> ∈ 1, …, <em>K</em></span>があり, これは<span class="math inline"><em>θ</em></span>によりパラメータ化されるカテゴリカル分布に従うとします.</p>
<p><br /><span class="math display"><em>z</em><sub><em>n</em></sub> ∼ Categorical(<em>θ</em>)</span><br /></p>
<p>変数<span class="math inline"><em>y</em><sub><em>n</em></sub></span>は, 混合成分<span class="math inline"><em>z</em><sub><em>n</em></sub></span>のパラメータに従って分布します.</p>
<p><br /><span class="math display"><em>y</em><sub><em>n</em></sub> ∼ Normal(<em>μ</em><sub><em>z</em>[<em>n</em>]</sub>, <em>σ</em><sub><em>z</em>[<em>n</em>]</sub>)</span><br /></p>
<p>離散パラメータ<span class="math inline"><em>z</em><sub><em>n</em></sub></span>があるので, Stanではこのモデルは直接扱うことができませんが, 次の節で記述するように<span class="math inline"><em>z</em></span>パラメータを総和で消去することにより<span class="math inline"><em>μ</em></span>と<span class="math inline"><em>σ</em></span>をサンプリングできます.</p>
<h3 id="負担率パラメータを総和で消去">10.2. 負担率パラメータを総和で消去</h3>
<p>前の節であらましを見た混合正規分布モデルは, 総和により離散パラメータをモデルから消去することによりStanで実装できます. <span class="math inline"><em>Y</em></span>が<span class="math inline"><em>K</em></span>個の正規分布の混合分布で, 正規分布のが位置<span class="math inline"><em>μ</em><sub><em>k</em></sub></span>, スケール<span class="math inline"><em>σ</em><sub><em>k</em></sub></span>, <span class="math inline"><em>K</em></span>次元単位単体に従う割合<span class="math inline"><em>θ</em></span>で混合される場合は, 次式のように表されます.</p>
<p><br /><span class="math display">$$p_{Y}(y \mid \theta,\mu,\sigma) = \sum_{k=1}^{K}\theta_{k}\mathsf{Normal}(\mu_{k},\sigma_{k})$$</span><br /></p>
<h3 id="指数の和の対数-対数軸での線形の和">10.3. 指数の和の対数: 対数軸での線形の和</h3>
<p>指数の和の対数関数は, 対数軸での混合分布を定義するのに使われます. 2つの入力値の場合は以下のように定義されます.</p>
<p><br /><span class="math display"><em>l</em><em>o</em><em>g</em>_<em>s</em><em>u</em><em>m</em>_<em>e</em><em>x</em><em>p</em>(<em>a</em>, <em>b</em>) = log(exp(<em>a</em>) + exp(<em>b</em>))</span><br /></p>
<p><span class="math inline"><em>a</em></span>と<span class="math inline"><em>b</em></span>が確率を対数軸にしたものなら, <span class="math inline">exp(<em>a</em>) + exp(<em>b</em>)</span>は線形軸での両者の和になります. そして外側の対数で, 結果を対数軸に戻します. まとめると, log_sum_expは対数軸で線形の加算を行なうということになります. Stanの組込み<code>log_sum_exp</code>関数を使う理由は, 指数計算でのアンダーフローやオーバーフローを防ぐことができるということにあります. 結果変数は以下のように計算されます.</p>
<p><br /><span class="math display">log(exp(<em>a</em>) + exp(<em>b</em>)) = max(<em>a</em>, <em>b</em>) + log(exp(<em>a</em> − max(<em>a</em>, <em>b</em>)) + exp(<em>b</em> − max(<em>a</em>, <em>b</em>)))</span><br /> （<strong>訳注:原文ではlog()の中の'+'が','になっています. </strong>）</p>
<p>この式の評価では, <span class="math inline"><em>a</em> − max(<em>a</em>, <em>b</em>)</span>か<span class="math inline"><em>b</em> − max(<em>a</em>, <em>b</em>)</span>のいずれかが0になり, 他方は負になります. これにより, 主項でオーバーフローやアンダーフローが発生する可能性をなくし, 演算での算術精度を可能な限り確保します.</p>
<p>例として, <span class="math inline">Normal( − 1, 2)</span>と<span class="math inline">Normal(3, 1)</span>とが確率<span class="math inline"><em>θ</em> = (0.3, 0.7)<sup>⊤</sup></span>で混ざる混合分布は, Stanでは以下のように実装できます.</p>
<pre><code>parameters {
  real y;
}
model {
  increment_log_prob(log_sum_exp(log(0.3)
                                   + normal_log(y,-1,2),
                                 log(0.7)
                                   + normal_log(y,3,1)));
}</code></pre>
<p>対数確率の項は以下のように導出されます.</p>
<p><br /><span class="math display">$$\begin{array}{ll}\log p_{Y}(y\mid\theta,\mu,\sigma) &amp;= \log(0.3\times\mathsf{Normal}(y \mid -1,2)+0.7\times\mathsf{Normal}(y\mid 3,1))\\ &amp;= \log(\exp(\log(0.3\times\mathsf{Normal}(y\mid -1,2)))\\\ &amp;\qquad+\exp(\log(0.7\times\mathsf{Normal}(y\mid 3,1))))\\ &amp;=\mathrm{log\_sum\_exp}(\log(0.3)+\log\mathsf{Normal}(y\mid -1,2),\\ &amp;\hspace{68pt}\log(0.7)+\log\mathsf{Normal}(y\mid 3,1)) \end{array}$$</span><br /></p>
<h4 id="混合パラメータの推定">混合パラメータの推定</h4>
<p>混合分布を表現する枠組みが与えられたところで, 推定の準備に移りましょう. ここでは, 位置, スケール, 混合成分が未知の量です. さらに, 混合成分の数も一般化してデータとして指定したものが以下のモデルになります.</p>
<pre><code>data {
  int&lt;lower=1&gt; K;          // 混合成分の数
  int&lt;lower=1&gt; N;          // データ点の数
  real y[N];               // 観測値
}
parameters {
  simplex[K] theta;        // 混合確率
  real mu[K];              // 混合成分の位置
  real&lt;lower=0&gt; sigma[K];  // 混合成分のスケール
} model {
  real ps[K];              // 成分密度の対数の一時変数
  sigma ~ cauchy(0,2.5);
  mu ~ normal(0,10);
  for (n in 1:N) {
    for (k in 1:K) {
      ps[k] &lt;- log(theta[k])
               + normal_log(y[n],mu[k],sigma[k]);
    }
    increment_log_prob(log_sum_exp(ps));
  }
}</code></pre>
<p>このモデルには, <span class="math inline"><em>K</em></span>個の混合成分と<span class="math inline"><em>N</em></span>個のデータ点があります. 混合割合のパラメータ<code>theta</code>は<span class="math inline"><em>K</em></span>次元単位単体として宣言されており, 成分の位置のパラメータ<code>mu</code>とスケールのパラメータ<code>sigma</code>はともに大きさ<code>K</code>の配列として定義されています. スケールの配列<code>sigma</code>の値は非負に制約されており, このモデルでは弱情報事前分布を与えています. モデル中で, 大きさ<code>K</code>の局所配列変数<code>ps</code>が宣言されており, 混合成分からの寄与を積算するのに使われています.</p>
<p>これは例題なので, 位置とスケールは単純な事前分布から抽出されていますが, Stanでサポートされることなら何でもできるでしょう. 混合成分は階層的にモデリングすることさえ可能でしょう.</p>
<p>作業の中心は, データ点<code>n</code>についてのループ中にあります. 各点について, <span class="math inline"><em>θ</em><sub><em>k</em></sub> × Normal(<em>y</em><sub><em>n</em></sub> ∣ <em>μ</em><sub><em>k</em></sub>, <em>σ</em><sub><em>k</em></sub>)</span>の対数が計算され, 配列<code>ps</code>に加えられます. それから, それらの値の指数の和の対数だけ対数確率を増加させます.</p>
<h3 id="混合分布のベクトル化">10.4. 混合分布のベクトル化</h3>
<p>Stanでは（今のところ）混合分布モデルを観測値のレベルでベクトル化する方法はありません. この節は読者に注意を行なうものです. 単純にベクトル化しようとしてはいけません. そのようにすると, 結果として別のモデルとなってしまいます. 観測値のレベルでの正しい混合は以下のように定義されます.</p>
<pre><code>for (n in 1:N)
  increment_log_prob(log(lambda)
                       + normal_log(y[n], mu[1], sigma[1]),
                     log1m(lambda)
                       + normal_log(y[n], mu[2], sigma[2]));</code></pre>
<p>下も等価です.</p>
<pre><code>for (n in 1:N)
  increment_log_prob(log_mix(lambda,
                             normal_log(y[n], mu[1], sigma[1]),
                             normal_log(y[n], mu[2], sigma[2])));</code></pre>
<p>この定義では, 各観測値<span class="math inline"><em>y</em><sub><em>n</em></sub></span>が混合成分のいずれかから得られると仮定しています. 以下のように定義されます.</p>
<p><br /><span class="math display">$$p(y \mid \lambda,\mu,\sigma)=\prod_{n=1}^{N}(\lambda\times\mathsf{Normal}(y_{n}\mid\mu_{1},\sigma_{1})+(1-\lambda)\times\mathsf{Normal}(y_{n}\mid\mu_{2},\sigma_{2})$$</span><br /></p>
<p>上のモデルと, 下の（間違った）モデルのベクトル化の試みを対比してみてください.</p>
<pre><code>increment_log_prob(log(lambda)
                     + normal_log(y, mu[1], sigma[1]),
                   log1m(lambda)
                     + normal_log(y, mu[2], sigma[2]));</code></pre>
<p>下も等価です.</p>
<pre><code>increment_log_prob(log_mix(lambda,
                           normal_log(y, mu[1], sigma[1]),
                           normal_log(y, mu[2], sigma[2])));</code></pre>
<p>この2番目の定義は, 観測値の系列<span class="math inline"><em>y</em><sub>1</sub>, …, <em>y</em><sub><em>n</em></sub></span>全体が一方の成分か, 他方の成分かから得られていることを意味します. 別の密度を定義しているのです.</p>
<p><br /><span class="math display">$$p(y\mid\lambda,\mu,\sigma)=\lambda\times\prod_{n=1}^{N}\mathsf{Normal}(y_{n}\mid\mu_{1},\sigma_{1})+(1-\lambda)\times\prod_{n=1}^{N}\mathsf{Normal}(y_{n}\mid\mu_{2},\sigma_{2})$$</span><br /></p>
<h3 id="ゼロ過剰モデルとハードルモデル">10.5. ゼロ過剰モデルとハードルモデル</h3>
<p>ゼロ過剰モデルとハードルモデルはともに, ポアソン分布と, ベルヌーイ確率質量関数との混合分布で, 結果変数が0になる確率をより柔軟にモデリングできます. ゼロ過剰モデルは, Lambert (1992)により定義されたもので, 零値の結果変数に対して追加の確率質量を与えるものです. 一方, ハードルモデルは零値と非零値の結果変数の純粋な混合分布として定式化されます.</p>
<p>ゼロ過剰モデルとハードルモデルは, ポアソン分布以外の離散分布についても定式化できます. ゼロ過剰はStanでは連続分布でうまくいきません. これは, 導関数の問題によるもので, 特に, 回帰係数の事前分布として正規分布をゼロ過剰にするようなときに, 連続分布に対して点の質量を加算する方法がありません.</p>
<h4 id="ゼロ過剰">ゼロ過剰</h4>
<p>以下のゼロ過剰ポアソン分布の例について考えます. ここではパラメータ<code>theta</code>を使って, 確率<span class="math inline"><em>θ</em></span>で0が抽出され, 確率<span class="math inline">1 − <em>θ</em></span>で<span class="math inline">Poisson(<em>λ</em>)</span>から抽出されるとしています. 確率関数は以下のとおりです.</p>
<p><br /><span class="math display">$$p(y_{n}\mid\theta,\lambda)=\begin{cases}\theta+(1-\theta)\times\mathsf{Poisson}(0\mid\lambda) &amp; y_{n}=0のとき \\(1-\theta)\times\mathsf{Poisson}(y_{n}\mid\lambda)   &amp; y_{n} &gt; 0のとき\end{cases}$$</span><br /></p>
<p>対数確率関数はStanでは以下のようにそのまま実装できます.</p>
<pre><code>data {
  int&lt;lower=0&gt; N;
  int&lt;lower=0&gt; y[N];
}
parameters {
  real&lt;lower=0,upper=1&gt; theta;
  real lambda;
}
model {
  for (n in 1:N) {
    if (y[n] == 0)
      increment_log_prob(log_sum_exp(bernoulli_log(1,theta),
                                     bernoulli_log(0,theta)
                                     + poisson_log(y[n],lambda)));
    else
      increment_log_prob(bernoulli_log(0,theta)
                         + poisson_log(y[n],lambda));
  }
}</code></pre>
<p><code>log_sum_exp(lp1,lp2)</code>関数は対数確率を線形軸で加算します. <code>log(exp(lp1) + exp(lp2))</code>と同じですが, より算術的に安定し, 高速です.</p>
<p><code>increment_log_prob</code>関数の内部で<code>if_else</code>構文を使うようにしたいと思うかもしれませんが, <code>if_else_(c,e1,e2)</code>は<code>c</code>の値に関わらず<code>e1</code>と<code>e2</code>の両方を評価するので, これはおすすめできません.</p>
<h4 id="ハードルモデル">ハードルモデル</h4>
<p>ハードルモデルはゼロ過剰モデルに似ていますが, もっと柔軟で, 零値の結果変数が過剰のときのみならず少なすぎるときも扱うことができます. ハードルモデルの確率質量関数は以下のように定義されます.</p>
<p><br /><span class="math display">$$p(y_{n}\mid\theta,\lambda)=\begin{cases}\theta &amp; y=0のとき \\(1-\theta)\frac{\mathsf{Poisson}(y\mid\lambda)}{1-\mathsf{PoissonCDF}(0\mid\lambda)} &amp; y &gt; 0のとき\end{cases}$$</span><br /></p>
<p>ここで<span class="math inline">PoissonCDF</span>は, ポアソン分布の累積分布関数です. ハードルモデルはStanではさらに直接的にプログラミングできます. 明示的な混合分布とする必要はありません.</p>
<pre><code>(y[n] == 0) ~ bernoulli(theta);
if (y[n] &gt; 0)
  y[n] ~ poisson(lambda) T[1,];</code></pre>
<p>ポアソン分布のあとの<code>[1,]</code>は1未満の値が切断されることを示しています. これについては40.5節を参照してください. 変数<code>y[n]</code>が2回「サンプリング」されていますが, 対数確率関数への効果は（対数軸での）定義に従っています.</p>
<p>Julian Kingは, 以下のように記述するとモデルが高速化するかもしれないと指摘しました.</p>
<p><br /><span class="math display">logPoissonCDF(0 ∣ <em>λ</em>) = log(1 − exp( − <em>λ</em>))</span><br /></p>
<p>こうすると, 切断ポアソン分布は以下のようにコーディングされます.</p>
<pre><code>if (y[n] &gt; 0) {
  y[n] ~ poisson(lambda);
  increment_log_prob(-log1m_exp(-lambda));
}</code></pre>
<p>前もって計数値をまとめておくことにより, 密度を変えることなく実行速度をおおいに高速化することもできるという1例を示します. 例えば, <code>y[n] == 0</code>の場合の数を数え, 変換データ量として格納しておきます. これは,</p>
<pre><code>model {
  ...
  for (n in 1:N)
    (y[n] == 0) ~ bernoulli(theta);</code></pre>
<p>を</p>
<pre><code>transformed data {
  int N_zero;
  N_zero &lt;- 0;
  for (n in 1:N)
    if (y[n] == 0)
      N_zero &lt;- N_zero + 1;
}
model {
  ...
  N_zero ~ binomial(N, theta);</code></pre>
<p>と, 変えればできます.</p>
<p>また別の高速化の方法として, 計数値が非零（すなわち<code>y[n] &gt; 0</code>）の場合に, その数を新しく定義した配列に格納し, それからベクトル化したポアソン分布のサンプリング文を使って, 非零の計数値の数を増加分に掛けるというものがあります. これはややトリッキーです. Stanでは宣言の時に大きさが必要ですので, 零と非零の場合の数を計算する関数をこのときに使う必要があるためです.</p>
<h2 id="測定誤差とメタアナリシス">11. 測定誤差とメタアナリシス</h2>
<p>統計モデルで使われる量のほとんどは測定で得られたものです. こうした測定にはほとんどの場合, 何らかの誤差があります. 測定された量に対して測定誤差が小さければ, モデルには通常あまり影響しません. 測定された量に対して測定誤差が大きいときや, 測定された量について非常に精密な関係を推定する可能性があるときは, 測定誤差を明示的に取り入れたモデルが役に立ちます. 測定誤差の一種には丸め誤差もあります.</p>
<p>メタアナリシスは統計的には, 測定誤差モデルと非常によく似ています. メタアナリシスでは, 複数のデータセットから導かれた推定が, それら全体についての推定にまとめられます. 各データセットについての推定は, 真のパラメータの値から, 一種の測定誤差があるものとして扱われます.</p>
<h3 id="ベイズ測定誤差モデル">11.1. ベイズ測定誤差モデル</h3>
<p>ベイズ統計の手法では, 真の量を欠測データとして扱うことにより, 測定誤差を直接的に定式化できます(Clayton, 1992; Richardson and Gilks, 1993). これには, 真の値から測定値がどのように導かれるのかというモデルが必要です.</p>
<h4 id="測定誤差のある回帰">測定誤差のある回帰</h4>
<p>測定誤差のある回帰を考える前にまず, 予測変数<span class="math inline"><em>x</em><sub><em>n</em></sub></span>と結果変数<span class="math inline"><em>y</em><sub><em>n</em></sub></span>を含む, <span class="math inline"><em>N</em></span>回の観測データの線形回帰モデルを考えましょう. Stanでは, 傾きと切片のある, <span class="math inline"><em>x</em></span>についての<span class="math inline"><em>y</em></span>の線形回帰は以下のようにモデリングされます.</p>
<pre><code>data {
  int&lt;lower=0&gt; N;       // 観測回数
  real x[N];            // 予測変数（共変量）
  real y[N];            // 結果変数（変量）
}
parameters {
  real alpha;           // 切片
  real beta;            // 傾き
  real&lt;lower=0&gt; sigma;  // 結果変数のノイズ
} model {
  y ~ normal(alpha + beta * x, sigma);
  alpha ~ normal(0,10);
  beta ~ normal(0,10);
  sigma ~ cauchy(0,5);
}</code></pre>
<p>ここで, 予測変数<span class="math inline"><em>x</em><sub><em>n</em></sub></span>の真値が既知ではないとします. ただし, 各<span class="math inline"><em>n</em></span>について, <span class="math inline"><em>x</em><sub><em>n</em></sub></span>の測定値<span class="math inline"><em>x</em><sub><em>n</em></sub><sup><em>m</em><em>e</em><em>a</em><em>s</em></sup></span>は分かっています. 測定誤差をモデリングできるならば, 測定値<span class="math inline"><em>x</em><sub><em>n</em></sub><sup><em>m</em><em>e</em><em>a</em><em>s</em></sup></span>は, 真値<span class="math inline"><em>x</em><sub><em>n</em></sub></span>に測定ノイズを加算したものとモデリングできます. 真値<span class="math inline"><em>x</em><sub><em>n</em></sub></span>は欠測データとして扱われ, モデル中の他の量と同時に推定されます. 非常に単純な方法としては, 測定誤差が既知の偏差<span class="math inline"><em>τ</em></span>で正規分布すると仮定する方法があります. 以下のような, 測定誤差が一定の回帰モデルになります.</p>
<pre><code>data {
  ...
  real x_meas[N];     // xの測定値
  real&lt;lower=0&gt; tau;  // 測定ノイズ
}
parameters {
  real x[N];          // 未知の真値
  real mu_x;          // 事前分布の位置
  real sigma_x;       // 事前分布のスケール
  ...
}
model {
  x ~ normal(mu_x, sigma_x);   // 事前分布
  x_meas ~ normal(x, tau);   // 測定モデル
  y ~ normal(alpha + beta * x, sigma);
  ...
}</code></pre>
<p>回帰係数の<code>alpha</code>と<code>beta</code>, 回帰ノイズのスケール<code>sigma</code>は前と同じですが, データではなくパラメータとして<code>x</code>が新たに宣言されています. データは<code>x_meas</code>になり, 真の<code>x</code>の値からスケール<code>tau</code>のノイズを含めて測定されているとなっています. そしてこのモデルは, 真値<code>x[n]</code>についての測定誤差<code>x_meas[n]</code>が偏差<code>tau</code>の正規分布に従うと指定しています. さらに真値<code>x</code>にはここでは階層事前分布が与えられています.</p>
<p>測定誤差が正規分布でない場合には, もっと複雑な測定誤差モデルを指定することもできます. 真値の事前分布も複雑にできます. 例えば, Clayton (1992)は, 既知の（測定誤差のない）リスク要因<span class="math inline"><em>c</em></span>に対する, 未知の（ただしノイズ込みで測定された）リスク要因<span class="math inline"><em>x</em></span>についての暴露モデルを紹介しています. 単純なモデルでは, 共変量<span class="math inline"><em>c</em><sub><em>n</em></sub></span>とノイズ項<span class="math inline"><em>υ</em></span>から<span class="math inline"><em>x</em><sub><em>n</em></sub></span>を回帰するというようになるでしょう.</p>
<p><br /><span class="math display"><em>x</em><sub><em>n</em></sub> ∼ Normal(<em>γ</em><sup>⊤</sup><em>c</em>, <em>υ</em>)</span><br /></p>
<p>これはStanでは, ほかの回帰とまったく同様にコーディングできます. もちろん, さらにほかの暴露モデルも使えます.</p>
<h4 id="丸め">丸め</h4>
<p>測定誤差でよくあるのは, 測定値を丸めることに由来するものです. 丸めのやり方はたくさんあります. 重さをもっとも近いミリグラムの値に丸めたり, もっとも近いポンドの値に丸めたりします. もっとも近い整数に切り下げるという丸めもあります.</p>
<p>Gelman et al. (2013)の演習3.5(b)に以下の例題があります.</p>
<blockquote>
<p>3.5 ある物体の重さを5回はかるとします. 測定値はもっとも近いポンドの値に丸められ, 10, 10, 12, 11, 9となりました. 丸める前の測定値は正規分布するとして, <span class="math inline"><em>μ</em></span>と<span class="math inline"><em>σ</em><sup>2</sup></span>には無情報事前分布を使います. (b) 測定値が丸められていることを考慮して, <span class="math inline">(<em>μ</em>, <em>σ</em><sup>2</sup>)</span>についての正しい事後分布を求めなさい.</p>
</blockquote>
<p><span class="math inline"><em>y</em><sub><em>n</em></sub></span>の丸められていない測定値を<span class="math inline"><em>z</em><sub><em>n</em></sub></span>とします. すると, 述べられている問題は以下の尤度を仮定することになります.</p>
<p><br /><span class="math display"><em>z</em><sub><em>n</em></sub> ∼ Normal(<em>μ</em>, <em>σ</em>)</span><br /></p>
<p>丸めの過程により, <span class="math inline"><em>z</em><sub><em>n</em></sub> ∈ (<em>y</em><sub><em>n</em></sub> − 0.5, <em>y</em><sub><em>n</em></sub> + 0.5)</span>となります. 離散値の観測値<span class="math inline"><em>y</em></span>の確率質量関数は, 丸められていない測定値を周辺化消去することにより与えられ, 以下の尤度が得られます.</p>
<p><br /><span class="math display">$$p(y_{n}\mid\mu,\sigma)=\int_{y_{n}-0.5}^{y_{n}+0.5}\mathsf{Normal}(z_{n}\mid\mu,\sigma)\mathrm{d}z_{n}=\Phi\left(\frac{y_{n}+0.5-\mu}{\sigma}\right)-\Phi\left(\frac{y_{n}-0.5-\mu}{\sigma}\right)$$</span><br /></p>
<p>この問題についてのGelmanの解答では, 分散<span class="math inline"><em>σ</em><sup>2</sup></span>について対数スケールで一様分布の無情報事前分布を使いました. このときの事前密度は（ヤコビアンの調整により）以下のようになります.</p>
<p><br /><span class="math display">$$p(\mu,\sigma^2) \propto \frac{1}{\sigma^2}$$</span><br /></p>
<p><span class="math inline"><em>y</em> = (10, 10, 12, 11, 9)</span>を観測した後の事後分布は, ベイズの定理により計算できます.</p>
<p><br /><span class="math display">$$\begin{array}{ll}p(\mu,\sigma^2\mid y) &amp;\propto p(\mu,\sigma^2)p(y\mid\mu,\sigma^2)\\ &amp;\propto \frac{1}{\sigma^2}\prod_{n=1}^{5}\left(\Phi\left(\frac{y_{n}+0.5-\mu}{\sigma}\right)-\Phi\left(\frac{y_{n}-0.5-\mu}{\sigma}\right)\right) \end{array}$$</span><br /></p>
<p>Stanのコードは単純に数学的定義に従っており, 確率関数をそのまま定義したものを, 割合にするまでの例となっています.</p>
<pre><code>data {
  int&lt;lower=0&gt; N;
  vector[N] y;
}
parameters {
  real mu;
  real&lt;lower=0&gt; sigma_sq;
}
transformed parameters {
  real&lt;lower=0&gt; sigma;
  sigma &lt;- sqrt(sigma_sq);
}
model {
  increment_log_prob(-2 * log(sigma));
  for (n in 1:N)
    increment_log_prob(log(Phi((y[n] + 0.5 - mu) / sigma)
                           - Phi((y[n] - 0.5 - mu) / sigma)));
}</code></pre>
<p>別のやり方として, 丸められていない測定値<span class="math inline"><em>z</em><sub><em>n</em></sub></span>についての潜在パラメータを使ってモデルを定義する方法もあるでしょう. この場合のStanのコードは, 制約<span class="math inline"><em>z</em><sub><em>n</em></sub> ∈ (<em>y</em><sub><em>n</em></sub> − 0.5, <em>y</em><sub><em>n</em></sub> + 0.5)</span>を満たしつつ<span class="math inline"><em>z</em><sub><em>n</em></sub></span>の尤度をそのまま使います. Stanでは, ベクトル（あるいは配列）の要素についての上下限は不変でなくてなりませんので, そのパラメータは丸め誤差<span class="math inline"><em>y</em> − <em>z</em></span>として宣言され, <span class="math inline"><em>z</em></span>は<code>transformed parameter</code>として定義されます.</p>
<pre><code>data {
  int&lt;lower=0&gt; N;
  vector[N] y;
}
parameters {
  real mu;
  real&lt;lower=0&gt; sigma_sq;
  vector&lt;lower=-0.5, upper=0.5&gt;[5] y_err;
}
transformed parameters {
  real&lt;lower=0&gt; sigma;
  vector[N] z;
  sigma &lt;- sqrt(sigma_sq);
  z &lt;- y + y_err;
}
model {
  increment_log_prob(-2 * log(sigma));
  z ~ normal(mu, sigma);
}</code></pre>
<p>丸められていない測定値<span class="math inline"><em>z</em></span>を明示したこのモデルは, <span class="math inline"><em>z</em></span>を周辺化消去した前のモデルと, <span class="math inline"><em>μ</em></span>と<span class="math inline"><em>σ</em></span>について同じ事後分布を生成します. どちらのやり方とも連鎖が良く混ざりますが, 潜在パラメータを使うバージョンは, iterationあたりの有効サンプルの点で2倍ほど効率的です. また, 丸められていないパラメータの事後分布も得られます.</p>
<h3 id="メタアナリシス">11.2. メタアナリシス</h3>
<p>メタアナリシスは, いくつかの学校での指導プログラムの利用や, いくつかの臨床試験での薬を使った治療といった, いくつかの研究からのデータをプールすることを目的としています.</p>
<p>ベイズの枠組みはメタアナリシスには特に便利です. というのは, 興味ある真の量をノイズ込みで測定したものとして, 各先行研究を扱うことができるからです. このとき, モデルはそのまま2つの部分から成ります. つまり, 興味ある真の量についての事前分布と, 解析する研究のそれぞれについての測定誤差タイプのモデルです.</p>
<h4 id="対照群と比較する臨床試験における治療の効果">対照群と比較する臨床試験における治療の効果</h4>
<p>治療群と対照群について対応のある二項分布のデータが得られている研究が全部で<span class="math inline"><em>M</em></span>個あるところから問題のデータが得られているとします. 例えばデータは, イブプロフェン投与下での外科手術後の痛みの軽減や(Warn et al., 2002), ベータブロッカー投与下での心筋梗塞後の死亡率(Gelman et al, 2013, Section 5.6)といったものでしょう.</p>
<h5 id="データ">データ</h5>
<p>この臨床データは<span class="math inline"><em>J</em></span>個の臨床試験から成り立っています. それぞれの臨床試験について, 治療群に割り付けられた人数は<span class="math inline"><em>n</em><sup><em>t</em></sup></span>, 対照群に割り付けられた人数は<span class="math inline"><em>n</em><sup><em>c</em></sup></span>, 治療群で改善した（成功した）人数は<span class="math inline"><em>r</em><sup><em>t</em></sup></span>, 対照群で改善した（成功した）人数は<span class="math inline"><em>r</em><sup><em>c</em></sup></span>です. このデータはStanでは以下のように宣言できます. <sup>1</sup></p>
<pre><code>data {
  int&lt;lower=0&gt; J;
  int&lt;lower=0&gt; n_t[J];  // 治療例の数
  int&lt;lower=0&gt; r_t[J];  // 治療群での成功数
  int&lt;lower=0&gt; n_c[J];  // 対照例の数
  int&lt;lower=0&gt; r_c[J];  // 対照群での成功数
}</code></pre>
<p><sup>1</sup> Stanの整数の制約は, <code>r_t[h]</code> <span class="math inline"><em>l</em><em>e</em></span> <code>n_t[j]</code>という制約を表現できるほど強力ではありません. しかしこの制約は, <code>transformed data block</code>でチェックすることができるでしょう.</p>
<h5 id="対数オッズへの変換と標準誤差">対数オッズへの変換と標準誤差</h5>
<p>この臨床試験データは, そのままでは2項分布のフォーマットですが, 対数オッズ比を考えると, 限度のない軸に変換できるでしょう.</p>
<p><br /><span class="math display">$$y_{j}=\log\left(\frac{r^{t}_{j}/(n^{t}_{j}-r^{t}_{j})}{r^{c}_{j}/(n^{c}_{j}-r^{c}_{j})}\right)=\log\left(\frac{r^{t}_{j}}{n^{t}_{j}-r^{t}_{j}}\right)-\log\left(\frac{r^{c}_{j}}{n^{c}_{j}-r^{c}_{j}}\right)$$</span><br /></p>
<p>対応する標準誤差です.</p>
<p><br /><span class="math display">$$\sigma_{j}=\sqrt{\frac{1}{r^t_j}+\frac{1}{n^t_j-r^t_j}+\frac{1}{r^c_j}+\frac{1}{n^c_j-r^c_j}}$$</span><br /></p>
<p>対数オッズと標準誤差は<code>transformed parameters</code>ブロックで定義できますが, 整数除算にならないように注意が必要です（31.1節参照）.</p>
<pre><code>transformed data {
  real y[J];
  real&lt;lower=0&gt; sigma[J];
  for (j in 1:J)
    y[j] &lt;- log(r_t[j]) - log(n_t[j] - r_t[j])
            - (log(r_c[j]) - log(n_c[j] - r_c[j]);
  for (j in 1:J)
    sigma[j] &lt;- sqrt(1.0/r_t[j] + 1.0/(n_t[j] - r_t[j])
                     + 1.0/r_c[j] + 1.0/(n_c[j] - r_c[j]));
}</code></pre>
<p>いずれかの成功数が0だったり, 試行回数と同じだったりすると, この定義には問題が発生します. そうなる場合には, 2項分布で直接モデリングするか, 正則化しない標本対数オッズを使うのではなく, 別の変換を使う必要があります.</p>
<h5 id="非階層モデル">非階層モデル</h5>
<p>変換したデータができたら, 二つのメタアナリシスの標準型を使うことができます. 最初は, いわゆる「固定効果」モデルで, 全体的なオッズ比について単一のパラメータを仮定します. このモデルはStanでは以下のようにコーディングされます.</p>
<pre><code>parameters {
  real theta;  // 全体的な治療の効果, 対数オッズ
}
model {
  y ~ normal(theta,sigma);
}</code></pre>
<p><code>y</code>についてのサンプリング文はベクトル化されており, 以下と同じ効果があります.</p>
<pre><code>for (j in 1:J)
  y[j] ~ normal(theta,sigma[j]);</code></pre>
<p>このモデルには<code>theta</code>の事前分布を含めるのが普通ですが, <code>y</code>が固定されており, <span class="math inline">Normal(<em>y</em> ∣ <em>θ</em>, <em>σ</em>) = Normal(<em>θ</em> ∣ <em>y</em>, <em>σ</em>)</span>ですので, モデルを正則にするためにどうしても必要というわけではありません.</p>
<h5 id="階層モデル">階層モデル</h5>
<p>治療の効果が臨床試験によって変動しうる, いわゆる「ランダム効果」をモデリングするには, 階層モデルを使うことができます. パラメータには, 試験ごとの治療の効果と, 階層事前分布のパラメータを含め, ほかの未知の量をともに推定します.</p>
<pre><code>parameters {
  real theta[J];      // 試験ごとの治療の効果
  real mu;            // 平均の治療の効果
  real&lt;lower=0&gt; tau;  // 治療の効果の偏差
}
model {
  y ~ normal(theta,sigma);
  theta ~ normal(mu,tau);
  mu ~ normal(0,10);
  tau ~ cauchy(0,5);
}</code></pre>
<p>ベクトル化した<code>y</code>のサンプリング文は変化ないように見えますが, パラメータ<code>theta</code>はベクトルになっています. <code>theta</code>のサンプリング文もベクトル化されており, 超パラメータ<code>mu</code>と<code>tau</code>はそれ自身が, データのスケールと比較して幅の広い事前分布を与えられています.</p>
<p>Rubin (1981)は, 8つの学校での大学進学適性試験(Scholatic Aptitude Test, SAT)の指導の処理効果に関して, 各学校での標本処理効果と標準誤差に基づいて, 階層ベイズでメタアナリシスを行なっています. <sup>2</sup></p>
<p><sup>2</sup> Gelman et al. (2013) 5.5節のこのデータについてのモデルは, Stan example modelリポジトリ, http://mc-stan.org/documentation にデータとともに入っています.</p>
<h5 id="拡張と代替法">拡張と代替法</h5>
<p>Smith et al. (1995)とGelman et al. (2013, Section 19.4)には, 二項データそのままに基づいたメタアナリシスがあります. Warn et al. (2002)は, 二項データの変換の際に対数オッズ比の代替法を使うと, モデリングにどのような影響があるかを考察しています.</p>
<p>試験に特有の予測変数が利用できるならば, 試験ごとの治療の効果<span class="math inline"><em>θ</em><sub><em>j</em></sub></span>として回帰モデルに直接含めることができます.</p>
<h2 id="潜在離散パラメータ">12. 潜在離散パラメータ</h2>
<p>Stanは離散パラメータのサンプリングをサポートしていません. そのため, BUGSやJAGSのモデルで, 離散パラメータ（すなわち, 離散値をとる確率変数のノード）のあるものをそのまま移植することはできません. それでも, 離散パラメータを周辺化消去することにより, 上下限のある離散パラメータを含むたくさんのモデルをコーディングすることができます. <sup>1</sup>この章では, 潜在離散パラメータを含むモデルのうち広く使われているものいくつかをコーディングする方法を紹介します. この後の14章では, クラスタリングモデルについて, 潜在離散パラメータを含むモデルについてさらに検討します.</p>
<p><sup>1</sup>この計算は, 期待値最大化(EM: Expectation Maximization)アルゴリズムに含まれるものに似ています(Dempster et al., 1977).</p>
<h3 id="周辺化の利点">12.1 周辺化の利点</h3>
<p>離散パラメータを周辺化消去するには, 同時確率関数の何らかの代数計算が必要になりますが, この計算のうれしい副産物が, 周辺化された変数の事後期待値です. その変数がモデル中で関心のある量であることが多いですし, また事後期待値は, 離散パラメータのサンプリングによって推定された期待値ではなく, とりうる値すべてについての期待値が用いられます. そのため, 分布の裾もしっかりと探られるほか, 個々のiterationベースでも, より効率的なサンプリングが可能になります.</p>
<p>期待値最大化(EM: Expectation Maximization)アルゴリズムはじめ, 標準的な最適化アルゴリズムは多くが, 最尤推定アルゴリズムを記述する応用統計学の論文として公表されています. Stanでモデルをコーディングするのに必要な周辺化は, まさにこのような論文に由来します.</p>
<h3 id="変化点モデル">12.2 変化点モデル</h3>
<p>最初の例は, 1851年から1962年の, イギリスの炭鉱災害のモデルです. <sup>2</sup></p>
<p><sup>2</sup>このデータの出典は(Jarrett, 1979)ですが, この論文自体は, 前からあるデータ集を訂正した短信です.</p>
<h4 id="潜在離散パラメータのあるモデル">潜在離散パラメータのあるモデル</h4>
<p>Fonnesbeck et al. (2013)の3.1節では, 年<span class="math inline"><em>t</em></span>における災害発生数<span class="math inline"><em>D</em><sub><em>t</em></sub></span>（<strong>訳注: 原文は'disaster rate'ですが, 原典では'number of disasters'となので, 「災害発生数」と思われます</strong>）のポアソンモデルが紹介されています. このモデルには, 初期発生率(<span class="math inline"><em>e</em></span>)と後期発生率(<span class="math inline"><em>l</em></span>)の2つの発生率パラメータがあり, ある時点<span class="math inline"><em>s</em></span>で切り替わるとされています. フルモデルは, 潜在離散パラメータ<span class="math inline"><em>s</em></span>を使うと以下のようになります.</p>
<p><br /><span class="math display">$$\begin{array}{ll}e &amp;\sim \mathrm{Exponential}(r_e) \\ l &amp;\sim \mathrm{Exponential}(r_l) \\ s &amp;\sim \mathrm{Uniform}(1,T) \\ D_t &amp;\sim \mathrm{Poisson}(t &lt; s\;?\;e : l)\end{array}$$</span><br /></p>
<p>最後の行では条件演算子（3値演算子ともいう）を使っています. これは, Cおよびそれに類する言語から借りてきたものです. 条件演算子は, Rの<code>ifelse</code>と同じ挙動を示しますが, より簡潔な記法を使い, 疑問符(?)とコロン(:)で3つの引数を区切ります. 条件演算子は以下のように定義されます.</p>
<p><br /><span class="math display">$$c\;?\;x_1 : x_2 = \begin{cases} x_1 \quad c\text{が真（つまり非零）のとき} \\ x_2 \quad c\text{が偽（つまり零）のとき} \end{cases}$$</span><br /></p>
<p>Stan自身は今のところ条件演算子をサポートしていませんが, いずれサポートする予定です.</p>
<h4 id="離散パラメータの周辺化消去">離散パラメータの周辺化消去</h4>
<p>このモデルをStanでコーディングするには, 離散パラメータ<span class="math inline"><em>s</em></span>を周辺化消去して, 確率関数<span class="math inline"><em>p</em>(<em>e</em>, <em>l</em>, <em>D</em><sub><em>t</em></sub>)</span>の対数を定義するモデルを作らなくてはなりません. フル同時確率は以下のようになります.</p>
<p><br /><span class="math display">$$\begin{array}{ll}p(e,l,s,D) &amp;= p(e)p(l)p(s)p(D \mid s,e,l) \\ &amp;= \mathrm{Exponential}(e \mid r_{e}) \mathrm{Exponential}(l \mid r_{l}) \mathrm{Uniform}(s \mid 1, T) \\ &amp; \quad \prod_{t=1}^{T}\mathrm{Poisson}(D_{t} \mid t&lt;s\;?\;e : l)\end{array}$$</span><br /></p>
<p>周辺化のため, 別のやり方で事前分布と尤度とに分解します.</p>
<p><br /><span class="math display"><em>p</em>(<em>e</em>, <em>l</em>, <em>D</em>) = <em>p</em>(<em>e</em>, <em>l</em>)<em>p</em>(<em>D</em> ∣ <em>e</em>, <em>l</em>)</span><br /></p>
<p>ここで, <span class="math inline"><em>s</em></span>を周辺化した尤度は以下のように定義されます.</p>
<p><br /><span class="math display">$$\begin{array}{ll}p(D \mid e,l) &amp;= \sum_{s=1}^{T}p(s,D \mid e,l) \\ &amp;= \sum_{s=1}^{T}p(s)p(D \mid s,e,l) \\ &amp;= \sum_{s=1}^{T}\mathrm{Uniform}(s \mid 1,T)\prod_{t=1}^{T}\mathrm{Poisson}(D_{t} \mid t&lt;s\;?\;e:l)\end{array}$$</span><br /></p>
<p>Stanは対数スケールで処理をおこないますので, 対数尤度が必要です.</p>
<p><br /><span class="math display">$$\begin{array}{ll}\log p(D \mid e,l) &amp;= \mathrm{log\_sum\_exp}_{s=1}^{T}\left(\log\mathrm{Uniform}(s \mid 1,T)\right. \\ &amp;\quad\left. + \sum_{t=1}^{T}\log\mathrm{Poisson}(D_{t} \mid t&lt;s\;?\;e : l)\right)\end{array}$$</span><br /></p>
<p>ここで, log_sum_exp関数は以下のように定義されます.</p>
<p><br /><span class="math display">$$\mathrm{log\_sum\_exp}_{n=1}^{N}\alpha_{n} = \log\sum_{n=1}^{N}\exp(\alpha_{n})$$</span><br /></p>
<p>log_sum_exp関数は, Stanでは<code>log_sum_exp</code>として組み込まれていますので, これを使ってモデルをそのままコーディングできます. これにより, 算術的にも安定し, 混合分布モデルの計算も効率的になります.</p>
<h4 id="stanでのモデルのコーディング-1">Stanでのモデルのコーディング</h4>
<p>変化点モデルのStanのプログラムは図12.1に示します. 変換パラメータ(transformed parameter) <code>lp[s]</code>は<span class="math inline">log<em>p</em>(<em>s</em>, <em>D</em> ∣ <em>e</em>, <em>l</em>)</span>の値を格納します.</p>
<h4 id="mcmcによるモデルの当てはめ">MCMCによるモデルの当てはめ</h4>
<p>このモデルは, デフォルト設定のNUTSによるMCMCで簡単に当てはめができます. 収束はとても速く, サンプリングでは, おおよそ2回の繰り返しあたり1個の有効サンプルが得られます. 相対的に小さなモデルなので（時間についての内部の2重ループはおおよそ20000ステップ）, とても速いのです.</p>
<p><code>lp</code>は変換パラメータ(transformed parameter)として宣言されていますので, 各変化点についてiterationごとの<code>lp</code>の値を得ることもできます. もし<code>lp</code>の値に関心がないのであれば, 局所変数としてコーディングできます. この場合, iterationごとに値を保存することによるI/Oのオーバヘッドがなくなります.</p>
<pre><code>data {
  real&lt;lower=0&gt; r_e;
  real&lt;lower=0&gt; r_l;
  int&lt;lower=1&gt; T;
  int&lt;lower=0&gt; D[T];
}
transformed data {
  real log_unif;
  log_unif &lt;- -log(T);
}
parameters {
  real&lt;lower=0&gt; e;
  real&lt;lower=0&gt; l;
}
transformed parameters {
  vector[T] lp;
  lp &lt;- rep_vector(log_unif, T);
  for (s in 1:T)
    for (t in 1:T)
      lp[s] &lt;- lp[s] + poisson_log(D[t], if_else(t &lt; s, e, l));
}
model {
  e ~ exponential(r_e);
  l ~ exponential(r_l);
  increment_log_prob(log_sum_exp(lp));
}</code></pre>
<p>図12.1: 変化点モデル. 災害発生数<code>D[t]</code>は, 変化点の前ではある発生率<code>e</code>に, 変化点の後では別の発生率<code>l</code>に従います. 変化点自身は<code>s</code>であり, 本文の記述のように周辺化消去されています.</p>
<h4 id="離散変化点の事後分布">離散変化点の事後分布</h4>
<p>あるiterationにおける<code>lp[s]</code>の値は, そのiterationにおける初期発生率<span class="math inline"><em>e</em></span>と後期発生率<span class="math inline"><em>l</em></span>の値を用いて, <span class="math inline">log<em>p</em>(<em>s</em>, <em>D</em> ∣ <em>e</em>, <em>l</em>)</span>により与えられます. 収束後はiterationごとに, 初期および後期災害発生率<span class="math inline"><em>e</em></span>および<span class="math inline"><em>l</em></span>が, 事後の<span class="math inline"><em>p</em>(<em>e</em>, <em>l</em> ∣ <em>D</em>)</span>からMCMCサンプリングにより抽出され, 関連する<code>lp</code>が計算されます. <code>lp</code>の値は, 各iterationにおけるその時点での<span class="math inline"><em>e</em></span>と<span class="math inline"><em>l</em></span>の値にもとづいて, <span class="math inline"><em>p</em>(<em>s</em> ∣ <em>e</em>, <em>l</em>, <em>D</em>)</span>を計算することで正規化できるでしょう. iteration全体を平均すると, 変化点が<span class="math inline"><em>s</em></span>であることの正規化されていない確率の推定値が得られます.</p>
<p><br /><span class="math display">$$\begin{array}{ll}p(s \mid D) &amp;\propto q(s \mid D) \\ &amp;= \frac{1}{M}\sum_{m=1}^{M}\exp(\mathrm{lp}[m,s])\end{array}$$</span><br /></p>
<p>ここで, <code>lp</code>[<span class="math inline"><em>m</em></span>,<span class="math inline"><em>s</em></span>]は, 事後抽出<span class="math inline"><em>m</em></span>における変化点<span class="math inline"><em>s</em></span>についての<code>lp</code>の値を表します. 抽出全体を平均すると, <span class="math inline"><em>e</em></span>と<span class="math inline"><em>l</em></span>の方が周辺化消去され, ある繰り返しでの<span class="math inline"><em>e</em></span>と<span class="math inline"><em>l</em></span>の値には結果は依存しなくなります. 最終的に正規化すると, 求めたい量, すなわちデータ<span class="math inline"><em>D</em></span>を条件とする, <span class="math inline"><em>s</em></span>が変化点であることの事後確率が得られます.</p>
<p><br /><span class="math display">$$p(s \mid D) = \frac{q(s \mid D)}{\sum_{s'=1}^{T}q(s' \mid D)}$$</span><br /></p>
<p>Stan 2.4のデフォルトのMCMC実装を使って計算した<span class="math inline">log<em>p</em>(<em>s</em> ∣ <em>D</em>)</span>の値のグラフを図12.2に示します.</p>
<figure>
<img src="data:image/png;base64,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" alt="変化点の事後推定値のグラフ" /><figcaption>変化点の事後推定値のグラフ</figcaption>
</figure>
<p>図12.2: 変化点の事後推定値. 左) <code>lp</code>を使って解析的に計算した, 各年が変化点である対数確率. 右) <code>lp</code>を使って生成した事後分布における変化点の抽出の頻度. 左のグラフは対数スケールで, 右のグラフは線形スケールです. 右側のグラフではサンプリングのために年の範囲が狭くなっていることに注意. <span class="math inline"><em>s</em></span>の事後平均はおよそ1891です.</p>
<h4 id="離散サンプリング">離散サンプリング</h4>
<p><code>generated quantities</code>ブロックを利用すると, 組込みの擬似乱数発生器を使って離散パラメータ値を抽出することができるでしょう. 例えば, <code>lp</code>を上のように定義すると, 繰り返しごとの<code>s</code>のランダムな値を抽出するプログラムは以下のようになります.</p>
<pre><code>generated quantities {
  int&lt;lower=1,upper=T&gt; s;
  s &lt;- categorical_rng(softmax(lp));
}</code></pre>
<p><code>s</code>の抽出の事後分布のヒストグラムを図12.2の右側に示します.</p>
<p>期待値について計算するのと比べると, 離散サンプリングはとても非効率的です. とくに分布の裾ではそうですので, こうした手法は, 分布からの抽出が明示的に必要なときにだけ使うべきです. そうでないときは期待値を計算すべきでしょう. <code>softmax(lp)</code>が与えられたときの<code>s</code>の事後分布にもとづいて<code>generated quantities</code>ブロックで計算できます.</p>
<h4 id="事後共分散">事後共分散</h4>
<p><code>s</code>について生成された離散サンプルは, 他のパラメータとの共分散を計算するのにも使えます. このサンプリング手法は単純ですが, 期待される共分散を<code>lp</code>を使って計算するのが（同じ程度の精度を得るのに必要な繰り返しがはるかに少ないという意味で）より統計学的に効率的です.</p>
<h4 id="多変化点">多変化点</h4>
<p>複数の変化点を持たせるのも原理的には難しいことはありません. 問題は計算が増えることだけです. 1変化点では1次だった周辺化消去が, 2変化点では2次になり, 3変化点では3次, というふうになります. 変化点が2つのときは, 期間のパラメータは3つ, <code>e</code>, <code>m</code>, <code>l</code>となり, 変化点についての2つのループと, 時間全体についての1つのループを設定します. 変化点が2つなので, 対数密度は行列に格納されます.</p>
<pre><code>matrix[T,T] lp;
  lp &lt;- rep_matrix(log_unif,T);
  for (s1 in 1:T)
    for (s2 in 1:T)
      for (t in 1:T)
        lp[s1,s2] &lt;- lp[s1,s2]
          + poisson_log(D[t], if_else(t &lt; s1, e, if_else(t &lt; s2, m, l)));</code></pre>
<p>この行列は, <code>log_sum_exp</code>に渡す前に<code>to_vector</code>でベクトルに戻すことができます.</p>
<h3 id="標識再捕獲モデル">12.3 標識再捕獲モデル</h3>
<p>生態学の野外調査法で広く応用されているものに, 動物を捕獲（または視認）し, （タグなどで）標識して, それから放すという方法があります. このプロセスはさらに1回以上繰り返され, 多くの場合は調査継続中の集団（<strong>訳注:'population'は, 生態学では「個体群」と訳すのが普通ですが, 遺伝学では「集団」と訳しますし, その方がわかりやすいでしょうから, ここでは「集団」と訳しています. </strong>）に対して行なわれます. 結果のデータは, 集団サイズを推定するのに使うことができます.</p>
<p>最初の小節では, 潜在離散パラメータを含まないごく単純な捕獲再捕獲モデルを記述します. その後の小節では, 動物の死についての潜在離散パラメータを含むCormack-Jolly-Seberモデルを記述します.</p>
<h4 id="単純な標識再捕獲モデル">単純な標識再捕獲モデル</h4>
<p>もっとも単純な場合では, 1段階の標識再捕獲調査から以下のデータが得られます.</p>
<ul>
<li><span class="math inline"><em>M</em></span>: 最初の捕獲で標識された動物の数</li>
<li><span class="math inline"><em>C</em></span>: 2回目に捕獲された動物の数</li>
<li><span class="math inline"><em>R</em></span>: 2回目に捕獲されたうち標識されていた動物の数</li>
</ul>
<p>目的の推定対象は以下です.</p>
<ul>
<li><span class="math inline"><em>N</em></span>: 集団内の動物の数</li>
</ul>
<p>上の説明と話が違うのですが, このモデルでは<span class="math inline"><em>N</em></span>を連続パラメータとしましょう. 集団は有限でなくてはならないといっても, それを表すパラメータはその限りではありません. このパラメータは, 集団サイズの推定値を実数値で求めるのに使います.</p>
<p>Lincoln-Petersen法(Lincoln, 1930; Petersen, 1896)を集団サイズの推定に使います.</p>
<p><br /><span class="math display">$$\hat{N} = \frac{MC}{R}$$</span><br /></p>
<p>この集団推定値は, 再捕獲された動物の数が二項分布に従うという確率モデルに基づいたものでしょう.</p>
<p><br /><span class="math display"><em>R</em> ∼ <em>B</em><em>i</em><em>n</em><em>o</em><em>m</em><em>i</em><em>a</em><em>l</em>(<em>C</em>, <em>M</em>/<em>N</em>)</span><br /></p>
<p>ここでは, 2回目に捕獲された動物の数の合計(<span class="math inline"><em>C</em></span>)と, 再捕獲率<span class="math inline"><em>M</em>/<em>N</em></span>を与えています. 再捕獲率は, 全個体数<span class="math inline"><em>N</em></span>のうち, 最初に標識された個体の割合に等しいとしています.</p>
<p>Lincoln-Petersen推定量を確率的にしたものは, 図12.3のようにStanではそのままコーディングできます. このときのLincoln-Petersen推定値はこのモデルでは最尤推定値(MLE)です.</p>
<pre><code>data {
  int&lt;lower=0&gt; M;
  int&lt;lower=0&gt; C;
  int&lt;lower=0,upper=min(M,C)&gt; R;
}
parameters {
  real&lt;lower=(C - R + M)&gt; N;
}
model {
  R ~ binomial(C, M / N);
}</code></pre>
<p>図12.3: Lincoln-Petersen推定量の確率的定式化. 1段階標識再捕獲調査からのデータに基づいて集団サイズを推定します. ありえない値を効率的に除くため, Nの下限は必要です.</p>
<p>最尤推定値が確実にLincoln-Petersen推定値になるように, <span class="math inline"><em>N</em></span>には非正則一様事前分布が使われています. 可能であれば, 調査している集団についての知識に基づいて, もっと情報のある事前分布を使うこともできるでしょう（そしてそうすべきです）.</p>
<p>このモデルでトリッキーなところは, 集団サイズ<span class="math inline"><em>N</em></span>の下限<span class="math inline"><em>C</em> − <em>R</em> + <em>M</em></span>でしょう. これより小さい値では, 再捕獲された<span class="math inline"><em>C</em></span>匹の動物から<span class="math inline"><em>R</em></span>だけのサンプルを抽出することができないので, この下限より小さな値は取りえないのです. 変換された（制約のない）空間でのパラメータに制限がかからないようにして, サンプリングと最適化が制約なく行なわれるのを確実にするため, この下限を実装することは必要です. <span class="math inline"><em>C</em></span>について宣言されたこの下限は, 次の変数変換を暗黙のうちに伴います: <span class="math inline"><em>f</em> : (<em>C</em> − <em>R</em> + <em>M</em>, ∞) → ( − ∞,  + ∞)</span>, <span class="math inline"><em>f</em></span>は<span class="math inline"><em>f</em>(<em>N</em>) = log(<em>N</em> − (<em>C</em> − <em>R</em> + <em>M</em>))</span>と定義されます. 下限を宣言する変数に使われる変換についてさらに知るには56.2節を参照してください.</p>
<h4 id="離散パラメータのあるcormack-jolly-seberモデル">離散パラメータのあるCormack-Jolly-Seberモデル</h4>
<p>Cormack-Jolly-Seber (CJS)モデル(Cormack, 1964; Jolly, 1965; Seber, 1965)は, 死亡により集団が時間的に変化する開放集団モデルです. ここで紹介している例は(Schofield, 2007)から多くを引いてきています.</p>
<p>基本データは以下のとおりです.</p>
<ul>
<li><span class="math inline"><em>I</em></span>: 個体数</li>
<li><span class="math inline"><em>T</em></span>: 捕獲時点の数</li>
<li><span class="math inline"><em>y</em><sub><em>i</em>, <em>t</em></sub></span>: 個体<span class="math inline"><em>i</em></span>が時点<span class="math inline"><em>t</em></span>に捕獲されたかどうかを示す論理値</li>
</ul>
<p>各個体は少なくとも1回は捕獲されていると仮定されます. これは, 最初に捕獲された後という条件付きでしか個体が情報に寄与しないからです.</p>
<p>このモデルには2個のベルヌーイパラメータがあります.</p>
<ul>
<li><span class="math inline"><em>ϕ</em><sub><em>t</em></sub></span>: 時点<span class="math inline"><em>t</em></span>で生存していた動物が<span class="math inline"><em>t</em> + 1</span>まで生存する確率</li>
<li><span class="math inline"><em>p</em><sub><em>t</em></sub></span>: 時点<span class="math inline"><em>t</em></span>で生存していた動物が時点<span class="math inline"><em>t</em></span>に捕獲される確率</li>
</ul>
<p>これらパラメータには一様事前分布を与えますが, 実際には情報を使って事前分布を狭くするべきでしょう.</p>
<p>CJSモデルはまた, 潜在離散パラメータ<span class="math inline"><em>z</em><sub><em>i</em>, <em>t</em></sub></span>を使います. これは, 各個体<span class="math inline"><em>i</em></span>が時点<span class="math inline"><em>t</em></span>で生存しているかどうかを示すものです.</p>
<p><br /><span class="math display"><em>z</em><sub><em>i</em>, <em>t</em></sub> ∼ <em>B</em><em>e</em><em>r</em><em>n</em><em>o</em><em>u</em><em>l</em><em>l</em><em>i</em>(<em>z</em><sub><em>i</em>, <em>t</em> − 1</sub> ? <em>ϕ</em><sub><em>t</em> − 1</sub> : 0)</span><br /></p>
<p>（<strong>訳注:原文では0と<span class="math inline"><em>ϕ</em><sub><em>t</em> − 1</sub></span>が逆になっていますが, 条件演算子の定義からするとこちらの方が正しいはずです. </strong>）</p>
<p>この条件によりゾンビが発生することを防ぎます. つまり, 動物はいったん死んだらならば, ずっと死んだままになります. データの分布は<span class="math inline"><em>z</em></span>を条件として単純に以下のように表されます.</p>
<p><br /><span class="math display"><em>y</em><sub><em>i</em>, <em>t</em></sub> ∼ <em>B</em><em>e</em><em>r</em><em>n</em><em>o</em><em>u</em><em>l</em><em>l</em><em>i</em>(<em>z</em><sub><em>i</em>, <em>t</em></sub> ? <em>p</em><sub><em>t</em></sub> : 0)</span><br /></p>
<p>（<strong>訳注:これも上の式と同様です. </strong>）</p>
<p>この条件により, 死亡した動物は捕獲されることがないという制約がつけられます.</p>
<h4 id="集合cormack-jolly-seberモデル">集合Cormack-Jolly-Seberモデル</h4>
<p>この小節では, 3回の捕獲調査を行なった場合について, 個体の捕獲プロファイルの別を計数するというモデルの実装を紹介します. このモデルは, どの動物も同じ捕獲率と同じ生存率を持つという交換可能性を仮定しています.</p>
<p>潜在離散パラメータ<span class="math inline"><em>z</em> &lt; <em>s</em><em>u</em><em>b</em> &gt; <em>i</em>, <em>t</em> &lt; /<em>s</em><em>u</em><em>b</em> &gt; </span>の周辺化を簡単にするため, Stanのモデルでは, ある時点<span class="math inline"><em>t</em></span>で生存していた個体（死亡していれば, 再捕獲率は0です）がもう2度と捕獲されない確率<span class="math inline"><em>χ</em><sub><em>t</em></sub></span>をうまく使います. この量は再帰的に定義されます.</p>
<p><br /><span class="math display">$$\begin{array}{ll}\chi_{t} = \begin{cases} 1 &amp;\quad t = Tのとき \\ (1 - \phi_{t}) + \phi_{t}(1 - p_{t+1})\chi_{t+1} &amp;\quad t &lt; Tのとき \end{cases}\end{array}$$</span><br /></p>
<p>ある個体が最終調査時に捕獲された場合, このとき, もう捕獲調査はありませんので, もう捕獲されない確率は1になります. そのため, これが再帰のベースになります. <span class="math inline"><em>χ</em><sub><em>t</em> + 1</sub></span>から, 再帰的に<span class="math inline"><em>χ</em><sub><em>t</em></sub></span>を定義しますが, この場合には2つの確率が含まれます. (1)確率(<span class="math inline">1 − <em>ϕ</em><sub><em>t</em></sub></span>)で, 次回調査時点まで生存しない, (2)確率<span class="math inline"><em>ϕ</em><sub><em>t</em></sub></span>で, 次回調査時点まで生存するが, 次回調査時点では確率(<span class="math inline">1 − <em>p</em><sub><em>t</em> + 1</sub></span>)で捕獲されず, かつ, 確率<span class="math inline"><em>χ</em><sub><em>t</em> + 1</sub></span>で, 時点<span class="math inline"><em>t</em> + 1</span>で生存していた後に2度と捕獲されない.</p>
<p>3回の捕獲調査の場合, ある個体についての捕獲/非捕獲のプロファイルが3つあることになります. これは2値の数字として以下のように自然に符号化されます.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">プロファイル</th>
<th style="text-align: center;">捕獲/1 2 3</th>
<th style="text-align: center;">確率</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;">- - -</td>
<td style="text-align: center;">n/a</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">- - +</td>
<td style="text-align: center;">n/a</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2</td>
<td style="text-align: center;">- + -</td>
<td style="text-align: center;"><span class="math inline"><em>χ</em><sub>2</sub></span></td>
</tr>
<tr class="even">
<td style="text-align: center;">3</td>
<td style="text-align: center;">- + +</td>
<td style="text-align: center;"><span class="math inline"><em>ϕ</em><sub>2</sub><em>ϕ</em><sub>3</sub></span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">4</td>
<td style="text-align: center;">+ - -</td>
<td style="text-align: center;"><span class="math inline"><em>χ</em><sub>1</sub></span></td>
</tr>
<tr class="even">
<td style="text-align: center;">5</td>
<td style="text-align: center;">+ - +</td>
<td style="text-align: center;"><span class="math inline"><em>ϕ</em><sub>1</sub>(1 − <em>p</em><sub>2</sub>)<em>ϕ</em><sub>2</sub><em>p</em><sub>3</sub></span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">6</td>
<td style="text-align: center;">+ + -</td>
<td style="text-align: center;"><span class="math inline"><em>ϕ</em><sub>1</sub><em>p</em><sub>2</sub><em>χ</em><sub>2</sub></span></td>
</tr>
<tr class="even">
<td style="text-align: center;">7</td>
<td style="text-align: center;">+ + +</td>
<td style="text-align: center;"><span class="math inline"><em>ϕ</em><sub>1</sub><em>p</em><sub>2</sub><em>ϕ</em><sub>2</sub><em>p</em><sub>3</sub></span></td>
</tr>
</tbody>
</table>
<p>履歴0, すなわち動物が1度も捕獲されない場合は観測不可能です. 捕獲された動物だけが観測されるからです. 履歴1, すなわち最終回にのみ動物が捕獲される場合は, CJSモデルでは情報は得られません. 捕獲/非捕獲の状態から情報が得られるのは, 前の捕獲を条件としたときだけだからです. 残りの場合には, 最後の列に示したように尤度への寄与があります.</p>
<p><span class="math inline"><em>χ</em></span>を使って直接こうした確率を定義することで, 動物が時点<span class="math inline"><em>t</em></span>で生きているかどうかを示す潜在2値パラメータは必要なくなります. この<span class="math inline"><em>χ</em></span>の定義は, CJSモデルでの尤度（すなわち潜在離散パラメータを周辺化消去した）の定義では典型的です(Schofield, 2007, 9ページ).</p>
<p>Stanのモデルでは, パラメータ<span class="math inline"><em>ϕ</em></span>と<span class="math inline"><em>p</em></span>に基づいて変換パラメータ(transformed parameter)として<span class="math inline"><em>χ</em></span>を定義しています. <code>model</code>ブロックでは, 各履歴についてその回数に応じて対数確率が加算されます. 2番目のステップは, ベルヌーイ分布の観測値を二項分布に, あるいはカテゴリカルな観測値を多項分布にまとめるのと似ています. ただし, このStanのプログラムでは組込みの確率関数を使わず, <code>increment_log_prob</code>を使って直接コーディングしています.</p>
<pre><code>data {
  int&lt;lower=0&gt; history[7];
}
parameters {
  real&lt;lower=0,upper=1&gt; phi[2];
  real&lt;lower=0,upper=1&gt; p[3];
}
transformed parameters {
  real&lt;lower=0,upper=1&gt; chi[2];
  chi[2] &lt;- (1 - phi[2]) + phi[2] * (1 - p[3]);
  chi[1] &lt;- (1 - phi[1]) + phi[1] * (1 - p[2]) * chi[2];
}
model {
  increment_log_prob(history[2] * log(chi[2]));
  increment_log_prob(history[3] * (log(phi[2]) + log(p[3])));
  increment_log_prob(history[4] * (log(chi[1])));
  increment_log_prob(history[5]
                     * (log(phi[1]) + log1m(p[2])
                        + log(phi[2]) + log(p[3])));
  increment_log_prob(history[6]
                     * (log(phi[1]) + log(p[2])
                        + log(chi[2])));
  increment_log_prob(history[7]
                     * (log(phi[1]) + log(p[2])
                        + log(phi[2]) + log(p[3])));
}
generated quantities {
  real&lt;lower=0,upper=1&gt; beta3;
  beta3 &lt;- phi[2] * p[3];
}</code></pre>
<p>図12.4: Cormack-Jolly-Seber標識再捕獲モデルのStanプログラム. 3回の捕獲調査での観測のありなしという観測履歴ごとに個体の数をまとめています.</p>
<h5 id="識別可能性-1">識別可能性</h5>
<p>パラメータ<span class="math inline"><em>ϕ</em><sub>2</sub></span>と<span class="math inline"><em>p</em><sub>3</sub></span>, すなわち時点2での生存率（<strong>訳注:原文ではprobability of deathとなっていますが, <span class="math inline"><em>ϕ</em></span>の定義は「生存率」です</strong>）と時点3での捕獲率とは識別可能ではありません. これは, 時点3で捕獲されなかったことの説明にこの両方ともが使えるからです. これらの積<span class="math inline"><em>β</em><sub>3</sub> = <em>ϕ</em><sub>2</sub><em>p</em><sub>3</sub></span>は識別可能です. Stanのモデルでは<code>beta3</code>を生成量(generated quantity)として定義しています. 識別不可能なパラメータは, Stanのサンプラーの適応(adaptation)に問題を引き起こします. パラメータに制限があって, 正則一様分布が使われていたため, このモデルでは適応に大きな問題は発生しませんでしたが, 識別可能なパラメータとなるように定式化した方が良いでしょう. そのためには, <span class="math inline"><em>p</em></span>と<span class="math inline"><em>ϕ</em></span>のパラメータについて階層モデルとして定式化するのもひとつの方法でしょう.</p>
<h4 id="個体cormack-jolly-seberモデル">個体Cormack-Jolly-Seberモデル</h4>
<p>この小節では, 前の小節で紹介したような集合的なものではなく, 個体レベルで動くバージョンのCormack-Jolly-Seber (CJS)モデルを紹介します. また, これによりモデルは, 任意の調査回数に対応することができるようになります. データは, 捕獲調査の回数<span class="math inline"><em>T</em></span>, 個体数<span class="math inline"><em>I</em></span>, 個体<span class="math inline"><em>i</em></span>が時点<span class="math inline"><em>t</em></span>で観測されたかを示す論理値フラグ<span class="math inline"><em>y</em><sub><em>i</em>, <em>t</em></sub></span>からなります. Stanでは以下のように記述されます.</p>
<pre><code>data {
  int&lt;lower=2&gt; T;
  int&lt;lower=0&gt; I;
  int&lt;lower=0,upper=1&gt; y[I,T];
}</code></pre>
<p>個体レベルモデルの利点に, 生存率や捕獲率に影響する個体の「ランダム効果」を含めることが可能になります. そのほか, <span class="math inline"><em>T</em></span>回の捕獲があった場合<span class="math inline">2<sup><em>T</em></sup></span>個の観測履歴を展開した組み合わせを使わなくて良いという点もあります.</p>
<h5 id="ユーティリティ関数">ユーティリティ関数</h5>
<p>この個体CJSモデルは, いくつかの関数定義を含むように書かれています. はじめの2つは, <code>transformed data</code>ブロックで使われており, 動物が捕獲された最初および最後の調査時点を計算します. <sup>3</sup></p>
<p><sup>3</sup>別の方法として, この値をモデルの外で計算して, Stanのモデルに前処理済みデータとして与えることもできるでしょう. さらに別のコーディングとして, 捕獲イベントのときにその時点と捕獲された個体とを記録するという疎なものもあります.</p>
<pre><code>functions {
  int first_capture(int[] y_i) {
   for (k in 1:size(y_i))
     if (y_i[k])
       return k;
    return 0;
  }
  int last_capture(int[] y_i) {
    for (k_rev in 0:(size(y_i) - 1)) {
      int k;
      k &lt;- size(y_i) - k_rev;
      if (y_i[k])
        return k;
    }
    return 0;
  }
  ...
}</code></pre>
<p>この2つの関数は, <code>transformed data</code>ブロックで各個体の最初と最後の捕獲時点を定義するのに使われます. <sup>4</sup></p>
<p><sup>4</sup>入力された配列の個体が1度も捕獲されていないときは, 両方の関数とも0を返します. このモデルでは, どの確率計算もより前の捕獲を条件としているので, 捕獲されていない個体はモデルの推定に無関係です. 通常はこうしたデータは除かれるのですが, このプログラムでは, 対数確率関数への寄与はなくとも, そうしたデータが含まれてもよいようになっています.</p>
<pre><code>transformed data {
  int&lt;lower=0,upper=T&gt; first[I];
  int&lt;lower=0,upper=T&gt; last[I];
  vector&lt;lower=0,upper=I&gt;[T] n_captured;
  for (i in 1:I)
    first[i] &lt;- first_capture(y[i]);
  for (i in 1:I)
    last[i] &lt;- last_capture(y[i]);
  n_captured &lt;- rep_vector(0,T);
  for (t in 1:T)
    for (i in 1:I)
      if (y[i,t])
        n_captured[t] &lt;- n_captured[t] + 1;
}</code></pre>
<p><code>transformed data</code>ブロックでは<code>n_captured[t]</code>も定義しています. これは, 時点<code>t</code>における捕獲数の合計値です. <code>n_captured[t]</code>変数は, 整数配列ではなくベクトルとして定義されています. これにより, 各時点での集団推定値をモデル化するために<code>generated quantities</code>ブロックで要素ごとのベクトル操作を使えるようになります.</p>
<p>パラメータと変換パラメータは前のとおりですが, ある個体が時点<span class="math inline"><em>t</em></span>で生きていたときにその2度と捕獲されない確率として, ベクトル<code>chi</code>を全部計算する関数が定義されています.</p>
<pre><code>parameters {
  vector&lt;lower=0,upper=1&gt;[T-1] phi;
  vector&lt;lower=0,upper=1&gt;[T] p;
}
transformed parameters {
  vector&lt;lower=0,upper=1&gt;[T] chi;
  chi &lt;- prob_uncaptured(T,p,phi);
}</code></pre>
<p><code>prob_uncaptured</code>の定義は<code>functions</code>ブロックに置きます.</p>
<pre><code>functions {
  ...
  vector prob_uncaptured(int T, vector p, vector phi) {
    vector[T] chi;
    chi[T] &lt;- 1.0;
    for (t in 1:(T - 1)) {
      int t_curr;
      int t_next;
      t_curr &lt;- T - t;
      t_next &lt;- t_curr + 1;
      chi[t_curr] &lt;- (1 - phi[t_curr])
                     + phi[t_curr]
                       * (1 - p[t_next])
                       * chi[t_next];
    }
    return chi;
  }
}</code></pre>
<p>この関数は<span class="math inline"><em>χ</em><sub><em>t</em></sub></span>の数学的定義をそのままなぞったものです. 再帰を繰り返しに展開し, <code>T</code>から1まで<code>chi</code>の要素を定義しています.</p>
<h5 id="モデル">モデル</h5>
<p>前もって計算させた量を与えられたもとで, <code>model</code>ブロックではCJSモデルの対数尤度関数をそのままエンコードしています. すべてのパラメータの事前分布はデフォルトの一様事前分布とし, パラメータ<code>p</code>および<code>phi</code>と, <code>p</code>および<code>phi</code>から定義される変換パラメータ<code>chi</code>とが与えられたもとでの観測<code>q</code>の対数確率をモデルは単純にエンコードしています.</p>
<pre><code>model {
  for (i in 1:I) {
    if (first[i] &gt; 0) {
      for (t in (first[i]+1):last[i]) {
        1 ~ bernoulli(phi[t-1]);
        y[i,t] ~ bernoulli(p[t]);
      }
      1 ~ bernoulli(chi[last[i]]);
    }
}</code></pre>
<p>外側のループは全個体についてのもので, 1度も捕獲されていない個体<code>i</code>は飛ばすという条件がついています. 1度も捕獲されていないことのチェックには, 1度も捕獲されていない個体では初回および最終捕獲の関数が0を返すという決まりにより, <code>first</code>が0になることを利用しています.</p>
<p>内側のループは個体<code>i</code>について, 個体が確率<code>phi[t-1]</code>で生存することに基づいて対数確率をまず加算します. 結果が1で固定されているのは, 最初と最後の捕獲の間にはその個体は生存していることが確実だからです（ゾンビはいません）. このループは最初の捕獲の後から始まります. これはCJSモデルでは全情報が最初の捕獲を条件としているからです.</p>
<p>内側のループでは, 個体<code>i</code>の時点<code>t</code>における観測捕獲状態<code>y[i,t]</code>は, 時点<code>t</code>における捕獲率<code>p[t]</code>に基づいたベルヌーイ分布に従います.</p>
<p>内側のループの後には, ある動物が時点<code>last[i]</code>で観測されてから2度と確認されない確率を入れています. これは, <code>last[i]</code>が, 動物<code>i</code>が観測された最後の調査時点と定義されているからです.</p>
<h5 id="識別されるパラメータ">識別されるパラメータ</h5>
<p>前の小節で記述した集合モデルと同じく, このモデルでも<code>phi[T-1]</code>と<code>p[T]</code>は識別可能ではありません. 識別可能なのはその積<code>beta</code>です. そこで, genetated quantityとして<code>beta</code>を定義し, 収束の監視と報告の対象とします.</p>
<pre><code>generated quantities {
  real beta;
  ...

  beta &lt;- phi[T-1] * p[T];
  ...
}</code></pre>
<p>パラメータ<code>p[1]</code>もモデル化されていませんし, 0から1の一様分布となるだけでしょう. もっとうまくモデルをつくるなら, 階層的成分や, 時系列成分を含めるようにするとよいかもしれません. その場合, <code>p[1]</code>は未知の初期状態となり, <code>phi[T-1]</code>と<code>p[T]</code>はともに識別可能となるでしょう.</p>
<h5 id="集団サイズの推定値">集団サイズの推定値</h5>
<p>generated quantitiesでは, 各時点<code>t</code>における集団サイズの平均の推定値も計算します. 方法は, 単純な標識再捕獲モデルと同様で, 時点<code>t</code>における捕獲個体数を, 時点<code>t</code>における捕獲確率で割って求めます. ここでは, <code>generated quantities</code>ブロックで, <code>vector</code>の要素ごとの除算演算子(./)を使用します.</p>
<pre><code>generated quantities {
  ...
  vector&lt;lower=0&gt;[T] pop;
  ...
  pop &lt;- n_captured ./ p;
  pop[1] &lt;- -1;
}</code></pre>
<h5 id="個体効果への一般化">個体効果への一般化</h5>
<p>このモデルは, 個体すべてが同じ捕獲率と持つというふうにモデル化されていますが, 一般化は簡単にできるでしょう. その場合, 個体レベルの入力値を予測変数として使って, これに基づくロジスティック回帰を使います.</p>
<h3 id="データ符号化と診断正答率のモデル">12.4 データ符号化と診断正答率のモデル</h3>
<p>かけはなれた仕事のように見えますが, アイテムをカテゴリーに分けて評価/符号化/注釈するのと, 病気か病気でないかを診断検査することとの間には共通する特徴があり, 両者の統計的特性は同じようにモデル化できます.</p>
<h4 id="診断正答率">診断正答率</h4>
<p>さまざまな感度と特異度の条件で診断検査を行なうとします. 感度とは, 検査をうける人が疾患ありの時に正しく検査が陽性となる確率です. 特異度は, 検査をうける人が疾患なしの時に正しく検査が陰性となる確率です. 乳癌の検査方法であるマンモグラムと穿刺生検を例にします. マンモグラムの感度は高く, 特異度は低いのですが, これは偽陽性が多いことを意味します. 一方, 穿刺生検は反対で, 感度は低く特異度は高いので, 偽陰性が多くなります.</p>
<p>こうした調査ではいくつかの推定対象があります. 疫学調査では, 集団内でマラリアのようなある種の感染症に罹っている患者数に関心があるでしょう. 検査法を開発する研究では, 新しい検査法の診断正答率に関心があるでしょう. 健康管理従事者が行なう検査では, ある患者の病状に関心があるでしょう.</p>
<h4 id="データ符号化">データ符号化</h4>
<p>データに符号をつける（評価や注釈も同様）仕事を与えられることはよくあります. 例えば, 論文誌や助成金の審査員は提出物を評価しますし, 政治学の研究では, キャンペーンコマーシャルが攻撃的広告か否かを符号化したりするでしょう. 自然言語処理では, ツイートが全体的な感情に照らしてポジティブかネガティブか注釈をつけるでしょうし, X線画像を見る歯科医は患者が虫歯かどうかを分類します. このような場合, データに符号をつける人は診断検査の役を演じており, すべての場合に共通するような推定対象があります. すなわち, データにつけた符号の正答率とバイアス, 符合をつけたアイテムの真のカテゴリー, データ中のアイテムのさまざまなカテゴリーはそれぞれいくつあるか, といったことです.</p>
<h4 id="ノイズのあるカテゴリー測定モデル">ノイズのあるカテゴリー測定モデル</h4>
<p>この小節では, カテゴリー評価だけに絞って, 離散パラメータを周辺化消去してStanでモデリングすることを考えます.</p>
<p>Dawid and Skene (1979)は, データ符号化にノイズのある測定モデルを導入して, 医師による記録から患者の履歴についてわかることを符号化するという疫学的状況に応用しています. 同じモデルは診断法についても使用できます.</p>
<h5 id="データ-1">データ</h5>
<p>データは, <span class="math inline"><em>J</em></span>評価者（診断検査）, <span class="math inline"><em>I</em></span>アイテム（患者）, <span class="math inline"><em>K</em></span>カテゴリー（状態）の注釈からなり, <span class="math inline"><em>y</em><sub><em>i</em>, <em>j</em></sub> ∈ 1 : <em>K</em></span>は, アイテム<span class="math inline"><em>i</em></span>について評価者<span class="math inline"><em>j</em></span>がつけた評価です. ある状態についての診断検査の状況では, 評価者は診断法であり, 多くは<span class="math inline"><em>K</em> = 2</span>, つまり疾患があるかないかの信号の値になります. <sup>5</sup></p>
<p>全評価者が全アイテムを1度だけ評価するとは限らないという状況にDawid and Skeneのモデルを拡張するのは比較的素直にできます.</p>
<p><sup>5</sup>診断法では順序尺度となることも多くあります. 腫瘍学的診断での癌の段階や, 歯科診断での虫歯の深刻度といったものです. Dawid and Skeneのモデルはそのままでも使えますし, 順序ロジスティック回帰として, 潜在連続値の評価とカットポイントを使い, 順序をつけて評価するように自然に一般化してもよいでしょう.</p>
<h4 id="モデルパラメータ">モデルパラメータ</h4>
<p>このモデルは3つのパラメータを持ちます. 最初のひとつは離散的です.</p>
<ul>
<li><span class="math inline"><em>z</em><sub><em>i</em></sub></span>: 1:<span class="math inline"><em>K</em></span>の値をとり, アイテム<span class="math inline"><em>i</em></span>の真のカテゴリーを示します.</li>
<li><span class="math inline"><em>π</em></span>: 集団内での<span class="math inline"><em>K</em></span>カテゴリーの出現率を示す<span class="math inline"><em>K</em></span>次元単体</li>
<li><span class="math inline"><em>θ</em><sub><em>j</em>, <em>k</em></sub></span>: 注釈者<span class="math inline"><em>j</em></span>の反応を示す, 真のカテゴリー<span class="math inline"><em>k</em></span>のアイテムについての<span class="math inline"><em>K</em></span>次元単体</li>
</ul>
<h4 id="ノイズのある測定モデル">ノイズのある測定モデル</h4>
<p>あるアイテムの真のカテゴリーは, アイテムの出現率をもとに単純なカテゴリカル分布で生成されると仮定します.</p>
<p><br /><span class="math display"><em>z</em><sub><em>i</em></sub> ∼ Categorical(<em>π</em>)</span><br /></p>
<p>アイテム<span class="math inline"><em>i</em></span>についての評価者<span class="math inline"><em>j</em></span>の評価<span class="math inline"><em>y</em><sub><em>i</em>, <em>j</em></sub></span>は, カテゴリー<span class="math inline"><em>z</em><sub><em>i</em></sub></span>のアイテムに対する評価者<span class="math inline"><em>i</em></span>（<strong>訳注: <span class="math inline"><em>j</em></span>が正しい?</strong>）のカテゴリカルな応答としてモデル化します. <sup>6</sup></p>
<p><sup>6</sup>添字の<span class="math inline"><em>z</em>[<em>i</em>]</span>は, <span class="math inline"><em>z</em><sub><em>i</em></sub></span>を読みやすくしたものです.</p>
<p><br /><span class="math display"><em>y</em><sub><em>i</em>, <em>j</em></sub> ∼ Categorical(<em>θ</em><sub><em>j</em>, <em>π</em><sub><em>z</em>[<em>i</em>]</sub></sub>)</span><br /></p>
<h5 id="事前分布と階層モデリング">事前分布と階層モデリング</h5>
<p>Dawid and Skeneは<span class="math inline"><em>θ</em></span>と<span class="math inline"><em>π</em></span>の最尤推定値を提供しています. これにより, 各<span class="math inline"><em>z</em><sub><em>i</em></sub></span>の確率の推定値を生成できます.</p>
<p>Dawid and Skeneの最尤モデルをまねるため, パラメータ<span class="math inline"><em>θ</em><sub><em>j</em>, <em>k</em></sub></span>と<span class="math inline"><em>π</em></span>には<span class="math inline"><em>K</em></span>次元単体についての一様事前分布を与えます. 一般化してディリクレ事前分布にするのも簡単です.</p>
<p><br /><span class="math display"><em>π</em> ∼ Dirichlet(<em>α</em>)</span><br /></p>
<p>および</p>
<p><br /><span class="math display"><em>θ</em><sub><em>j</em>, <em>k</em></sub> ∼ Dirichlet(<em>β</em><sub><em>k</em></sub>)</span><br /></p>
<p><span class="math inline"><em>α</em></span>と<span class="math inline"><em>β</em></span>は固定された値を持つハイパーパラメータです. <span class="math inline"><em>θ</em><sub><em>j</em>, <em>k</em></sub></span>の事前分布は<span class="math inline"><em>k</em></span>によって変わることができるようにしないといけません. そうすると例えば, 偶然よりも良い注釈をする注釈者が, ランダムまたは反対に注釈する注釈者よりも高い事前確率を持てるように<span class="math inline"><em>β</em><sub><em>k</em>, <em>k</em></sub></span>を大きくできるようになります.</p>
<p><span class="math inline"><em>J</em></span>だけ評価者がいるので, <span class="math inline"><em>β</em></span>に階層事前分布をつけて, 評価者の正答率とバイアスの推定値を部分的に合算するようにモデルを拡張するのは自然なことでしょう.</p>
<h5 id="真のカテゴリーの周辺化消去">真のカテゴリーの周辺化消去</h5>
<p>真のカテゴリーのパラメータ<span class="math inline"><em>z</em></span>は離散的ですので, 同時事後分布から周辺化消去して, Stanでサンプリングあるいは最尤推定ができるようにします. 同時事後分布の因子は以下のようになります.</p>
<p><br /><span class="math display"><em>p</em>(<em>y</em>, <em>θ</em>, <em>π</em>) = <em>p</em>(<em>y</em> ∣ <em>θ</em>, <em>π</em>)<em>p</em>(<em>π</em>)<em>p</em>(<em>θ</em>)</span><br /></p>
<p>ここで, <span class="math inline"><em>p</em>(<em>y</em> ∣ <em>θ</em>, <em>π</em>)</span>は, 次式から<span class="math inline"><em>z</em></span>を周辺化消去して得られます.</p>
<p><br /><span class="math display">$$p(z, y \mid \theta,\pi) = \prod_{i=1}^{I}\left(\mathsf{Categorial}(z_{i} \mid \pi) \prod_{j=1}^{J}\mathsf{Categorical}(y_{i,j} \mid \theta_{j,z[i]})\right)$$</span><br /></p>
<p>この式はアイテムごとの形式にできます.</p>
<p><br /><span class="math display">$$p(y \mid \theta,\pi) = \prod_{i=1}^{I}\sum_{k=1}^{K}\left(\mathsf{Categorial}(z_{i} \mid \pi) \prod_{j=1}^{J}\mathsf{Categorical}(y_{i,j} \mid \theta_{j,z[i]})\right)$$</span><br /></p>
<p>欠測データモデルでは, 内側の積の部分で観測されたかどうかのラベルだけを使うようにすればよいでしょう.</p>
<p>Dawid and Skene (1979)は, まったく同じ式を彼らの式(2.7)で導出しています. これは, 彼らの期待値最大化(EM)アルゴリズムにおけるEステップに必要となるものです. Stanでは対数スケールでの周辺化確率関数が必要になります.</p>
<p><br /><span class="math display">$$\log p(y \mid \theta,\pi) = \sum_{i=1}^{I}\log\left(\sum_{k=1}^{K}\exp\left(\log\mathsf{Categorial}(z_{i} \mid \pi) + \sum_{j=1}^{J}\log\mathsf{Categorical}(y_{i,j} \mid \theta_{j,z[i]})\right)\right)$$</span><br /></p>
<p>この式はStanの組込み<code>log_sum_exp</code>関数を使ってそのままコーディングできます.</p>
<h4 id="stanでの実装">Stanでの実装</h4>
<p>Dawid and SkeneモデルのStanプログラムを図12.5に示します. NUTSを使うStanのモデルは, ばらばらの初期値から素早く収束し, よく混合されます. 離散パラメータをギブズサンプリングするように実装した同等のモデルではそうはいきません. 適切な弱情報事前分布として, <span class="math inline"><em>α</em><sub><em>k</em></sub> = 3</span>, <span class="math inline"><em>β</em><sub><em>k</em>, <em>k</em></sub> = 2.5<em>K</em></span>, <span class="math inline"><em>β</em><sub><em>k</em>, <em>k</em>′</sub> = 1</span>（<span class="math inline"><em>k</em> ≠ <em>k</em>′</span>のとき）としています. <span class="math inline"><em>α</em></span>と<span class="math inline"><em>β</em><sub><em>k</em></sub></span>を単位ベクトルにして, 最適化を適用すると, Dawid and Skene (1979)の期待値最大化(EM)アルゴリズムと同じ結果が得られるでしょう.</p>
<pre><code>data {
  int&lt;lower=2&gt; K;
  int&lt;lower=1&gt; I;
  int&lt;lower=1&gt; J;
  int&lt;lower=1,upper=K&gt; y[I,J];
  vector&lt;lower=0&gt;[K] alpha;
  vector&lt;lower=0&gt;[K] beta[K];
}
parameters {
  simplex[K] pi;
  simplex[K] theta[J,K];
}
transformed parameters {
  vector[K] log_q_z[I];
  for (i in 1:I) {
    log_q_z[i] &lt;- log(pi);
    for (j in 1:J)
      for (k in 1:K)
        log_q_z[i,k] &lt;- log_q_z[i,k]
                        + log(theta[j,k,y[i,j]]);
  }
}
model {
  pi ~ dirichlet(alpha);
  for (j in 1:J)
    for (k in 1:K)
      theta[j,k] ~ dirichlet(beta[k]);
  for (i in 1:I)
    increment_log_prob(log_sum_exp(log_q_z[i]));
}</code></pre>
<p>図12.5: Dawid and Skene (1979)の評価（あるいは診断精度）モデルのStanプログラム. このモデルは, 離散パラメータ<span class="math inline"><em>z</em></span>を周辺化消去し, 非正規化条件付き確率<span class="math inline">log<em>q</em>(<em>z</em><sub><em>i</em></sub> = <em>k</em> ∣ <em>θ</em>, <em>π</em>)</span>を<code>log_q_z[i,k]</code>に格納しています.</p>
<h5 id="真のカテゴリーの推定">真のカテゴリーの推定</h5>
<p><code>log_q_z[i]</code>は変換パラメータ(transformed parameter)として定義されます. これは, <span class="math inline"><em>p</em>(<em>z</em><sub><em>i</em></sub> ∣ <em>θ</em>, <em>π</em>)</span>の対数（正規化されていません）をコード化したものです. 繰り返しごとに, その繰り返しでの<span class="math inline"><em>θ</em></span>と<span class="math inline"><em>π</em></span>との値を条件とした値が得られます. softmax関数を<code>log_q_z[i]</code>に適用することで, 事後の<span class="math inline"><em>z</em><sub><em>i</em></sub></span>の確率質量関数に相当する単体が得られます. これを各繰り返しについて平均することで, それぞれの<span class="math inline"><em>z</em><sub><em>i</em></sub></span>についての事後確率分布が得られます.</p>
<h2 id="まばらなデータ構造と不ぞろいなデータ構造">13. まばらなデータ構造と不ぞろいなデータ構造</h2>
<p>Stanは, まばらなデータ構造も不ぞろいなデータ構造も直接はサポートしていません. しかし, プログラミングの努力をいくらか払うことで両方とも扱うことができます. 35章では, 特別な目的のための疎行列と密ベクトルとの積を紹介しており, 適用可能なところではこれを使うことができますが, この章ではもっと一般的なデータ構造を対象とします.</p>
<h3 id="まばらなデータ構造">13.1 まばらなデータ構造</h3>
<p>まばらなデータ構造のコーディングは難しくなく, 行列状のデータ構造をデータベース状のデータ構造に変換するのと同じです. 例えば, 6.10節で議論したIRTモデルのまばらなデータを考えます. <span class="math inline"><em>J</em></span>人の生徒と<span class="math inline"><em>K</em></span>問の問題があり, すべての生徒がすべての問題を解くとすると, データは, <span class="math inline"><em>J</em> × <em>K</em></span>の解答の配列として宣言するのが実用的です.</p>
<pre><code>data {
  int&lt;lower=1&gt; J;
  int&lt;lower=1&gt; K;
  int&lt;lower=0,upper=1&gt; y[J,K];
  ...
model {
  for (j in 1:J)
    for (k in 1:K)
      y[j,k] ~ bernoulli_logit(delta[k] * (alpha[j] - beta[k]));
...</code></pre>
<p>すべての生徒がすべての問題に答えているわけではないなら, この密な配列のコーディングはもはや動作しません. Stanは未定義の値をサポートしていないからです. 図13.1は, <span class="math inline"><em>J</em> = 3</span>で<span class="math inline"><em>K</em> = 4</span>の場合の例で, 欠測の応答はRと同じくNAとしています. StanにはRのNA値のサポートはありませんので, このデータ構造を直接使うことはできません. かわりに, データベースのような「長い形式」に変換する必要があります. そのためには, 値とは別に, <span class="math inline"><em>j</em></span>および<span class="math inline"><em>k</em></span>のインデックスを示す列を持たせます. 例えば, <span class="math inline"><em>j</em><em>j</em></span>と<span class="math inline"><em>k</em><em>k</em></span>をインデックスのために使うと（Gelman and Hill, 2007による）, データ構造は, 図13.1の右側の例のようにコーディングできます. この例では, <span class="math inline"><em>y</em><sub>1, 1</sub> = 0</span>, <span class="math inline"><em>y</em><sub>1, 2</sub> = 1</span>などとなっており, 最後は<span class="math inline"><em>y</em><sub>3, 2</sub> = 1</span>です. ここにない項目はすべて未定義です.</p>
<p><br /><span class="math display">$$y = \left[\begin{array}{cccc} 0 &amp; 1 &amp; \mathrm{NA} &amp; 1 \\ 0 &amp; \mathrm{NA} &amp; \mathrm{NA} &amp; 1 \\ \mathrm{NA} &amp; 0 &amp; \mathrm{NA} &amp; \mathrm{NA} \end{array}\right] \quad \begin{array}{ll|l}jj &amp; kk &amp; y \\ \hline 1 &amp; 1 &amp; 0 \\ 1 &amp; 2 &amp; 1 \\ 1 &amp; 4 &amp; 1 \\ 2 &amp; 1 &amp; 0 \\ 2 &amp; 4 &amp; 1 \\ 3 &amp; 2 &amp; 0 \end{array}$$</span><br /></p>
<p>図13.1: まばらな配列をStanでコーディングした例. 左側は, R由来のNA記法を使った疎行列<span class="math inline"><em>y</em></span>の定義です（これはStanではサポートされていません）. 右側は, 同じ疎行列<span class="math inline"><em>y</em></span>をデータベース状にエンコーディングしたもので, これはStanで直接扱うことができます. 最初の2列, <span class="math inline"><em>j</em><em>j</em></span>と<span class="math inline"><em>k</em><em>k</em></span>はインデックスを示し, 最後の列<span class="math inline"><em>y</em></span>がその値を示します. 例えば, 右側のデータベース状のデータ構造の5行目は<span class="math inline"><em>y</em><sub>2, 4</sub> = 1</span>を示します.</p>
<p>定義されている<span class="math inline"><em>y</em></span>の数を<span class="math inline"><em>N</em></span>とすると, ここでは<span class="math inline"><em>N</em> = 6</span>です. データとモデルは以下のように定式化できます.</p>
<pre><code>data { ...
  int&lt;lower=1&gt; N;
  int&lt;lower=1,upper=J&gt; jj[N];
  int&lt;lower=1,upper=K&gt; kk[N];
  int&lt;lower=0,upper=1&gt; y[N];
  ...
model {
  for (n in 1:N)
    y[n] ~ bernoulli_logit(delta[kk[n]]
                           * (alpha[jj[n]] - beta[kk[n]]));
...</code></pre>
<p>欠測値がない場合には, この2種のモデルの定式化は完全に同じ対数事後密度を生成します.</p>
<h3 id="不ぞろいなデータ構造">13.2. 不ぞろいなデータ構造</h3>
<p>不ぞろいな配列とは, 長方形になっておらず, 項目によってサイズが違うような配列のことです. この種の構造は, 配列ごとに観測回数が違うときに出てきます.</p>
<p>不ぞろいなデータ構造を扱う一般的な方法は, 前の節で議論したように, 完全なデータベース状のデータ構造に移行することです. より簡潔に, 線形の配列に何らかのインデックスをつける方法もあります.</p>
<p>例えば, 3群があって, それぞれの観測回数が異なるようなデータ構造を考えます.</p>
<p>とても単純な, 切片が変化するモデルを仮定します. ベクトル化した記法を使うと, 尤度は以下のようになります（訳注: 原文では積（<span class="math inline">∏</span>）になっていますが, 対数にしているので和（<span class="math inline">∑</span>）のはずです）.</p>
<p><br /><span class="math display">$$\sum_{n=1}^{3}\log\mathsf{Normal}(y_{n}\mid\mu_{n},\sigma)$$</span><br /></p>
<p>これを直接Stanでエンコードする方法はありません.</p>
<p>まばらなデータの例のように, 完全なデータベース型の構造を使うことができるでしょうが, これは非効率です. 不要なインデックスがスペースを無駄にしますし, ベクトルによる密度の演算ができません. このデータをコーディングするもっと良い方法は, 単一のリストに値を持たせ, 各部分配列のサイズを示すデータ構造を別に用意するというものです. 図13.2の右側にこれを示しています. このコーディングでは, 値のための配列を1つだけ, それから各列のサイズのためにもう1つ別の配列を使っています.</p>
<p><br /><span class="math display">$$\begin{minipage}[c]{0.35\textwidth} $y_1 = \left[1.3 \ \ 2.4\ \ 0.9\right]$ \\ $y_2 = \left[-1.8\ \ -0.1 \right]$ \\ $y_3 = \left[12.9\ \ 18.7\ \ 42.9\ \ 4.7\right]$ \end{minipage} \begin{minipage}[c]{0.60\textwidth} $z = [1.3\ \ 2.4\ \ 0.9\ \ -1.8\ \ -0.1\ \ 12.9\ \ 18.7\ \ 42.9\ \ 4.7]$ \\ $s  =  \{3\ \ 2\ \ 4 \}$ \end{minipage}$$</span><br /></p>
<p>図13.2: 不ぞろいな配列をStanでコーディングした例. 左側は, サイズの異なる3行（<span class="math inline"><em>y</em><sub>1</sub></span>はサイズ3, <span class="math inline"><em>y</em><sub>2</sub></span>はサイズ2, <span class="math inline"><em>y</em><sub>3</sub></span>はサイズ4）からなる不ぞろいなデータ構造<span class="math inline"><em>y</em></span>の定義です. 右側は, Stanでこのデータをコーディングする方法の例です. 単一のベクトル<span class="math inline"><em>y</em></span>を使ってすべての値を保持し, 整数値の別の配列<span class="math inline"><em>s</em></span>に, 群ごとの行のサイズを持たせています. この例では<span class="math inline"><em>y</em><sub>1</sub> = <em>z</em><sub>1 : 3</sub></span>, <span class="math inline"><em>y</em><sub>2</sub> = <em>z</em><sub>4 : 5</sub></span>, <span class="math inline"><em>y</em><sub>3</sub> = <em>z</em><sub>6 : 9</sub></span>です.</p>
<p>するとこのモデルは, スライシング演算を使って, 以下のようにコーディングできます.</p>
<pre><code>data {
  int&lt;lower=0&gt; N;  // 観測回数
  int&lt;lower=0&gt; K;  // 群の数
  vector[N] y;     // 観測値
  int s[K];        // 群のサイズ
  ...
model {
  int pos;
  pos &lt;- 1;
  for (k in 1:K) {
    segment(y, pos, s[k]) ~ normal(mu[k], sigma);
    pos &lt;- pos + s[k];
  }</code></pre>
<p>このコーディングでは十分なベクトル化が可能で, これは<code>segment()</code>によるベクトルのスライシング演算によって生じるコピーのコストに見合うものです.</p>
<h2 id="クラスタリングモデル">14. クラスタリングモデル</h2>
<p>データを教師なしでグループにまとめる方法は, 総じてクラスタリングと呼ばれます. 本章では, 広く用いられている統計的なクラスタリングモデルの内2つ, ソフト<span class="math inline"><em>K</em></span>-means法と潜在ディリクレ配分法(LDA)に関して, Stanでの実装について説明します. 本章にはさらに, 教師ありの場合のクラスタリングの一種とみなせるナイーブベイズ分類法が含まれます. 通常これらのモデルは, クラスタの割り当てに離散パラメータを用いて表現されます. しかしながら, Stanでは離散パラメータを周辺化消去することで, これらのモデルを混合モデルのように実装することができます(10章も参照してみてください).</p>
<h3 id="ソフトk-means法">14.1. ソフト<span class="math inline"><em>K</em></span>-Means法</h3>
<p><span class="math inline"><em>K</em></span>-meansクラスタリングとは, <span class="math inline"><em>D</em></span>次元のベクトルで表わされるデータをクラスタリングする方法です. 具体的に言うと, 分類される項目が<span class="math inline"><em>N</em></span>個存在し, その各々はベクトル<span class="math inline"><em>y</em><sub><em>n</em></sub> ∈ ℝ<sup><em>D</em></sup></span>で表わされます. <span class="math inline"><em>K</em></span>-means法の「ソフト」版では, クラスタの割り当てが確率的になります.</p>
<h4 id="幾何的なハードk-meansクラスタリング">幾何的なハード<span class="math inline"><em>K</em></span>-Meansクラスタリング</h4>
<p><span class="math inline"><em>K</em></span>-meansクラスタリングは以下のアルゴリズムの様に, 通常幾何的な観点で記述されます(クラスタ数<span class="math inline"><em>K</em></span>とデータベクトル<span class="math inline"><em>y</em></span>は入力として仮定します).</p>
<ol type="1">
<li><span class="math inline">1 : <em>N</em></span>の各<span class="math inline"><em>n</em></span>に対し, ベクトル<span class="math inline"><em>y</em><sub><em>n</em></sub></span>を<span class="math inline">1 : <em>K</em></span>のとあるクラスタにランダムに割り当てます</li>
<li>以下の処理を繰り返します
<ol type="a">
<li><span class="math inline">1 : <em>K</em></span>の各クラスタ<span class="math inline"><em>k</em></span>に対し, そのクラスタに割り当てられたベクトルを平均してクラスタの重心<span class="math inline"><em>μ</em><sub><em>k</em></sub></span>を計算します</li>
<li><span class="math inline">1 : <em>N</em></span>の各<span class="math inline"><em>n</em></span>に対し, <span class="math inline"><em>y</em><sub><em>n</em></sub></span>から<span class="math inline"><em>μ</em><sub><em>k</em></sub></span>への(ユークリッド)距離が最小となるクラスタ<span class="math inline"><em>k</em></span>に, <span class="math inline"><em>y</em><sub><em>n</em></sub></span>を再度割り当てます</li>
<li>どのベクトルもクラスタが変わらなくなったら, そのクラスタ割り当てを返します</li>
</ol></li>
</ol>
<p>このアルゴリズムは, 停止することが保証されています.</p>
<h4 id="ソフトk-meansクラスタリング">ソフト<span class="math inline"><em>K</em></span>-Meansクラスタリング</h4>
<p>ソフト<span class="math inline"><em>K</em></span>-meansクラスタリングでは, クラスタの割り当ては複数のクラスタに渡る確率分布に基づき扱われます. ユークリッド距離と共分散が固定された多変量正規モデルの間の関係から, ソフト<span class="math inline"><em>K</em></span>-means法は, 多変量正規分布の混合モデルとして(Stanのコードも)表現することができます.</p>
<p>この全生成モデルでは, <span class="math inline">1 : <em>N</em></span>の各データ点<span class="math inline"><em>n</em></span>に, 次のような対称で一様な確率でクラスタ<span class="math inline"><em>z</em><sub><em>n</em></sub> ∈ 1 : <em>K</em></span>が割り当てられます.</p>
<p><br /><span class="math display"><em>z</em><sub><em>n</em></sub> ∼ Categorical(<strong>1</strong>/<em>K</em>), </span><br /></p>
<p>ここで<span class="math inline"><strong>1</strong></span>は<span class="math inline"><em>K</em></span>次元の単位ベクトルですので, <span class="math inline"><strong>1</strong>/<em>K</em></span>は対称な<span class="math inline"><em>K</em></span>単体となります. このようにして, このモデルではクラスタが割り当てられると, そこからおのずと各データ点が抽出されてゆくことが仮定されます. 「ソフト」さは, どのクラスタがデータ点を生成するかという不確実性のみから発生します.</p>
<p>データ点自体は, 割り当てられたクラスタ<span class="math inline"><em>z</em><sub><em>n</em></sub></span>から決定されるパラメータを持つ, 多変量正規分布より生成されます.</p>
<p><br /><span class="math display"><em>y</em><sub><em>n</em></sub> ∼ Normal(<em>μ</em><sub><em>z</em>[<em>n</em>]</sub>, Σ<sub><em>z</em>[<em>n</em>]</sub>)</span><br /></p>
<p>本節での実装例では, 共分散行列には全クラスタ<span class="math inline"><em>k</em></span>に共通して固定された単位行列を仮定します.</p>
<p><br /><span class="math display">Σ<sub><em>k</em></sub> = diag_matrix(<strong>1</strong>), </span><br /></p>
<p>こうすると, 比例定数を除き, 多変量の対数正規分布を次のように直接実装することができます.</p>
<p><br /><span class="math display">$$ \mathsf{Normal} \left(y_n \mid \mu_k, \text{diag\_matrix}(\mathbf{1})\right) \propto \exp \left(-\frac{1}{2}\sum_{d=1}^{D} (\mu_{k, d}-y_{n, d})^2 \right). $$</span><br /></p>
<p><span class="math inline"><em>K</em></span>-means法に対する空間的な観点では, 上式のかっこの内側の項が, まさにクラスタ平均<span class="math inline"><em>μ</em><sub><em>k</em></sub></span>からデータ点<span class="math inline"><em>y</em><sub><em>n</em></sub></span>までのユークリッド距離(を半分にして符号を反転した値)になっていることに注意して下さい.</p>
<h4 id="ソフトk-means法のstanでの実装">ソフト<span class="math inline"><em>K</em></span>-Means法のStanでの実装</h4>
<p><span class="math inline"><em>K</em></span>-meansクラスタリングを実装した, 次のStanのプログラムを考えましょう. <a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></p>
<pre><code>data {
  int&lt;lower=0&gt; N;  // データ点の数
  int&lt;lower=1&gt; D;  // 次元数
  int&lt;lower=1&gt; K;  // クラスタ数
  vector[D] y[N];  // 観測値
}
transformed data {
  real&lt;upper=0&gt; neg_log_K;
  neg_log_K &lt;- -log(K);
}
parameters {
  vector[D] mu[K]; // クラスタの平均
}
transformed parameters {
  real&lt;upper=0&gt; soft_z[N,K]; // 規格化されていないクラスタ割当確率の対数
  for (n in 1:N)
    for (k in 1:K)
      soft_z[n,k] &lt;- neg_log_K
                     - 0.5 * dot_self(mu[k] - y[n]);
}
model {
  // 事前分布
  for (k in 1:K)
    mu[k] ~ normal(0,1);

  // 尤度
  for (n in 1:N)
    increment_log_prob(log_sum_exp(soft_z[n]));
}</code></pre>
<p>重心のパラメータに関する事前分布は, 独立な標準正規分布としています. この事前分布は別の分布に変更してもよく, 問題全般のスケールと位置に適合するような階層モデルにしてもよいでしょう.</p>
<p>パラメータは<code>mu</code>のみであり, <code>mu[k]</code>はクラスタ<span class="math inline"><em>k</em></span>の重心となります. 変換パラメータ<code>soft_z[n]</code>には, 規格化されていないクラスタ割当確率の対数が含まれます. ベクトル<code>soft_z[n]</code>は, Stanの外部か, もしくはモデルの<code>generated quantities</code>ブロックの中で, <code>softmax</code>関数を用いて規格化された単体に戻すことができます(34.11節もご参照ください).</p>
<h4 id="ソフトk-means法の一般化">ソフト<span class="math inline"><em>K</em></span>-Means法の一般化</h4>
<p>共分散行列が単位行列の多変量正規分布は, ユークリッド距離(すなわち<span class="math inline"><em>L</em><sub>2</sub></span>距離)に比例する対数確率密度を生成します. ここで分布を変えると, 関連する幾何的な距離も別なものに変わります. 例えば, 正規分布を二重指数(ラプラス)分布に変えると, <span class="math inline"><em>L</em><sub>1</sub></span>距離(すなわちマンハッタン距離もしくはタクシー距離)に基づくクラスタリングモデルが生成されます.</p>
<p><span class="math inline"><em>K</em></span>-means法を多変量正規分布の視点でとらえる限り, 共分散行列を単位行列からクラスタに共通する別の行列に変更しても, 結局その共分散行列の逆行列で変換された空間で定義されるユークリッド距離で動作しているのと同じことになります.</p>
<p>なお空間との大域的な類似性はありませんが, 共分散行列がクラスタ毎に異なるソフト<span class="math inline"><em>K</em></span>-means法も一般的です. この様な場合には, 共分散行列に対して階層的な事前分布が用いられる事があります.</p>
<h3 id="クラスタリングにおけるベイズ推定のむずかしさ">14.2. クラスタリングにおけるベイズ推定のむずかしさ</h3>
<p>クラスタリングモデルのフルベイズ推定はほぼ実行不可能であり, これはパラメータの識別可能性の欠如と事後分布の極度な多峰性の2つの問題によるものです. 20.2節には, ラベルスイッチングに起因する識別不可能性について追加で議論が行われています.</p>
<h4 id="識別不可能性">識別不可能性</h4>
<p>クラスタの割り当ては, そもそも識別することができません. これは, クラスタの平均ベクトル<code>mu</code>を入れ替えたとしても, 尤度の同じモデルが導れるためです. 例えば, <code>mu</code>の最初の2つのインデックスと各<code>soft_z[n]</code>の最初の2つのインデックスを入れ替えても, 尤度は(事後分布も)同じになります. (訳注: かっこの中は原文priorですが, posteriorの誤記と捉えました)</p>
<p>このような識別可能性の欠如は, 複数のマルコフ連鎖の間でクラスタのパラメータが比較できないことを意味しています. ソフト<span class="math inline"><em>K</em></span>-means法におけるパラメータは一つですが, このパラメータが識別されないため, 実際収束をモニタする際に問題が生じます. 単一の連鎖の中であっても, 連鎖が長すぎたりデータがきれいに分離されないような場合にインデックスの入れ替えが伴うと, クラスタの識別に失敗する可能性すらあります.</p>
<h4 id="多峰性">多峰性</h4>
<p>クラスタリングモデルのもう一つの問題は, 事後分布が非常に多峰的になる点にあります. ある種の多峰性は識別不可能であり, インデックスの入れ替えを招きます. しかしながらたとえインデックスの問題がなかったとしても, 事後分布は非常に多峰的になります.</p>
<p>多峰性が高い場合, ベイズ推定は失敗します. これは, 事後分布のあらゆる峰を適切な比率で訪れる方法がなく, 従って事後予測の推定に含まれる積分を評価する方法もなくなるためです.</p>
<p>これら2つの問題の観点からクラスタリングモデルのあてはめに関してよく聞くアドバイスは, 初期値をたくさん変えて試してみる事であったり, 全体的に最も高い確率をもつサンプルを選択する事であったりします. なお, 期待値最大化法や変分ベイズ法のような最適化に基づく点推定量を用いるのもポピュラーであり, サンプリングに基づくアプローチよりずっと効率的な場合があります.</p>
<h3 id="ナイーブベイズ分類法およびクラスタリング法">14.3. ナイーブベイズ分類法およびクラスタリング法</h3>
<p>ナイーブベイズ法は一種の混合モデルであり, 観測される項目のラベルにより, 分類もしくはクラスタリング(または両者のミックス)に使用することができます. <a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a></p>
<p>混合多項分布モデルは「ナイーブベイズ」ともよばれます. これはこのモデルが, カテゴリを決める多項分布が独立しているという仮定が明らかに成り立たないような分類問題にも適用されてしまうことが多いためです.</p>
<p>ナイーブベイズによる分類とクラスタリングは, 多項分布で表せるような構造(以降ではこれを, 多項的な構造と呼びます)をもつあらゆるデータに適用できます. 典型的な例は自然言語テキストの分類とクラスタリングであり, これはこの後例として使用します.</p>
<p>この場合の観測データは一連の<span class="math inline"><em>M</em></span>本の文書から構成され, その文書は<span class="math inline"><em>V</em></span>種の異なる語彙から抽出される単語の多重集合(bags of words)からなっています. 文書<span class="math inline"><em>m</em></span>の単語数は<span class="math inline"><em>N</em><sub><em>m</em></sub></span>であり, 単語には<span class="math inline"><em>w</em><sub><em>m</em>, 1</sub>, …, <em>w</em><sub><em>m</em>, <em>N</em>[<em>m</em>]</sub> ∈ 1 : <em>V</em></span>というインデックスが振られています. 文書中の単語のインデックスには順番がありますが, このモデルはその順番を考慮しないため, 人間の自然言語のモデルとしては明らかに欠点があります. なお, トピック(もしくはカテゴリ)の数は<span class="math inline"><em>K</em></span>に固定されています.</p>
<p>混合多項分布モデルでは, 各文書<span class="math inline"><em>m</em> ∈ 1 : <em>M</em></span>に対して, カテゴリカル分布に従い単一のカテゴリ<span class="math inline"><em>z</em><sub><em>m</em></sub> ∈ 1 : <em>K</em></span>が生成されます.</p>
<p><br /><span class="math display"><em>z</em><sub><em>m</em></sub> ∼ Categorical(<em>θ</em>).</span><br /></p>
<p><span class="math inline"><em>K</em></span>単体のパラメータ<span class="math inline"><em>θ</em></span>は, データにおける各カテゴリの出現頻度を表しています.</p>
<p>続いて文書のカテゴリに基づき, 各文書の単語が, その文書中の他の単語や他の文書中の単語と条件付き独立になるように生成されます. つまり, 文書<span class="math inline"><em>m</em></span>の単語<span class="math inline"><em>n</em></span>は次のように生成されます.</p>
<p><br /><span class="math display"><em>w</em><sub><em>m</em>, <em>n</em></sub> ∼ Categorical(<em>ϕ</em><sub><em>z</em>[<em>m</em>]</sub>).</span><br /></p>
<p>ここでパラメータ<span class="math inline"><em>ϕ</em><sub><em>z</em>[<em>m</em>]</sub></span>は<span class="math inline"><em>V</em></span>単体であり, これはカテゴリ<span class="math inline"><em>z</em><sub><em>m</em></sub></span>の文書に関する語彙における各単語の確率を表しています.</p>
<p>パラメータの<span class="math inline"><em>θ</em></span>と<span class="math inline"><em>ϕ</em></span>の事前分布には, 通常対称ディリクレ分布が用いられます. なお出現頻度<span class="math inline"><em>θ</em></span>は, 各カテゴリ<span class="math inline"><em>k</em> ∈ 1 : <em>K</em></span>が等確率となるように固定される場合もあります.</p>
<h4 id="不ぞろいなragged配列のコード化">不ぞろいな(Ragged)配列のコード化</h4>
<p>前節におけるナイーブベイズモデルの規定では, 単語<span class="math inline"><em>w</em></span>の表記に不ぞろいな配列を使用しました. Stanは不ぞろいな配列をサポートしていないため, このモデルのコード化には別のやり方(全単語のリストから定まる語彙種別のインデックスを, 各単語に付与する方法)が用いられます. このデータは次のように構成されており, 一列目は出現した単語を全て並べて全文書で通して付けたインデックス<code>n</code>, 二列目は単語<code>n</code>に対する語彙種別のインデックス, 三列目は単語<code>n</code>が含まれる文書のインデックス, となります.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: right;"><code>n</code></th>
<th style="text-align: center;"><code>w[n]</code></th>
<th style="text-align: center;"><code>doc[n]</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: center;"><span class="math inline"><em>w</em><sub>1, 1</sub></span></td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: right;">2</td>
<td style="text-align: center;"><span class="math inline"><em>w</em><sub>1, 2</sub></span></td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><span class="math inline">⋮</span></td>
<td style="text-align: center;"><span class="math inline">⋮</span></td>
<td style="text-align: center;"><span class="math inline">⋮</span></td>
</tr>
<tr class="even">
<td style="text-align: right;"><span class="math inline"><em>N</em><sub>1</sub></span></td>
<td style="text-align: center;"><span class="math inline"><em>w</em><sub>1, <em>N</em>[1]</sub></span></td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><span class="math inline"><em>N</em><sub>1</sub> + 1</span></td>
<td style="text-align: center;"><span class="math inline"><em>w</em><sub>2, 1</sub></span></td>
<td style="text-align: center;">2</td>
</tr>
<tr class="even">
<td style="text-align: right;"><span class="math inline"><em>N</em><sub>1</sub> + 2</span></td>
<td style="text-align: center;"><span class="math inline"><em>w</em><sub>2, 2</sub></span></td>
<td style="text-align: center;">2</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><span class="math inline">⋮</span></td>
<td style="text-align: center;"><span class="math inline">⋮</span></td>
<td style="text-align: center;"><span class="math inline">⋮</span></td>
</tr>
<tr class="even">
<td style="text-align: right;"><span class="math inline"><em>N</em><sub>1</sub> + <em>N</em><sub>2</sub></span></td>
<td style="text-align: center;"><span class="math inline"><em>w</em><sub>2, <em>N</em>[2]</sub></span></td>
<td style="text-align: center;">2</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><span class="math inline"><em>N</em><sub>1</sub> + <em>N</em><sub>2</sub> + 1</span></td>
<td style="text-align: center;"><span class="math inline"><em>w</em><sub>3, 1</sub></span></td>
<td style="text-align: center;">3</td>
</tr>
<tr class="even">
<td style="text-align: right;"><span class="math inline">⋮</span></td>
<td style="text-align: center;"><span class="math inline">⋮</span></td>
<td style="text-align: center;"><span class="math inline">⋮</span></td>
</tr>
<tr class="odd">
<td style="text-align: right;"><code>N</code><span class="math inline">${}=\sum_{m=1}^{M} N_m$</span></td>
<td style="text-align: center;"><span class="math inline"><em>w</em><sub><em>M</em>, <em>N</em>[<em>M</em>]</sub></span></td>
<td style="text-align: center;"><span class="math inline"><em>M</em></span></td>
</tr>
</tbody>
</table>
<p>関連するプログラムの変数は, 全文書における総単語数<code>N</code>, 単語の配列<code>w</code>, 文書を特定する配列<code>doc</code>となります.</p>
<h4 id="カテゴリのラベルが付与された訓練データが存在する場合の推定">カテゴリのラベルが付与された訓練データが存在する場合の推定</h4>
<p>カテゴリが既知の文書が訓練データとして与えられた場合, ナイーブベイズモデルで単体上のパラメータを推定するStanのコードは, 次のように書けます. <a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a></p>
<pre><code>data {
  // 訓練データ
  int&lt;lower=1&gt; K;               // トピック数
  int&lt;lower=1&gt; V;               // 語彙数
  int&lt;lower=0&gt; M;               // 文書数
  int&lt;lower=0&gt; N;               // 総単語数
  int&lt;lower=1,upper=K&gt; z[M];    // 文書mにおけるトピック
  int&lt;lower=1,upper=V&gt; w[N];    // 単語n
  int&lt;lower=1,upper=M&gt; doc[N];  // 単語nに対する文書ID
  // 超パラメータ
  vector&lt;lower=0&gt;[K] alpha;     // トピックの事前分布向け
  vector&lt;lower=0&gt;[V] beta;      // 単語の事前分布向け
}
parameters {
  simplex[K] theta;   // トピックの出現頻度
  simplex[V] phi[K];  // トピックkにおける単語の分布
}
model {
  theta ~ dirichlet(alpha);
  for (k in 1:K)
    phi[k] ~ dirichlet(beta);
  for (m in 1:M)
    z[m] ~ categorical(theta);
  for (n in 1:N)
    w[n] ~ categorical(phi[z[doc[n]]]);
}</code></pre>
<p>トピックの識別子<span class="math inline"><em>z</em><sub><em>m</em></sub></span>がデータとして宣言され, カテゴリの割り当てが尤度関数の一部として含まれていることに注意してください.</p>
<h4 id="カテゴリのラベルが付与された訓練データが存在しない場合の推定">カテゴリのラベルが付与された訓練データが存在しない場合の推定</h4>
<p>ナイーブベイズモデルは, 多項的な構造のデータを, 教師なしのやり方で<span class="math inline"><em>K</em></span>個(一定)のカテゴリにクラスタリングするためにも使用できます. データの宣言には前節のモデルと同じ変数が含まれますが, トピックのラベル<code>z</code>は除かれます. <code>z</code>は離散的ですので, モデルの計算から積分消去しておく必要があります. この処理は, ナイーブベイズモデルでも他の混合モデルでも同様です. パラメータは事前分布を除いて先ほどと同じですが, 今度の尤度は, 文書あたりで見ると, 次のようにカテゴリ(トピック)について周辺化した確率として計算されます.</p>
<p><br /><span class="math display">$$ \begin{aligned}&amp; \log p \left(w_{m,1}, \ldots, w_{m,N_{m}} \mid \theta, \phi \right) \\&amp; \quad = \log \textstyle \sum_{k=1}^{K} \left(\mathsf{Categorical}(k \mid \theta) \times \prod_{n=1}^{N_{m}} \mathsf{Categorical}(w_{m,n} \mid \phi_{k})\right) \\&amp; \quad= \log \textstyle \sum_{k=1}^{K} \exp \left(\log \mathsf{Categorical}(k \mid \theta) + \sum_{n=1}^{N_{m}} \log \mathsf{Categorical}(w_{m,n} \mid \phi_{k}) \right).\end{aligned} $$</span><br /></p>
<p>最終行で<code>log_sum_exp</code>関数が用いられているのは数値計算を安定させるためであり, 結果は対数スケールで返されることになります.</p>
<pre><code>model {
  real gamma[M,K];
  theta ~ dirichlet(alpha);
  for (k in 1:K)
    phi[k] ~ dirichlet(beta);
  for (m in 1:M)
    for (k in 1:K)
      gamma[m,k] &lt;- categorical_log(k,theta);
  for (n in 1:N)
    for (k in 1:K)
      gamma[doc[n],k] &lt;- gamma[doc[n],k]
                         + categorical_log(w[n],phi[k]);
  for (m in 1:M)
    increment_log_prob(log_sum_exp(gamma[m]));
}</code></pre>
<p>ローカル変数の<code>gamma[m,k]</code>は次の値を表しています.</p>
<p><br /><span class="math display">$$ \gamma_{m,k} = \log \mathsf{Categorical}(k \mid \theta) + \sum_{n=1}^{N_{m}} \log \mathsf{Categorical}(w_{m,n} \mid \phi_{k}). $$</span><br /></p>
<p><span class="math inline"><em>γ</em></span>が与えられた下で, 文書<span class="math inline"><em>m</em></span>にカテゴリ<span class="math inline"><em>k</em></span>が割り当てられる事後確率は次のようになります.</p>
<p><br /><span class="math display">$$ \text{Pr}\left[z_{m}=k \mid w, \alpha, \beta \right] = \exp \left(\gamma_{m,k} - \log \sum_{k=1}^{K} \exp(\gamma_{m,k}) \right). $$</span><br /></p>
<p>変数<code>gamma</code>を<code>transformed parameters</code>ブロックで宣言して定義すると, Stanはそのサンプル値を保存します. そうすれば, 規格化された事後確率も生成量として定義できます.</p>
<h4 id="ナイーブベイズのフルベイズ推定">ナイーブベイズのフルベイズ推定</h4>
<p>Stanでは, ナイーブベイズモデルの事後予測分布をフルベイズ推定する実装も可能です. この場合, ラベルが付与されたデータと付与されていないデータが組み合わされます. 推定対象には, モデルのパラメータと, ラベルが付与されていないデータのカテゴリに関する事後分布の, 両方が含まれることになります. このモデルは本質的に, 未知のカテゴリラベルがMCAR (missing completely at random; どの値が欠測するかが完全にランダムであるような欠測)だと仮定したモデルになっています. 欠測データの補完に関する更なる情報については, (Gelman et al., 2013; Gelman and Hill, 2007) をご参照ください. またこのモデルは, ラベルの付与されていないデータがパラメータの推定に寄与するため, 半教師あり学習の例にもなっています.</p>
<p>Stanでフルベイズ推定を実行するモデルを規定するためには, ラベルが付与されたデータのモデルとラベルが付与されていないデータのモデルを組み合わせます. 後者の文書の集まりはデータとして宣言されますがカテゴリラベルは付与されず, 関連して新しい変数<code>M2</code>, <code>N2</code>, <code>w2</code>, <code>doc2</code>が設定されます. 超パラメータのみならずカテゴリ数や単語数は両者で共通となっており, 宣言も一度のみです. 同様にパラメータも1種類しか存在しません. 続くモデル部には, 事前分布に関する共通する記述, ラベルが付与されたデータに関する記述, およびラベルが付与されていないデータに関する記述, が含まれます.</p>
<h4 id="モデル更新を伴わない予測">モデル更新を伴わない予測</h4>
<p>フルベイズ推定の代替手段の一つに, まずラベルの付与されたデータを用いてモデルを推定し, 続いてその結果をラベルの付与されていないデータに適用する(ラベルの付与されていないデータに基いてパラメータの推定値は更新しない), という方法があります. ラベルの付与されていない文書に関する<code>gamma</code>の定義を<code>generated quantities</code>ブロックに移行すると, この様な振る舞いを行う実装が可能になります. この場合その変数はもはや対数確率には寄与しなくなるので, モデルパラメータの推定においても, もはや同時には影響しないことになります.</p>
<h3 id="潜在ディリクレ配分法">14.4. 潜在ディリクレ配分法</h3>
<p>潜在ディリクレ配分法(Latent Dirichlet Allocation, 略してLDA)は, ナイーブベイズを一般化した, 混合メンバーシップの多項クラスタリングモデル(Blei et al., 2003)です. LDAの説明で一般的なトピックと文書という用語を用いれば, まず各文書が複数のトピックを持つ様にモデル化され, その混成比に基づいて, あるトピックから各単語が抽出されることになります.</p>
<h4 id="ldaモデル">LDAモデル</h4>
<p>このモデルの基本型では, まず各文書が, 固定の超パラメータに基づいて独立に生成されると仮定されます. そして文書<span class="math inline"><em>m</em></span>に対する最初のステップとして, <span class="math inline"><em>K</em></span>個のトピックに渡るトピック分布である単体<span class="math inline"><em>θ</em><sub><em>m</em></sub></span>が抽出されます.</p>
<p><br /><span class="math display"><em>θ</em><sub><em>m</em></sub> ∼ Dirichlet(<em>α</em>).</span><br /></p>
<p>ここで事前分布の超パラメータ<span class="math inline"><em>α</em></span>は固定されており, これは<span class="math inline"><em>K</em></span>個の正の値のベクトルとなります. 文書中の単語は, この分布<span class="math inline"><em>θ</em><sub><em>m</em></sub></span>が与えられた条件の下で, 各々独立に生成されることになります. 具体的にはまず最初に, 文書に固有のトピック分布に基づき, 単語に対するトピック<span class="math inline"><em>z</em><sub><em>m</em>, <em>n</em></sub> ∈ 1 : <em>K</em></span>が抽出されます.</p>
<p><br /><span class="math display"><em>z</em><sub><em>m</em>, <em>n</em></sub> ∼ Categorical(<em>θ</em><sub><em>m</em></sub>).</span><br /></p>
<p>そして, トピック<span class="math inline"><em>z</em><sub><em>m</em>, <em>n</em></sub></span>における単語の分布に従って, 最終的に単語<span class="math inline"><em>w</em><sub><em>m</em>, <em>n</em></sub></span>が抽出されます.</p>
<p><br /><span class="math display"><em>w</em><sub><em>m</em>, <em>n</em></sub> ∼ Categorical(<em>ϕ</em><sub><em>z</em>[<em>m</em>, <em>n</em>]</sub>).</span><br /></p>
<p>なお, トピック<span class="math inline"><em>k</em></span>における単語の分布<span class="math inline"><em>ϕ</em><sub><em>k</em></sub></span>にも, ディリクレ事前分布が設定されます.</p>
<p><br /><span class="math display"><em>ϕ</em><sub><em>k</em></sub> ∼ Dirichlet(<em>β</em>)</span><br /></p>
<p>ここで<span class="math inline"><em>β</em></span>は固定であり, <span class="math inline"><em>V</em></span>個の正の値のベクトルとなります.</p>
<h4 id="離散パラメータの積分消去">離散パラメータの積分消去</h4>
<p>Stanでは離散パラメータのサンプリングが(まだ)サポートされていませんが, 他の混合モデルの場合と同じように, 離散パラメータを積分消去して連続パラメータのみの周辺分布を計算できます. トピックと単語の周辺事後分布は, 次のようになります.</p>
<p><br /><span class="math display">$$ \begin{aligned}p(\theta, \phi \mid w, \alpha, \beta) &amp; \propto p(\theta \mid \alpha) \times p(\phi \mid \beta) \times p(w \mid \theta, \phi) \\&amp; = \prod_{m=1}^{M} p(\theta_{m} \mid \alpha) \times \prod_{k=1}^{K} p(\phi_k \mid \beta) \times \prod_{m=1}^{M} \prod_{n=1}^{M[n]} p(w_{m,n} \mid \theta_m, \phi).\end{aligned} $$</span><br /></p>
<p>上式における最後の積の内側の項は単語の確率であり, トピックの割り当てを積分消去することで次のように定義されます.</p>
<p><br /><span class="math display">$$ \begin{aligned}p(w_{m,n} \mid \theta_{m}, \phi) &amp; = \sum_{z=1}^{K} p(z, w_{m,n} \mid \theta_m,\phi). \\&amp; = \sum_{z=1}^{K} p(z \mid \theta_{m}) \times p(w_{m,n} \mid \phi_z).\end{aligned} $$</span><br /></p>
<p>この分布を先ほどの式に代入してスケールを対数に変換すると, Stanで直接実装が可能な式が得られます.</p>
<p><br /><span class="math display">$$ \begin{aligned}&amp; \log p(\theta, \phi \mid w, \alpha, \beta) \\&amp; \quad = \textstyle \sum_{m=1}^{M} \log \mathsf{Dirichlet}(\theta_m \mid \alpha) + \textstyle \sum_{k=1}^{K} \log \mathsf{Dirichlet}(\phi_k \mid \beta) \\&amp; \qquad + \textstyle \sum_{m=1}^{M} \textstyle \sum_{n=1}^{N[m]} \log \left(\textstyle \sum_{\mathrm{z}=1}^{K} \mathsf{Categorical}(z \mid \theta_m) \times \mathsf{Categorical}(w_{m,n} \mid \phi_z) \right)\end{aligned} $$</span><br /></p>
<h4 id="ldaの実装">LDAの実装</h4>
<p>前節で導出した周辺分布に対し, 本節でデータ構造をあてはめて記述すると, StanにおけるLDAのプログラムは次のようになります.</p>
<pre><code>data {
  int&lt;lower=2&gt; K;               // トピック数
  int&lt;lower=2&gt; V;               // 語彙数
  int&lt;lower=1&gt; M;               // 文書数
  int&lt;lower=1&gt; N;               // 総単語数
  int&lt;lower=1,upper=V&gt; w[N];    // 単語n
  int&lt;lower=1,upper=M&gt; doc[N];  // 単語nに対する文書ID
  vector&lt;lower=0&gt;[K] alpha;     // トピックの事前分布向け
  vector&lt;lower=0&gt;[V] beta;      // 単語の事前分布向け
}
parameters {
  simplex[K] theta[M];   // 文書mにおけるトピックの分布
  simplex[V] phi[K];     // トピックkにおける単語の分布
}
model {
  for (m in 1:M)
    theta[m] ~ dirichlet(alpha);  // 事前分布
  for (k in 1:K)
    phi[k] ~ dirichlet(beta);     // 事前分布
  for (n in 1:N) {
    real gamma[K];
    for (k in 1:K)
      gamma[k] &lt;- log(theta[doc[n],k]) + log(phi[k,w[n]]);
    increment_log_prob(log_sum_exp(gamma));  // 尤度
  }
}</code></pre>
<p>他の混合モデルの場合と同じように, log-sum-of-exponents関数は数値演算を安定化させるために使用されています.</p>
<h4 id="相関を持つトピックモデル">相関を持つトピックモデル</h4>
<p>文書のトピック分布における相関を考慮するために, (Blei and Lafferty, 2007)ではLDAの変種が提案されており, そこでは文書毎のトピックに関する事前分布が, ディリクレ分布から多変量のロジスティック正規分布に置き換えられています.</p>
<p>この論文の筆者らは, 超パラメータが固定の場合を扱っています. なお論文では共分散の<span class="math inline"><em>L</em><sub>1</sub></span>正則化推定値が用いられていますが, これは事前分布に二重指数分布を設定した場合の最大事後確率推定値と等価です. Stanでは最大事後確率推定は(まだ)サポートされていませんので, 多変量のロジスティック正規分布の平均と共分散は, データとして規定する必要があります.</p>
<h5 id="超パラメータが固定の場合の相関を持つトピックモデル">超パラメータが固定の場合の相関を持つトピックモデル</h5>
<p>前節におけるStanのモデルは, 相関を持つトピックモデルを実装するように修正することができます. このためには, トピックの事前分布に関するデータの宣言において, ディリクレ分布における<code>alpha</code>を多変量のロジスティック正規分布における平均と共分散で置き換えます.</p>
<pre><code>data {
  ... dataブロックの他の記載は, alphaを除いて前と同じ ...
  vector[K] mu;          // トピック分布の事前分布に関する平均
  cov_matrix[K] Sigma;   // トピック分布の事前分布に関する共分散
}</code></pre>
<p>そして, ディリクレ分布から単体となるパラメータ<code>theta</code>を抽出する代わりに, まず多変量正規分布からパラメータ <code>eta</code>を抽出し, 次いで<code>softmax</code>を用いてその値を単体に変換します.</p>
<pre><code>parameters {
  simplex[V] phi[K];   // トピックkにおける単語の分布
  vector[K] eta[M];    // 文書mにおけるトピックの分布
}
transformed parameters {
  simplex[K] theta[M];
  for (m in 1:M)
    theta[m] &lt;- softmax(eta[m]);
}
model {
  for (m in 1:M)
    eta[m] ~ multi_normal(mu,Sigma);
  ... modelブロックの他の記載は, thetaに関する事前分布を除いて前と同じ ...
}</code></pre>
<h5 id="相関を持つトピックモデルのフルベイズ推定">相関を持つトピックモデルのフルベイズ推定</h5>
<p>先ほどの平均と共分散に事前分布を加えれば, 相関を持つトピックモデルに対するフルベイズ推定がStanでもサポートされることになります. このためには, トピック分布に関する平均<code>mu</code>と共分散<code>Sigma</code>の宣言を<code>data</code>ブロックから<code>parameters</code>ブロックに移行し, それらの事前分布をモデル部で与える必要があります. 共分散行列<code>Sigma</code>の事前分布は次のようにコード化すると, 比較的効率が良く解釈も容易でしょう.</p>
<pre><code>... dataブロックは前と同じ(ただしalphaはない) ...
parameters {
  vector[K] mu;              // トピック分布の事前分布に関する平均
  corr_matrix[K] Omega;      // 相関行列
  vector&lt;lower=0&gt;[K] sigma;  // スケール
  vector[K] eta[M];          // 文書mにおけるトピック分布のロジット
  simplex[V] phi[K];         // トピックkにおける単語の分布
}
transformed parameters {
  ... etaに関する記載は前と同じ ...
  cov_matrix[K] Sigma;       // 共分散行列
  for (m in 1:K)
    Sigma[m,m] &lt;- sigma[m] * sigma[m] * Omega[m,m];
  for (m in 1:(K-1)) {
    for (n in (m+1):K) {
      Sigma[m,n] &lt;- sigma[m] * sigma[n] * Omega[m,n];
      Sigma[n,m] &lt;- Sigma[m,n];
    }
  }
}
model {
  mu ~ normal(0,5);       // ベクトル化に対応, 散漫な事前分布
  Omega ~ lkj_corr(2.0);  // 単位相関行列への正則化
  sigma ~ cauchy(0,5);    // 制約により半コーシー分布
  ... 単語のサンプリングに関する記載は前と同じ ...
}</code></pre>
<p>形状パラメータが<span class="math inline"><em>α</em> &gt; 0</span>の<code>LkjCorr</code>分布は, 相関行列(すなわち, 対角成分が1の対称な正定値行列)と同じ台を持ちます. この密度は次のように定義されています.</p>
<p><br /><span class="math display">LkjCorr(Ω ∣ <em>α</em>) ∝ det(Ω)<sup><em>α</em> − 1</sup></span><br /></p>
<p><span class="math inline"><em>α</em> = 2</span>というスケールでは, この分布は単位相関行列寄りの弱情報事前分布となります. したがって, 多変量のロジスティック正規分布の共分散行列<span class="math inline">Σ</span>にこの事前分布を適用すると, 対角成分への集中度がいくらか増す一方で, そのスケールは<code>sigma</code>の事前分布から決定されるため, これらの複合効果がもたらされることになります.</p>
<h2 id="方向-回転-超球面">16. 方向, 回転, 超球面</h2>
<p>方向統計は, 方向という制約のあるデータあるいはパラメータ, もしくはその両方を扱います. 方向を組み合わせると球面となります. 球面のジオメトリは, ユークリッド空間のジオメトリにすんなりとマッピングすることができません. 球面は, 1周まわって元に戻るということも可能だからです. そういうわけで, 平面の紙の上に地図を作るのに, 地球上の互いに近い地点ならどこでも, 地図上でも互いに近くにあるというようにはできません. この根本的な問題は2次元では容易に可視化できます. 円周に沿って移動していると, 初めの場所に戻ってしまいます. 言い換えると, 0度と360度は（0と<span class="math inline">2<em>π</em></span>ラジアンでも同じことですが）同じ点を指しますし, 359度と2度との間の距離は137度と140度との距離と同じです.</p>
<p>Stanには単位ベクトルのデータ型があり, 方向統計に対応しています. 単位ベクトルの値は, 超球面（2次元では円周, 3次元では球面）上の点を決定します.</p>
<h3 id="単位ベクトル">16.1. 単位ベクトル</h3>
<p>ベクトル<span class="math inline"><em>x</em> ∈ ℝ<sup><em>K</em></sup></span>の長さは次式で与えられます.</p>
<p><br /><span class="math display">$$ \|x\| = \sqrt{x^{\top} x} = \sqrt{x_1^2 + x_2^2 + \cdots + x_K^2} $$</span><br /></p>
<p>単位ベクトルは, 単位長（すなわち長さ1）を持つベクトルと定義されます.</p>
<p>以下のように変数宣言します.</p>
<pre><code>unit_vector[K] x;</code></pre>
<p><code>x</code>の値は, 単位長を持つサイズ<code>K</code>のベクトルに制約されます. 58.7節に, Stanのアルゴリズムで使っている, 単位ベクトルに制約されているパラメータを制約のない空間に変換する方法の詳細があります.</p>
<h3 id="円-球面-超球面">16.2. 円, 球面, 超球面</h3>
<p><span class="math inline"><em>n</em></span>次元球面を<span class="math inline"><em>S</em><sup><em>n</em></sup></span>と書くことにしますが, これは<span class="math inline">(<em>n</em> + 1)</span>次元の単位ベクトルの組で定義されます.</p>
<p><br /><span class="math display"><em>S</em><sup><em>n</em></sup> = {<em>x</em> ∈ ℝ<sup><em>n</em> + 1</sup> : ∥<em>x</em>∥ = 1}</span><br /></p>
<p><span class="math inline"><em>S</em><sup><em>n</em></sup></span>は, <span class="math inline">(<em>n</em> + 1)</span>次元の点で作られるにもかかわらず, <span class="math inline"><em>n</em></span>次元の多様体でしかありません. 例えば, <span class="math inline"><em>S</em><sup>2</sup></span>は<span class="math inline">ℝ<sup>3</sup></span>の点の組で定義されますが, そうした点は緯度・経度により一意に記述することができます. 幾何学的には, <span class="math inline">ℝ<sup>3</sup></span>の<span class="math inline"><em>S</em><sup>2</sup></span>で定義される表面は, 局所的には平面つまり<span class="math inline">ℝ<sup>2</sup></span>に似ています. しかし, <span class="math inline"><em>S</em><sup>2</sup></span>の全体的な形は, コンパクト（すなわち, 点間に最大距離が存在する）というところで平面とは違っています. 地球を「直線」（すなわち測地線）に沿って回り始めると, 最終的には初めの場所に戻ってしまいます. 球面(<span class="math inline"><em>S</em><sup>2</sup></span>)の測地線が「大円」と呼ばれるのはこれが理由です. そしてまた, 円周あるいは球面統計には何らかの上手な表現が必要となる理由でもあります.</p>
<p><span class="math inline"><em>S</em><sup><em>n</em> − 1</sup></span>が局所的には<span class="math inline">ℝ<sup><em>n</em> − 1</sup></span>に似ているとしても, 両者をすんなりとマッピングする方法はありません. 例えば, 緯度と経度は, （自然単位では<span class="math inline">2<em>π</em></span>で一回りする）モジュラ基底に基づいていますので, すんなりマッピングできません.</p>
<p>上下限のある区間<span class="math inline">(<em>a</em>, <em>b</em>)</span>のように, あらゆる2点間の距離には上限があることから, 幾何学的意味で球面はコンパクトです.</p>
<h3 id="制約のないパラメータへの変換">16.3. 制約のないパラメータへの変換</h3>
<p>Stanは, Marsaglia (1972)の方法で補助変数を使い, <span class="math inline">ℝ<sup><em>K</em> + 1</sup></span>における任意の点を<span class="math inline"><em>S</em><sup><em>K</em></sup></span>の点へと（逆）変換します. 点<span class="math inline"><em>y</em> ∈ ℝ<sup><em>K</em></sup></span>は, 点<span class="math inline"><em>x</em> ∈ <em>S</em><sup><em>K</em> − 1</sup></span>に以下のように変換されます.</p>
<p><br /><span class="math display">$$ x = \frac{y}{\sqrt{y^{\top} y}} $$</span><br /></p>
<p>このマッピングの問題は, 多対1であることです. 原点から出ているベクトル上の点はすべて, 球面上の同じ点に投影されます. Marsaglia (1972)は, このマッピングの補助変数の解釈を行い, <span class="math inline"><em>x</em></span>が超球面上で一様に分布することを示しました. 詳細は58.7節を参照してください.</p>
<h5 id="警告-ゼロでは未定義">警告: ゼロでは未定義</h5>
<p>上の<span class="math inline">ℝ<sup><em>n</em></sup></span>から<span class="math inline"><em>S</em><sup><em>n</em></sup></span>へのマッピングはゼロでは定義されません. サンプリング中はこの点の結果は測度ゼロであり, そのため無視されるでしょう. 同様に単位ベクトルのパラメータはゼロで初期化できません. 単純な回避法は, ゼロに近い非常に狭い区間で初期化することです. これはStanのインターフェイスすべてに組み込まれているオプションです.</p>
<h3 id="単位ベクトルと回転">16.4. 単位ベクトルと回転</h3>
<p>単位ベクトルはそのまま角度に対応していますから, 回転にも対応します. これは2次元で見るとわかりやすいでしょう. 円周上のある点は, コンパス上の方角を決定します（等価ですが, 角度<span class="math inline"><em>θ</em></span>を決定します）. ある角度<span class="math inline"><em>θ</em></span>が与えられたとすると, 左からの掛け算で角度<span class="math inline"><em>θ</em></span>だけ回転させるような行列を定義できます. （2次元の）角度<span class="math inline"><em>θ</em></span>については, <span class="math inline">2 × 2</span>回転行列は以下のように定義されます.</p>
<p><br /><span class="math display">$$ R_{\theta} = \left[\begin{array}{cc} \cos\theta &amp; -\sin\theta \\ \sin\theta &amp; \cos\theta \end{array}\right] $$</span><br /></p>
<p>2次元ベクトル<span class="math inline"><em>x</em></span>が与えられたとすると, <span class="math inline"><em>R</em><sub><em>θ</em></sub><em>x</em></span>は<span class="math inline"><em>x</em></span>を（原点のまわりで）<span class="math inline"><em>θ</em></span>度だけ回転させます.</p>
<h3 id="日と年の円周表現">16.5. 日と年の円周表現</h3>
<p>24時間時計は, 深夜0時から正午を経て, 1回転して再び0時に戻るという, 1日の時間の進み方を自然に表しています. したがって, 円周上を24時間に分割した点は, 1日の時刻を自然に表したものです. 同様に, 1年は季節をめぐって, また元の季節に戻ります.</p>
<p>人為的なものには慣習による時間効果がよく見られます. こうしたものも, 休日や週末を示すアドホックな予測変数によって, あるいは, 夏時間を自然のスケールに戻すようなデータの正規化によって, 直接モデリングできます.</p>
<h2 id="再パラメータ化と変数変換">17. 再パラメータ化と変数変換</h2>
<p>BUGSと同様に, Stanでは直接的な再パラメータ化がサポートされています. また, Stanでは変数変換もサポートされており, 変数変換のヤコビアンの対数を対数確率の合計に直接加えることで実現します.</p>
<h3 id="理論的かつ実践的な背景">17.1. 理論的かつ実践的な背景</h3>
<p>ベイズの事後分布は専門的には確率「測度」であり, それはパラメータ化によって不変な, 抽象数学のものです. <a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a>他方, Stanのモデリング言語は確率「密度」を定義します. 確率密度は<span class="math inline">ℛ<sup><em>N</em></sup></span>から<span class="math inline">ℛ<sup> + </sup></span>に写す関数であり, ユニークではなく, パラメータ化に依存します. 実際には, これは与えられたモデルをStanで表現する方法は複数あり得ることを意味し, 表現が異なれば計算のパフォーマンスも異なることを意味します.</p>
<p>パラメータ化とベイズモデリングの関係を議論したGelman (2004)で指摘されたように, パラメータ化を変えるとどのようにモデルが変わるかに関する示唆が得られる場合がしばしばあります. 私たちは特定の自然なクラスの事前分布を多用する傾向があります. それゆえ, 再パラメータ化の恩恵は, サンプリングをしたいと決めた分布に対する計算の助け「だけ」にはとどまらないのです. さらに, いったん再パラメータ化をして事前情報を加えると, モデル自体も典型的には変わりますし, しばしば有益な方向に変わります. <a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a></p>
<h3 id="再パラメータ化">17.2. 再パラメータ化</h3>
<p>再パラメータ化は素直に実装できます. 例えば, ベータ分布は2つの正のカウントを表すパラメータ<span class="math inline"><em>α</em>, <em>β</em> &gt; 0</span>によってパラメータ化されています. 次の例は, <code>vector</code>型のパラメータ<code>theta</code>を持つ階層的なStanのモデルを表しています. ここで, <code>theta</code>は独立同一なベータ分布から抽出されており, ベータ分布のパラメータは超事前分布から抽出されます.</p>
<pre><code>parameters {
  real&lt;lower = 0&gt; alpha;
  real&lt;lower = 0&gt; beta;
  ...
model {
  alpha ~ ...
  beta ~ ...
  for (n in 1:N)
    theta[n] ~ beta(alpha, beta);
  ...</code></pre>
<p>変換パラメータ（transformed parameter）を使って超事前分布を定めた方がしばしばより自然です. ベータ分布の場合, 明らかな再パラメータ化のやり方は平均パラメータ</p>
<p><br /><span class="math display"><em>ϕ</em> = <em>α</em>/(<em>α</em> + <em>β</em>)</span><br /></p>
<p>とカウントの合計を表すパラメータ</p>
<p><br /><span class="math display"><em>λ</em> = <em>α</em> + <em>β</em></span><br /></p>
<p>を使う方法です. (Gelman et al., 2013, Chapter 5)に従うと, 平均パラメータの事前分布に一様分布を設定し, カウントの合計を表すパラメータの事前分布に<span class="math inline"><em>p</em>(<em>λ</em>) ∝ <em>λ</em><sup> − 2.5</sup></span>のパレート分布を設定します.</p>
<pre><code>parameters {
  real&lt;lower=0,upper=1&gt; phi;
  real&lt;lower=0.1&gt; lambda;
  ...
  transformed parameters {
  real&lt;lower=0&gt; alpha;
  real&lt;lower=0&gt; beta;
  ...
  alpha = lambda * phi;
  beta = lambda * (1 - phi);
  ...
model {
  phi ~ beta(1, 1); // phiは一様分布に従う. この行は省略してもよい.
  lambda ~ pareto(0.1, 1.5);
  for (n in 1:N)
    theta[n] ~ beta(alpha, beta);
  ...</code></pre>
<p>新しいパラメータ<code>phi</code>と<code>lambda</code>は<code>parameters</code>ブロックで宣言され, ベータ分布のパラメータである<code>alpha</code>と<code>beta</code>は<code>transformed parameters</code>ブロックで宣言されて定義されます. もし, <code>alpha</code>と<code>beta</code>の値に興味がなければ, 代わりに<code>model</code>ブロックで局所変数として定義することもできます. 次のようになります.</p>
<pre><code>model {
  real alpha;
  real beta;
  alpha = lambda * phi;
  beta = lambda * (1 - phi);
  ...
  for (n in 1:N)
    theta[n] ~ beta(alpha, beta);
  ...
}</code></pre>
<p>ベクトル化すると, 次のようにもっとコンパクトで効率的に表現できます.</p>
<pre><code>model {
  theta ~ beta(lambda * phi, lambda * (1 - phi));
  ...
}</code></pre>
<p>もし, <code>alpha</code>と<code>beta</code>の値に興味があるならば, これらを<code>transformed parameters</code>ブロックで定義してから<code>model</code>ブロックで使うとよいでしょう.</p>
<h4 id="ヤコビアンは必要ない">ヤコビアンは必要ない</h4>
<p>分布を与えるのではなく, 変換パラメータ（transformed parameter）を使う場合は, 変換に対するヤコビアンの調整は不要です. 例えば, ベータ分布の例では<code>alpha</code>と<code>beta</code>は適切な事後分布を持ちます.</p>
<h3 id="変数変換">17.3. 変数変換</h3>
<p>パラメータの変換が確率分布によって特徴づけられるときに, 確率の調整が必要な「変数変換」となります. 標準的な教科書の例は対数正規分布です. 確率変数<span class="math inline"><em>y</em> &gt; 0</span>の分布が対数正規分布に従うとき, <span class="math inline"><em>y</em></span>の対数である<span class="math inline">log<em>y</em></span>は正規分布に従います. 分布が<span class="math inline">log<em>y</em></span>に割り当てられている点に注意です.</p>
<p>変数変換は変換によるゆがみを考慮に入れるため, 確率の調整が必要となります. この調整がうまくいくためには, 一変量の変数変換が台（support）において単調かつ至るところで微分可能でなければなりません.</p>
<p>一変量の変数変換では, 変換の微分の絶対値を使って確率をスケーリングする必要があります（一変量の変数変換のより正確な定義については58.1節を見てください）.</p>
<p>対数正規分布の場合, <span class="math inline"><em>y</em></span>の対数が平均<span class="math inline"><em>μ</em></span>・標準偏差<span class="math inline"><em>σ</em></span>の正規分布に従うとすると, <span class="math inline"><em>y</em></span>の分布は以下で与えられます.</p>
<p><br /><span class="math display">$$p(y) = \mathsf{Normal}(\log y \mid \mu , \sigma) \left| \frac{d}{dy}\log y \right| = \mathsf{Normal}(\log y \mid \mu , \sigma) \frac{1}{y}$$</span><br /></p>
<p>Stanはアンダーフローを防ぐため, 対数スケールで動作します. そのため, 以下の形で扱います.</p>
<p><br /><span class="math display">log<em>p</em>(<em>y</em>) = logNormal(log<em>y</em> ∣ <em>μ</em>, <em>σ</em>) − log<em>y</em></span><br /></p>
<p>Stanでは変数変換はサンプリング文において適用されます. 曲率を調整するために, 変換の微分の絶対値の対数が対数確率の合計に足しこまれます. Stanでは対数正規分布は次のように直接的に実装できます. <a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a></p>
<pre><code>parameters {
  real&lt;lower=0&gt; y;
  ...
model {
  log(y) ~ normal(mu, sigma);
  target += -log(y);
  ...</code></pre>
<p>パラメータの宣言時に適切な制約を課すことは毎度のことながら重要です. ここで<code>y</code>は正に制限されています.</p>
<p>次にように, 対数をとったあとの局所変数を定義すると, わずかにより効率的でしょう.</p>
<pre><code>model {
  real log_y;
  log_y = log(y);
  log_y ~ normal(mu, sigma);
  target += -log_y;
  ...</code></pre>
<p>もし<code>y</code>がパラメータではなくデータとして宣言されているならば, ヤコビアンによる調整は無視されます. なぜなら, データは定数であり, Stanでは定数を除いた対数確率だけが必要となるからです.</p>
<h4 id="変数変換-vs.-単純な変換パラメータ">変数変換 vs. 単純な変換パラメータ</h4>
<p>この節では変数変換と単純な変換パラメータの違いを説明します. 単純な変換パラメータを使う場合, パラメータをサンプリングし, そのあとでサンプリングした値を変換します. ところが変数変換の場合, パラメータを変換し, そのあとでサンプリングします. 後者だけがヤコビアンの調整を必要とします.</p>
<p>確率関数がサンプリング文で表現されているかどうかは関係ありません. 例えば, 以下のようなサンプリング文で表現しても</p>
<pre><code>log(y) ~ normal(mu, sigma);</code></pre>
<p>以下の対数確率を累積する（インクリメントする）文で表現しても, ヤコビアンの調整が必要です.</p>
<pre><code>target += normal_lpmf(log(y) | mu, sigma);</code></pre>
<h5 id="ガンマ分布と逆ガンマ分布">ガンマ分布と逆ガンマ分布</h5>
<p>対数正規分布と同じように, 逆ガンマ分布に従う変数の逆数はガンマ分布に従います. この節では二つのアプローチを比較します. はじめに単純な変換パラメータを使う場合を, 次に変数変換の場合を扱います.</p>
<p>単純な変換パラメータを使うアプローチで<code>y_inv</code>を逆ガンマ分布からサンプリングするには, 以下のようにコーディングします.</p>
<pre><code>parameters {
  real&lt;lower=0&gt; y;
}
transformed parameters {
  real&lt;lower=0&gt; y_inv;
  y_inv = 1 / y;
}
model {
  y ~ gamma(2,4);
}</code></pre>
<p>変数変換のアプローチで<code>y_inv</code>を逆ガンマ分布からサンプリングするには, 以下のようにコーディングします.</p>
<pre><code>parameters {
  real&lt;lower=0&gt; y_inv;
}
transformed parameters {
  real&lt;lower=0&gt; y;
  y = 1 / y_inv; // 変換
  target += -2 * log(y_inv); // 調整
}
model {
  y ~ gamma(2,4);
}</code></pre>
<p>ヤコビアンによる調整は変換の微分の絶対値の対数です. ここでは以下のようになります.</p>
<p><br /><span class="math display">$$\log \left| \frac{d}{du} \left( \frac{1}{u} \right) \right| = \log | -u^{-2} | = \log u^{-2} = -2 \log u$$</span><br /></p>
<h4 id="多変量の変数変換">多変量の変数変換</h4>
<p>多変量の変数変換の場合には, 変換のヤコビアンの対数を対数確率の合計に足しこまなければなりません（多変量の変換とヤコビアンのより正確な定義については58.1節を見てください）. Stanでは, ヤコビアンが密行列となる一般の場合には以下のようにコーディングできます.</p>
<pre><code>parameters {
  vector[K] u; // 多変量のパラメータ
  ...
transformed parameters {
  vector[K] v; // 変換パラメータ
  matrix[K, K] J; // 変換のヤコビ行列
  ... uの関数としてvを計算する ...
  ... J[m, n] = d.v[m] / d.u[n] を計算する ...
  target += log(fabs(determinant(J)));
  ...
model {
  v ~ ...;
  ...</code></pre>
<p>もちろん, ヤコビアンが解析的に分かっていれば, 行列式を求める関数を呼ぶよりも直接そのヤコビアンを適用した方がより効率的でしょう. 行列式を求める関数は効率的でないし, 数値的に安定でもありません.</p>
<p>多くの場合, ヤコビ行列は三角行列になるでしょう. その場合, 行列式の計算には対角成分だけが必要となります. 変換パラメータの<code>vector</code>のそれぞれの要素<code>v[k]</code>が, パラメータの<code>vector</code>の要素<code>u[1], ..., u[k]</code>にのみ依存する時, ヤコビ行列は三角行列となります. 三角行列の行列式は対角成分の積となるので, 上記のモデルの<code>transformed parameters</code>ブロックはシンプルにでき, 以下のように専用の変数を使うとより効率的にできます.</p>
<pre><code>transformed parameters {
  ...
  vector[K] J_diag; // ヤコビ行列の対角成分
  ...
  ... J_diag[k] = d.v[k] / d.u[k] を計算する ...
  target += sum(log(J_diag));
  ...</code></pre>
<h3 id="変化する境界をもつvector">17.4. 変化する境界をもつ<code>vector</code></h3>
<p>Stanではコンテナの型に対する制約は, 一つの下限と一つの上限しか宣言できません. しかし, ある<code>vector</code>型のパラメータの各要素の下限値が, 同じように<code>vector</code>型で与えられているとしましょう. すると, 要素ごとの変換とそのヤコビアン（これらは58章に記述があります）をStanで計算する必要があります.</p>
<p>例えば, 下限を表す<code>vector</code><span class="math inline"><em>L</em></span>を持つ, パラメータの<code>vector</code><span class="math inline"><em>α</em></span>を考えてみましょう. 以下のプログラムでは制約のないraw（生の）パラメータを宣言し, それからヤコビアンを考慮してrawパラメータを明示的に<span class="math inline"><em>α</em></span>に変換します.</p>
<pre><code>data {
  int N;
  vector[N] L; // 下限値
  ...
parameters {
  vector[N] alpha_raw;
  ...
transformed parameters {
  vector[N] alpha;
  alpha = L + exp(alpha_raw);
  ...
model {
  target += sum(alpha_raw); // ヤコビアンの対数
  ...</code></pre>
<p>調整項は, <span class="math inline"><em>α</em><sub>raw</sub></span>から<span class="math inline"><em>α</em> = <em>L</em> + exp(<em>α</em><sub>raw</sub>)</span>への変換に関するヤコビ行列の行列式の対数になります. この場合はヤコビ行列が対角行列になるのでシンプルになります（詳しくは58.2節をみてください）. ここで, <span class="math inline"><em>L</em></span>は<span class="math inline"><em>α</em><sub>raw</sub></span>に依存しないパラメータを含むことすらできます. もし境界が<span class="math inline"><em>α</em><sub>raw</sub></span>に依存するならば, 依存性を考慮に入れてヤコビアンを計算しなおす必要があります.</p>
<h2 id="自作の確率分布関数">18. 自作の確率分布関数</h2>
<p>自作の確率分布もStanの中で直接実装することができます. 必要なことは対数確率の合計を累積する（インクリメントする）ことだけです. 以降では2つの例を扱います.</p>
<h3 id="例">18.1.　例</h3>
<h4 id="三角分布">三角分布</h4>
<p>単純な例は三角分布です. その密度関数は二等辺三角形のような形をしており, 指定された境界において角をもち, 密度を積分すると1になるという制約から高さが決まります. もし, <span class="math inline"><em>α</em> ∈ ℝ</span>と<span class="math inline"><em>β</em> ∈ ℝ</span>が境界で, <span class="math inline"><em>α</em> &lt; <em>β</em></span>とすると, <span class="math inline"><em>y</em> ∈ (<em>α</em>, <em>β</em>)</span>は以下で定義される密度を持ちます.</p>
<p><br /><span class="math display">$$\mathsf{Triangle}(y \mid \alpha,\beta) = \frac{2}{\beta - \alpha}\left( 1 - \left| y - \frac{\alpha + \beta}{\beta - \alpha} \right| \right)$$</span><br /></p>
<p>もし, <span class="math inline"><em>α</em> =  − 1</span>, <span class="math inline"><em>β</em> = 1</span>, <span class="math inline"><em>y</em> ∈ ( − 1, 1)</span>ならば, この式は以下のように簡単になります.</p>
<p><br /><span class="math display">Triangle(<em>y</em> ∣  − 1, 1) = 1 − |<em>y</em>|</span><br /></p>
<p><span class="math inline">Triangle( − 1, 1)</span>からサンプリングするため, 以下のStanの実装を考えてみましょう. <a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a></p>
<pre><code>parameters {
  real&lt;lower=-1,upper=1&gt; y;
}
model {
  target += log1m(fabs(y));
}</code></pre>
<p>唯一のスカラーのパラメータである<code>y</code>が区間<code>(-1,1)</code>に入るように宣言されています. 対数確率の合計に, すべてのパラメータの同時対数確率（すなわち<span class="math inline">logTriangle( − 1, 1)</span>）を加えてインクリメントしています. この同時対数確率の値はStanでは<code>log1m(fabs(y))</code>とコーディングされています. <code>log1m(x)</code>関数は<code>log(1.0-x)</code>と同じ値ですが, 計算がより速く, より正確で, より安定です.</p>
<p><code>y</code>の宣言における型の制約<code>real&lt;lower=-1,upper=1&gt;</code>は, 正しくサンプリングが行われるために必須です. もし, プログラムから<code>y</code>の制約を取り除いたら, すなわち<code>y</code>が制約のない実数値をとるように宣言すると, プログラムはコンパイルされるけれども, 実行時においてサンプラーが<span class="math inline">( − 1, 1)</span>の外側を探索した時に算術例外を投げることでしょう.</p>
<p>以下のように<span class="math inline">( − 1, 1)</span>の外側の値に<code>log(0.0)</code>の対数確率（すなわち負の無限大）を定義することで, <span class="math inline">ℝ</span>全体の値に拡張した対数確率関数を考えてみましょう.</p>
<pre><code>target += log(fmax(0.0,1 - fabs(y)));</code></pre>
<p><code>y</code>に制約を課した元のプログラムと比べて, これは非効率で遅くて数値計算上不安定です. しかし, <code>y</code>についての制約を取り除いても, プログラムはコンパイルされ, 算術例外が発生することなしに実行されるでしょう. しかし, 適切にサンプリングはされないでしょう. <a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a></p>
<h4 id="指数分布">指数分布</h4>
<p>もし, 仮にStanに指数分布が用意されていないとすると, 指数分布は以下の代入文を用いて直接コーディングできます.</p>
<pre><code>target += log(lambda) - y * lambda;</code></pre>
<p>ここで<code>lambda</code>はスケールの逆数で, <code>y</code>はサンプリングされた確率変数を表します. このコーディングは任意の<code>lambda</code>と<code>y</code>についてうまくいきます. これらの両方もしくはどちらか一方がパラメータでもデータでもよいですし, ローカル変数でも大丈夫です.</p>
<p>前の段落の代入文は, 以下のサンプリング文によって生成されるC++コードと非常によく似たC++コードを生成します.</p>
<pre><code>y ~ exponential(lambda);</code></pre>
<p>注目すべき違いが二つあります. 一つ目は, サンプリング文は<code>lambda</code>が正で<code>y</code>が非負であることを確認するため入力をチェックします（どちらも非数でないかもチェックします）.</p>
<p>二つ目の違いは, もし<code>lambda</code>がパラメータ・変換パラメータ（transformed parameter）・<code>model</code>ブロックの局所変数のいずれでもなければ, サンプリング文は賢いので定数である<code>log(lambda)</code>の項を落とします. 結果は同じ事後分布になります. なぜなら, Stanは付加定数を除いた対数確率だけを必要とするからです. もし, <code>lambda</code>と<code>y</code>の両方とも定数ならば, サンプリング文は両方の項を落とします（しかし, 入力が妥当かどうかのチェックは変わらず行います）.</p>
<h2 id="ユーザー定義関数">19. ユーザー定義関数</h2>
<p>この章ではユーザーの視点から例を交えてユーザー定義関数を説明します. 厳密な仕様は28章を見てください. ユーザー定義関数を使うと, 計算をカプセル化して一つの名前をつけることができ, その名前を使ってどこでも呼び出すことができます. 同様に, 関数を使うと, 複雑な手続きをより理解しやすい構成要素に分解することができます. 適切な名前をつけた関数を使用したモジュール性の高いコードは, 大きな一枚岩のプログラムよりも理解しやすいです. たとえ一枚岩のプログラムにコメントを多くつけたとしてもです. <a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a></p>
<h3 id="基本的な関数">19.1. 基本的な関数</h3>
<p>ここではユーザー定義関数のStanプログラムの例を扱います. その関数は<code>generated quantities</code>ブロックで呼び出され, 2つのパラメータの相対差を計算します.</p>
<pre><code>functions {
  real relative_diff(real x, real y) {
    real abs_diff;
    real avg_scale;
    abs_diff = fabs(x - y);
    avg_scale = (fabs(x) + fabs(y)) / 2;
    return abs_diff / avg_scale;
  }
}
...
generated quantities {
  real rdiff;
  rdiff = relative_diff(alpha, beta);
}</code></pre>
<p>関数名は<code>relative_diff</code>とし, 2つの<code>real</code>型の値を引数にとり, 1つの<code>real</code>型の値を結果として返すように宣言しています. この関数はビルトイン関数を<code>generated quantities</code>ブロックで使うのとまったく同じように使われています.</p>
<h4 id="user-defined-functionsブロック"><code>user-defined functions</code>ブロック</h4>
<p>すべての関数は固有のブロックで定義されます. そのブロックには<code>functions</code>というラベルがつけられ, 他のどのブロックよりも前に現れなくてはなりません. ただし, <code>user-defined functions</code>ブロックはなくても構いません.</p>
<h4 id="関数の本体">関数の本体</h4>
<p>本体（波括弧<code>{ }</code>の間の部分）には, ローカル変数を含む通常のStanコードが入ります.</p>
<h4 id="return文"><code>return</code>文</h4>
<p><code>return</code>文は関数定義の本体でのみ使うことができます. 上の<code>relative_diff</code>の例では関数定義の最後の行にあります. <code>return</code>文は関数の中のどこに現れても構いません. しかし, <code>void</code>ではない値を返す関数は, 必ず<code>return</code>文で終わる必要があります. これがどのように実行されるかについての詳細は28.7節を見てください.</p>
<h4 id="reject文"><code>reject</code>文</h4>
<p>Stanの<code>reject</code>文は, プログラムの実行中に遭遇したエラーや問題のある値を報告するための仕組みを提供します. <code>reject</code>文では, 引用符のついた文字列またはStanの式を任意の数だけ引数にとることができます. この文は, 何らかの処理で妥当でない結果が出たことを検出するために, 典型的には条件文の中で使われます.</p>
<p>この文の使い方を説明するために, 19.1節のユーザー定義の相対差関数の例を修正して, 相対差が何らかの閾値よりも小さい場合に棄却するようにします.</p>
<pre><code>functions {
  real relative_diff(real x, real y, real min) {
    real abs_diff;
    real avg_scale;
    abs_diff = fabs(x - y);
    avg_scale = (fabs(x) + fabs(y)) / 2;
    if (abs_diff / avg_scale &lt; min)
      reject(&quot;relative_diff below &quot;, min);
    return abs_diff / avg_scale;
  }
}</code></pre>
<p>棄却の効果は関数が呼び出されたブロックに依存します. 詳しくは27.9節を見てください.</p>
<h4 id="関数のための型の宣言">関数のための型の宣言</h4>
<p>関数の引数と返値の型の宣言に, 変数のサイズは書きません. また, 値の制約も含みません. 図19.1に一覧にしましたので見てください.</p>
<figure>
<img src="data:image/png;base64,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" alt="型の宣言" /><figcaption>型の宣言</figcaption>
</figure>
<!--
| 関数:<br>次元なし |  ローカル変数:<br>制約なし | ローカル変数ではない変数:<br>制約あり |
| ---------------- | ------------ | -------- |
| `int` | `int` | `int<lower=L>`<br>`int<upper=U>`<br>`int<lower=L,upper=U>` |
| `real` | `real` | `real<lower=L>`<br>`real<upper=U>`<br>`real<lower=L,upper=U>` |
| `vector` | `vector[N]` | `vector<lower=L>[N]`<br>`vector<upper=U>[N]`<br>`vector<lower=L,upper=U>[N]`<br><br>`simplex[N]`<br>`ordered[N]`<br>`positive_ordered[N]`<br>`unit_vector[N]` |
| `row_vector` | `row_vector[M]` | `row_vector<lower=L>[M]`<br>`row_vector<upper=U>[M]`<br>`row_vector<lower=L,upper=U>[M]` |
| `matrix` | `matrix[M,N]` | `matrix<lower=L>[M, N]`<br>`matrix<upper=U>[M, N]`<br>`matrix<lower=L,upper=U>[M, N]`<br><br>`cov_matrix[K]`<br>`corr_matrix[K]`<br>`cholesky_factor_cov[K]`<br>`cholesky_factor_corr[K]` |
-->
<p>図19.1: 一番左の列は制約と次元のない基本の型です. これらは関数の返値の型や引数の型に使われます. 中央の列は次元つきの制約のない型です. これらはローカル変数として使われます. 一番右の列は左と中央の列に対応する制約のある型です. 右側のどの型の式でも, それに対応する左側の型に代入できます. 実行時にすべての変数について次元が一貫しているかチェックされます. そして, どんなサイズのコンテナも関数の引数に代入できます. 制約のある<code>matrix</code>型である<code>cov_matrix[K]</code>, <code>corr_matrix[K]</code>, <code>cholesky_factor_cov[K]</code>, <code>cholesky_factor_corr[K]</code>は<code>matrix[K, K]</code>という次元を持つ<code>matrix</code>に対してだけ代入できます. Stanではこれらの型の任意の配列も使うことができます. ただし, 関数の引数や返値の型や変数の宣言は少し変わります.</p>
<p>変数の型宣言と異なり, 関数の型宣言においては<code>matrix</code>型と<code>vector</code>型のサイズは宣言されません. ローカル変数の宣言と同じで, 関数の引数の型は制約をつけて宣言することはできません（下限や上限の制約も, 単体や相関行列のような構造を持つ制約もつけることはできません）.</p>
<p>例えば, 単体のパラメータ<code>theta</code>を使ってカテゴリカル分布のエントロピーを計算する関数は以下のようになります.</p>
<pre><code>real entropy(vector theta) {
  return sum(theta .* log(theta));
}</code></pre>
<p><code>theta</code>は単体でなければなりませんが, 型には単に<code>vector</code>が使われます. <a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a>関数の宣言において返値の型や引数の型に上下限や制約のある型を使うことは許されていません.</p>
<h4 id="関数の宣言における配列の型">関数の宣言における配列の型</h4>
<p>引数としての配列は独自の構文を持っており, その構文はこのマニュアルにおいても関数を区別するために使われています. 例えば, 2次元配列に作用して1次元配列を生成する関数は以下のように宣言されるでしょう.</p>
<pre><code>real[] baz(real[,] x);</code></pre>
<p>1次元配列（上のコードの返値）には<code>[ ]</code>という記法が使われており, 2次元配列には<code>[ , ]</code>という記法が使われています. 3次元配列には<code>[ , , ]</code>という記法が使われ, 4次元配列以降も同様です.</p>
<p>関数は<code>matrix</code>型と<code>vector</code>型を含む任意の型の配列をサポートしています. 他の型と同様に, 制約をつけることはできません.</p>
<h3 id="ステートメント文としての関数">19.2. ステートメント（文）としての関数</h3>
<p>場合によっては, 値を返さない関数にするのが理にかなっています. 例えば, 行列の下三角成分を表示する処理は以下のように定義されるでしょう.</p>
<pre><code>functions {
  void pretty_print_tri_lower(matrix x) {
    if (rows(x) == 0) {
      print(&quot;empty matrix&quot;);
      return;
    }
    print(&quot;rows=&quot;, rows(x), &quot; cols=&quot;, cols(x));
    for (m in 1:rows(x))
      for (n in 1:m)
        print(&quot;[&quot;, m, &quot;,&quot;, n, &quot;]=&quot;, x[m, n]);
  }
}</code></pre>
<p>特別な語である<code>void</code>が返値の型として使われています. <code>void</code>は型そのものではなく空の型で値がないこと, すなわち単に値が欠けていることを表しています. そのため, <code>void</code>の関数において, <code>return</code>文は引数をとることはできません. 上のコード例における<code>return</code>文のようになります.</p>
<p>適切な型の引数をとった<code>void</code>の関数は, それ自体が文として使われます. 例えば, 上で定義した<code>pretty-print</code>関数を<code>transformed parameters</code>ブロックで定義された分散共分散行列に適用すると以下になります.</p>
<pre><code>transformed parameters {
  cov_matrix[K] Sigma;
  ... Sigmaを設定するコード ...
  pretty_print_tri_lower(Sigma);
  ...</code></pre>
<h3 id="対数確率を累積する機能にアクセスする関数">19.3. 対数確率を累積する機能にアクセスする関数</h3>
<p><code>_lp</code>で名前が終わる関数の中では, サンプリング文と<code>target +=</code>文を使うことができます. 他の関数名では使うことはできません. このアクセスのため, <code>_lp</code>で名前が終わる関数は<code>transformed parameters</code>ブロックと<code>model</code>ブロックの中でしか使うことができません.</p>
<p>以下の関数の例では, 係数のベクトルの事前分布に標準正規分布を設定し, 位置（分布の中心）とスケールの事前分布も設定します. そして, 引数のベクトル<code>beta_raw</code>を中心<code>mu</code>とスケール<code>sigma</code>に従って平行移動してスケーリングして返します. 中心化に関するさらなる情報は22.2節を見てください.</p>
<pre><code>functions {
  vector center_lp(vector beta_raw, real mu, real sigma) {
    beta_raw ~ normal(0, 1);
    sigma ~ cauchy(0, 5);
    mu ~ cauchy(0, 2.5);
    return sigma * beta_raw + mu;
  }
  ...
}
parameters {
  vector[K] beta_raw;
  real mu_beta;
  real&lt;lower=0&gt; sigma_beta;
  ...

transformed parameters {
  vector[K] beta;
  ...
  beta = center_lp(beta_raw, mu_beta, sigma_beta);
  ...</code></pre>
<h3 id="乱数生成器として振る舞う関数">19.4. 乱数生成器として振る舞う関数</h3>
<p><code>_rng</code>で終わる名前をつけることで, （疑似）乱数生成器（pseudo random number generator, 略してPRNG）として振る舞うようにユーザー定義関数を宣言することができます. <code>_rng</code>で終わる名前をつけると, その関数の中ではすべてのPRNG関数を含むビルトイン関数にアクセスでき, <code>_rng</code>で終わるユーザー定義関数にもアクセスすることができます. <code>_rng</code>で名前が終わる関数だけが, その中でビルトインのPRNG関数にアクセスすることができます. そのため, <code>_rng</code>で名前が終わる関数は, 他のPRNG関数と同様に<code>generated quantities</code>ブロックの中でしか使うことができません.</p>
<p>例えば, 以下の関数は<span class="math inline">(<em>N</em> × <em>K</em>)</span>のデータの行列を生成します. その行列の1番目の列は切片として<code>1</code>で埋められ, 残りの要素は標準正規分布に従う疑似乱数発生器（PRNG）から抽出されます.</p>
<pre><code>matrix predictors_rng(int N, int K) {
  matrix[N, K] x;
  for (n in 1:N) {
    x[n,1] = 1.0; // 切片
    for (k in 2:K)
      x[n, k] = normal_rng(0,1);
  }
  return x;
}</code></pre>
<p>以下の関数は重回帰のモデルにおいて, データの行列<code>x</code>, 回帰係数<code>beta</code>, ノイズのスケール<code>sigma</code>が与えられた場合の結果をシミュレーションしています.</p>
<pre><code>vector regression_rng(vector beta, matrix x, real sigma) {
  vector[rows(x)] y;
  vector[rows(x)] mu;
  mu = x * beta;
  for (n in 1:rows(x))
    y[n] = normal_rng(mu[n], sigma);
  return y;
}</code></pre>
<p>以下のように<code>generated quantities</code>ブロックでこれらの関数を使うと, あてはめを行った回帰モデルを使ってシミュレーションデータを生成することができるでしょう.</p>
<pre><code>parameters {
  vector[K] beta;
  real&lt;lower=0&gt; sigma;
  ...
generated quantities {
  matrix[N_sim, K] x_sim;
  vector[N_sim] y_sim;
  x_sim = predictors_rng(N_sim, K);
  y_sim = regression_rng(beta, x_sim, sigma);
}</code></pre>
<p>より洗練されたシミュレーションでは, 予測変数<code>x</code>に多変量正規分布をあてはめて, 推定されたパラメータを使ってシミュレーションのための予測変数<code>x_sim</code>をその多変量正規分布から抽出するかもしれません.</p>
<h3 id="ユーザー定義の確率分布関数">19.5. ユーザー定義の確率分布関数</h3>
<p>Stanでは確率分布関数を区別するために, 確率密度関数の名前は<code>_lpdf</code>で終わり, 確率質量関数の名前は<code>_lpmf</code>で終わります. どちらの関数も<code>real</code>型を返します.</p>
<p>標準正規分布を複数使うモデルを考えましょう. Stanでは標準正規分布のために, 特定の多重定義された密度関数もデフォルトの確率分布関数も存在しません. そのため, 標準正規分布の部分をすべて平均0・スケール1の正規分布で書くよりも, 新しい密度関数を定義して再利用した方がいいでしょう.</p>
<pre><code>functions {
  real unit_normal_lpdf(real y) {
    return normal_lpdf(y | 0, 1);
  }
}
...
model {
  alpha ~ unit_normal();
  beta ~ unit_normal();
  ...
}</code></pre>
<p><code>unit_normal</code>関数を密度関数として使うには, 関数名が<code>_lpdf</code>で終わる必要があります（同様に質量関数として使うには<code>_lpmf</code>で終わる必要があります）.</p>
<p>一般に, もし<code>foo_lpdf</code>が<span class="math inline">(<em>N</em> + 1)</span>個の引数をとるように定義されているならば, 以下のように使われます.</p>
<pre><code>y ~ foo(theta1, ..., thetaN);</code></pre>
<p>これは以下を略した書き方です.</p>
<pre><code>target += foo_lpdf(y | theta1, ..., thetaN);</code></pre>
<p>ビルトイン関数と同じようにサンプリング文においては, 末尾の<code>_lpdf</code>は落ちて, 1番目の引数はサンプリングの記号（<code>~</code>）の左側に移動します.</p>
<p>（確率質量関数を表す）<code>_lpmf</code>で終わる関数もまったく同じように振る舞います. 違いは, 密度関数（<code>_lpdf</code>）の1番目の引数は連続値でなければならない（整数値または整数値の配列ではない）のに対し, 質量関数（<code>_lpmf</code>）の1番目の引数は離散値でなければならない（整数値または整数値の配列である）ことです.</p>
<h3 id="多重定義の関数">19.6. 多重定義の関数</h3>
<p>Stanではユーザー定義関数の多重定義は許されていません. 同じ関数名で引数の型が異なる, 2つの別の関数を定義することはできないということです.</p>
<h3 id="関数にドキュメントをつける">19.7. 関数にドキュメントをつける</h3>
<p>理想はインターフェースのレベルで, 関数にドキュメントをつけることでしょう. 関数の説明文のためのStanのスタイルガイドは, Doxygen（C++）とJavadoc（Java）の自動文書化システムで使われているのと同じフォーマットに従います. これらのフォーマットでは, 何らかの説明文からはじまり, それから引数の変数とそれらの型および返値を表示します.</p>
<p>例えば, 19.4節で扱ったデータの行列を生成する関数のドキュメントは以下のようになるでしょう.</p>
<pre><code>/**
* データの行列を返す. 行はアイテムに対応する. また1番目の列は切片を表す1で埋まり,
* 残りの列は標準正規分布から抽出された乱数で埋まっている.
*
* @param N は行数で, データのアイテムに対応する.
* @param K はアイテムあたりの切片項込みの予測変数の数.
* @return シミュレーションで生成された予測変数の行列.
*/
matrix predictors_rng(int N, int K) {
  ...</code></pre>
<p>コメントは<code>/**</code>で始まって<code>*/</code>で終わります. そしてコメントの各行にアスタリスク（<code>*</code>）があります. <code>@param</code>のあとに引数の変数名が続き, 関数の引数を説明します. <code>@return</code>は返値を表します. Stanは（まだ）JavadocやDoxygenのような自動文書生成システムを持っていないので, Stanのパーサにとってはこのコメントは単に<code>/*</code>から始まって<code>*/</code>で終わる大きなコメントに見えます.</p>
<p>例外を発生させる関数については, <code>@throws</code>を使って例外の説明をしましょう. <a href="#fn14" class="footnoteRef" id="fnref14"><sup>14</sup></a>例えば,</p>
<pre><code>...
* @param theta
* @throws もしthetaの要素が1つでも負なら例外を発生します.
*/
real entropy(vector theta) {
  ...</code></pre>
<p>たいてい例外のタイプもドキュメントにしますが, Stanの言語の一部として用意されていないので, 書く必要はありません.</p>
<h3 id="関数の型の要約">19.8. 関数の型の要約</h3>
<p>関数は<code>void</code>型または<code>void</code>ではない返値の型を持ちます. 特別な接尾語である<code>_lpdf</code>・<code>_lpmf</code>・<code>_lp</code>・<code>_rng</code>のうちの1つが関数名の末尾につくかもしれません.</p>
<h4 id="void-vs.-voidではない返値"><code>void</code> vs. <code>void</code>ではない返値</h4>
<p><code>void</code>を返すように宣言された関数だけがステートメント（文）として使われます. また, このような関数の内部では引数なしの<code>return</code>文が使われます.</p>
<p><code>void</code>でない値を返すように宣言された関数だけが式として使われます. また, このような関数の内部では引数ありの<code>return</code>文が使われ, その引数は宣言した返値の型とマッチする必要があります.</p>
<h4 id="接尾語がつく-または-つかない">接尾語がつく または つかない</h4>
<p>名前が<code>_lpmf</code>または<code>_lpdf</code>で終わり, <code>real</code>型を返す関数だけが, サンプリング文で確率分布関数として使うことができます.</p>
<p>名前が<code>_lp</code>で終わる関数だけが, サンプリング文や<code>target +=</code>文を通して対数確率を累積する機能にアクセスすることができます. このような関数は<code>transformed parameters</code>ブロックまたは<code>model</code>ブロックだけで使うことができます.</p>
<p>名前が<code>_rng</code>で終わる関数だけがビルトインの疑似乱数発生器にアクセスすることができます. このような関数は<code>generated quantities</code>ブロックだけで使うことができます.</p>
<h3 id="再帰関数">19.9. 再帰関数</h3>
<p>Stanは再帰関数の定義をサポートしています. 再帰関数は場合によっては有用です. 例えば, 行列の累乗<span class="math inline">(<em>A</em><sup><em>n</em></sup>)</span>を考えましょう. <span class="math inline">(<em>A</em><sup><em>n</em></sup>)</span>は正方行列<span class="math inline">(<em>A</em>)</span>と正の整数値<span class="math inline">(<em>n</em>)</span>に対して次のように定義されます.</p>
<p><br /><span class="math display">$$ A^n = \left\{\begin{array}{ll} I &amp; \text{$n = 0$のとき} \\ A A^{n-1} &amp; \text{$n &gt; 0$のとき} \end{array}\right.$$</span><br /></p>
<p>ここで<span class="math inline">(<em>I</em>)</span>は単位行列です. この定義は直接再帰関数の定義に変換できます.</p>
<pre><code>matrix matrix_pow(matrix a, int n);

matrix matrix_pow(matrix a, int n) {
  if (n == 0)
    return diag_matrix(rep_vector(1, rows(a)));
  else
    return a * matrix_pow(a, n - 1);
}</code></pre>
<p>再帰関数の中で<code>matrix_pow</code>が矛盾なく使えるように, 関数の定義の前に宣言だけは必要になります. <a href="#fn15" class="footnoteRef" id="fnref15"><sup>15</sup></a>次の条件節を加えることで, ベースケースまで再帰が到達しないようになり, より効率的になるでしょう.</p>
<pre><code>else if (n == 1)
  return a;</code></pre>
<h2 id="微分方程式を解く">20. 微分方程式を解く</h2>
<p>Stanでは常微分方程式（ordinary differential equations, 略してODE）の系を解くビルトインの仕組みがあります. Stanでは2つの異なるソルバーが用意されています. 1つはstiffでない系を, もう片方はstiffの系を解くためにチューニングされています.</p>
<ul>
<li>rk45: stiffでない系のための4-5次のルンゲクッタ法です. (Dormand and Prince, 1980; Ahnert and Mulansky, 2011), and</li>
<li>bdf: stiffな系のための, 刻み幅が変動して近似の次数が変動する, 後退差分公式の実装です (Cohen and Hindmarsh, 1996; Serban and Hindmarsh, 2005)</li>
</ul>
<p>stiffな常微分方程式の系に関する議論は20.4節を見てください. 短く言えば, stiffな系のソルバーは遅いけれどより頑健です. どれくらいそういう傾向があるかは, 解く系やパラメータ空間の領域に依存します. StanのODEソルバーの引数と返値については38章に記載があります.</p>
<h3 id="例単純な調和振動子">20.1. 例：単純な調和振動子</h3>
<p>ODEの系の具体例として調和振動子を考えましょう. 調和振動子の系は平衡状態の位置とそこからのズレに比例して戻ろうとする力（摩擦の影響を含む）で特徴づけられます. 系の状態は位置と運動量を表すペアである<span class="math inline">(<em>y</em> = (<em>y</em><sub>1</sub>, <em>y</em><sub>2</sub>))</span>で記述されます（すなわち相空間上の点になります）. 時刻による系の変化は次の微分方程式で与えられます. <a href="#fn16" class="footnoteRef" id="fnref16"><sup>16</sup></a></p>
<p><br /><span class="math display">$$ \frac{d}{dt}y_{1} = y_{2} \quad \frac{d}{dt}y_{2} = -y_{1} - \theta y_{2} $$</span><br /></p>
<p>この状態式はある与えられた時刻における系の状態を, 初期状態・初期状態からの経過時間・系のパラメータの関数として暗黙のうちに定義しています.</p>
<h4 id="初期条件が与えられた場合の解法">初期条件が与えられた場合の解法</h4>
<p>系のパラメータ<span class="math inline">(<em>θ</em>)</span>の値と時刻<span class="math inline">(<em>t</em><sub>0</sub>)</span>における初期状態<span class="math inline">(<em>y</em>(<em>t</em><sub>0</sub>))</span>が与えられると, ある時刻の系列<span class="math inline">(<em>t</em><sub>0</sub> &lt; <em>t</em><sub>1</sub> &lt; <em>t</em><sub>2</sub> &lt; ⋯)</span>における<span class="math inline">(<em>y</em>(<em>t</em>))</span>を求めるために, 数値計算で解の時間発展をシミュレーションできます.</p>
<h3 id="常微分方程式の系をコーディングする">20.2. 常微分方程式の系をコーディングする</h3>
<p>Stanでは厳密に引数と返値が決められた関数によって, ODEの系は直接的にコーディングされます. 例えば, (20.1)式で与えられた単純な調和振動子は, Stanでは次のようにコーディングされるでしょう（ユーザー定義関数のコーディングに関するさらなる情報は19章を見てください）.</p>
<pre><code>real[] sho(real t,        // 時刻
           real[] y,      // 状態
           real[] theta,  // パラメータ
           real[] x_r,    // データ (`real`)
           int[] x_i) {   // データ (`integer`)
  real dydt[2];
  dydt[1] = y[2];
  dydt[2] = -y[1] - theta[1] * y[2];
  return dydt;
}</code></pre>
<p>この系を表す関数は, 時刻<code>t</code>（<code>real</code>型の値）, 系の状態<code>y</code>（<code>real</code>型の配列）, 系のパラメータ<code>theta</code>（<code>real</code>型の配列）, 実数値のデータである変数<code>x_r</code>（<code>real</code>型の配列）, 整数値のデータである変数<code>x_i</code>（<code>int</code>型の配列）を引数にとります. この関数は系の状態<code>y</code>の時間についての微分の値の配列を返します. 微分の値は時刻<code>t</code>・状態<code>y</code>で評価したものです. ここでコーディングした単純な調和振動子は時間に依存する方程式ではありません. すなわち, <code>t</code>は<code>dtdt</code>の定義に現れません. 単純な調和振動子は<code>real</code>型および<code>int</code>型のデータを使いません. しかし, 上のコードで示した関数の引数と返値の型に厳密に一致するように, これらの使わない引数も系を表す関数の引数として宣言に含める必要があります.</p>
<h4 id="厳密な関数の引数と返値の型">厳密な関数の引数と返値の型</h4>
<p>系を定義する関数はこれらの引数と返値の方を必ず持っている必要があります. このため, 系がデータやパラメータを含まない場合には, データやパラメータに0の長さの配列を渡すことになるかもしれません. データを表す変数に全く依存しない単純な調和振動子の完全な例は図20.1にあります.</p>
<h4 id="不連続なodeの系の関数">不連続なODEの系の関数</h4>
<p>ODEのソルバーは状態<code>y</code>の関数に不連続点があっても積分できます. ただし, 不連続点の近くの点の精度は問題になるかもしれません（多数の小さな計算ステップを要します）. そのような不連続点の例は薬物動態モデルの中のラグです. 体内の薬物濃度は, あるラグ時間を<span class="math inline">(<em>t</em><sup>′</sup>)</span>とすると, <span class="math inline">(0 &lt; <em>t</em> &lt; <em>t</em><sup>′</sup>)</span>を満たす時刻<code>t</code>に対しては濃度がゼロとなる一方で, <span class="math inline">(<em>t</em> ≥ <em>t</em><sup>′</sup>)</span>を満たす時刻<code>t</code>に対しては非ゼロになります. 例を挙げると, コードは以下のようになるでしょう.</p>
<pre><code>if (t &lt; t_lag)
  return 0;
else
  ... 非ゼロの値を返す ...;</code></pre>
<h4 id="さまざまな初期時刻">さまざまな初期時刻</h4>
<p>StanのODEソルバーでは引数の初期時刻は定数である必要があります（すなわち, データか変換データ（transformed data）か定数の関数となっている必要があります）. これは, 一般に, 初期時刻をパラメータにして<code>integrate_ode</code>関数を使うことはできないことを意味します. したがって, 一般に, 測定値からODEの系の初期時刻を推定することはできないことを意味します.</p>
<h3 id="測定エラーモデル">20.3. 測定エラーモデル</h3>
<p>有限の時点における, ノイズを含む系の状態の測定結果が与えられた場合に, 統計モデルや微分方程式は力学系のパラメータと初期状態の両方あるいはいずれか一方を推定するために使うことができるでしょう.</p>
<p>例えば, 単純な調和振動子において, パラメータの値が<span class="math inline">(<em>θ</em> = 0.15)</span>で初期状態が<span class="math inline">(<em>y</em>(<em>t</em> = 0) = (1, 0))</span>である場合を考えましょう. 今, 系が10時点<span class="math inline">(<em>t</em> = 1, 2, ⋯, 10)</span>において測定されたとしましょう. それぞれの時点における<span class="math inline">(<em>y</em>(<em>t</em>))</span>の測定には, <span class="math inline">(<em>y</em><sub>1</sub>(<em>t</em>))</span>と<span class="math inline">(<em>y</em><sub>2</sub>(<em>t</em>))</span>のどちらの軸にも<span class="math inline">(Normal(0, 0.1))</span>に従う独立な誤差が入るとしましょう. そのような測定のプロット例を図20.1に示します.</p>
<figure>
<img src="data:image/png;base64,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" alt="単純な調和振動子における軌跡" /><figcaption>単純な調和振動子における軌跡</figcaption>
</figure>
<p>図20.1: 単純な調和振動子において, パラメータの値が<span class="math inline">(<em>θ</em> = 0.15)</span>で初期状態が<span class="math inline">(<em>y</em>(<em>t</em> = 0) = (1, 0))</span>である場合の軌跡. 横軸・縦軸の両方に<span class="math inline">(Normal(0, 0.1))</span>に従う独立な測定誤差が入っています.</p>
<h4 id="ノイズを含む測定をシミュレートする">ノイズを含む測定をシミュレートする</h4>
<p>図20.2で与えられたStanのモデルを使ってノイズを含む測定をシミュレートして, このプロットを描くためのデータを生成しました.</p>
<pre><code>functions {
  real[] sho(real t,
             real[] y,
             real[] theta,
             real[] x_r,
             int[] x_i) {
    real dydt[2];
    dydt[1] = y[2];
    dydt[2] = -y[1] - theta[1] * y[2];
    return dydt;
  }
}
data {
  int&lt;lower=1&gt; T;
  real y0[2];
  real t0;
  real ts[T];
  real theta[1];
}
transformed data {
  real x_r[0];
  int x_i[0];
}
model {
}
generated quantities {
  real y_hat[T,2];
  y_hat = integrate_ode_rk45(sho, y0, t0, ts, theta, x_r, x_i);

  // 測定誤差を加える
  for (t in 1:T) {
    y_hat[t,1] = y_hat[t,1] + normal_rng(0,0.1);
    y_hat[t,2] = y_hat[t,2] + normal_rng(0,0.1);
  }
}</code></pre>
<p>図20.2: 単純な調和振動子からノイズを含む測定をシミュレートするStanのプログラム. 微分方程式の系は関数としてコーディングされています. 系のパラメータ<code>theta</code>と初期状態<code>y0</code>および初期時刻<code>t0</code>と観測時刻<code>ts</code>はデータとして読み込まれます. <code>generated quantities</code>ブロックはODEを解くのに使われています. 指定した時刻における状態を求め, 測定誤差を加え, 観測<code>y_hat</code>を生成しています. 系はstiffではないので, <code>rk45</code>ソルバーが使われています.</p>
<p>このプログラムはStanのプログラムにおいてどのようにODEソルバーが呼ばれるかを示しています.</p>
<pre><code>y_hat = integrate_ode_rk45(sho, y0, t0, ts, theta, x_r, x_i);</code></pre>
<p>このコードは, 初期状態<code>y0</code>・初期時刻<code>t0</code>・解の時刻<code>ts</code>（<code>ts</code>で指定した時刻における状態が求まる）・パラメータ<code>theta</code>・実数値のデータ<code>x_r</code>・整数値のデータ<code>x_i</code>が与えられた際に, 関数<code>sho</code>で定義された系の解を求めています.</p>
<p>ここではODEソルバーは<code>generated quantities</code>ブロックで呼ばれており, <span class="math inline">(10 × 2)</span>の配列で解<code>y_hat</code>を生成しています. 解<code>y_hat</code>には, 正規分布に従う疑似乱数生成器である<code>normal_rng</code>関数を使って測定誤差が加えられています. 解の配列の行の数は, 指定した解の時刻である<code>ts</code>の大きさと一致しています.</p>
<h4 id="データ-vs-パラメータ">データ vs パラメータ</h4>
<p>他の関数とは違い, ODEソルバーは引数の変数に制限があります. 特に時刻<code>t</code>・実数値のデータ<code>x_r</code>・整数値のデータ<code>x_i</code>はデータまたは変換データ（transformed data）でなくてはなりません. 初期状態<code>y0</code>もしくはパラメータ<code>theta</code>だけがパラメータになることが許されています.</p>
<h4 id="系のパラメータと初期状態を推定する">系のパラメータと初期状態を推定する</h4>
<p>Stanは未知の初期状態とパラメータの両方あるいはいずれか一方を推定できます. 一般化線形モデルにおいて線形予測子を生成するのと同じように, 予測値を生成するためにODEソルバーを確定的に使うことでしょう. それから確定的に生成された状態に測定誤差が加わって観測されるでしょう.</p>
<p>図20.3はノイズを含む観測データが与えられた場合に, 単純な調和振動子の初期状態とパラメータの値の両方を推定するStanのプログラムです.</p>
<pre><code>functions {
  real[] sho(real t,
             real[] y,
             real[] theta,
             real[] x_r,
             int[] x_i) {
    real dydt[2];
    dydt[1] = y[2];
    dydt[2] = -y[1] - theta[1] * y[2];
    return dydt;
  }
}
data {
  int&lt;lower=1&gt; T;
  real y[T,2];
  real t0;
  real ts[T];
}
transformed data {
  real x_r[0];
  int x_i[0];
}
parameters {
  real y0[2];
  vector&lt;lower=0&gt;[2] sigma;
  real theta[1];
}
model {
  real y_hat[T,2];
  sigma ~ cauchy(0, 2.5);
  theta ~ normal(0, 1);
  y0 ~ normal(0, 1);
  y_hat = integrate_ode_rk45(sho, y0, t0, ts, theta, x_r, x_i);
  for (t in 1:T)
    y[t] ~ normal(y_hat[t], sigma);
}</code></pre>
<p>図20.3: 独立した正規分布に従う測定誤差を含む単純な調和振動子において, 未知の初期状態<code>y0</code>と系のパラメータ<code>theta</code>を推定するStanのプログラム.</p>
<p>図20.2のシミュレーションのモデルと比べると, パラメータを推定するために<code>generated quantities</code>ブロックではなく<code>model</code>ブロックで<code>integrate_ode</code>関数が使われています. 測定誤差のスケール<code>sigma</code>の事前分布にコーシー分布を設定し, 系のパラメータの配列<code>theta</code>と初期状態のパラメータの配列<code>y0</code>の事前分布に標準正規分布を設定しています. ODEの解は配列<code>y_hat</code>に代入されます. それから以下のように, <code>y_hat</code>は観測ノイズを加える際の平均パラメータとなります.</p>
<pre><code>y_hat = integrate_ode_rk45(sho, y0, t0, ts, theta, x_r, x_i);
for (t in 1:T)
  y[t] ~ normal(y_hat[t], sigma);</code></pre>
<p>他の回帰のようなモデルと同じように, ノイズの分布を頑健なもの（例：Studentのt分布）に変えたり, 状態を表す変数に相関を入れたり（例：多変量正規分布を使う）, もしくはその両方（例：多変量のStudentのt分布）を使うのは簡単です.</p>
<p>スケールが0.10の独立なノイズが加わったこの単純なモデルにおいて, 時刻<span class="math inline">(<em>t</em> = 1, ⋯, 10)</span>の10個の観測データ点は, 初期状態やノイズのスケールといったODEのパラメータを確実に推定するのに十分な数です.</p>
<h3 id="stiffなode">20.4. stiffなODE</h3>
<p>常微分方程式のstiffな系は, 傾きをもとにしたステップを使うソルバーで解こうとすると数値計算上難しいことでおおまかに特徴づけられます. stiffさは典型的には, 状態の座標空間でさまざまな曲率があることが原因です. 例えば, 1つの成分の時間変化が他の成分の時間変化よりもオーダーが異なるほど遅い場合です. <a href="#fn17" class="footnoteRef" id="fnref17"><sup>17</sup></a></p>
<p>StanはstiffなODEに特化したソルバー（Cohen and Hindmarsh, 1996; Serban and Hindmarsh, 2005）を提供しています. ODEの系は前述と全く同じ引数と返値の型をとるように指定します. 唯一の違いは解を求めるためにソルバーを呼ぶところです. 以下のように<code>rk45</code>の接尾語は<code>bdf</code>に置き換わります.</p>
<pre><code>y_hat = integrate_ode_bdf(sho, y0, t0, ts, theta, x_r, x_i);</code></pre>
<p>stiffでない系にstiffなソルバー（<code>bdf</code>）を使うと, stiffでないソルバー（<code>rk45</code>）を使う場合よりずっと遅くなるかもしれません. これはstiffなソルバーでは追加のヤコビアンの計算をするからです. 他方, stiffな系にstiffでないソルバーを使おうとすると, 小さなステップ幅と非常に多数のステップを要するために実行は失敗するでしょう.</p>
<h3 id="odeソルバーの制御パラメータ">20.5. ODEソルバーの制御パラメータ</h3>
<p>上で示したソルバーの呼び出しではデフォルトの制御パラメータの設定を使っていました. stiffでないソルバーとstiffなソルバーともに, 3つの引数を追加することができます. 3つのうちどれか1つでも追加する場合には, 残りの2つも追加する必要があります.</p>
<pre><code>y_hat = integrate_ode_bdf(sho, y0, t0, ts, theta, x_r, x_i,
                          rel_tol, abs_tol, max_steps);</code></pre>
<p>3つの制御のための引数は相対許容値（relative tolerance）・絶対許容値（absolute tolerance）・最大ステップ数（maximum number of steps）です. 相対許容値と絶対許容値のデフォルトの値は両方とも<code>1e-6</code>（<span class="math inline">(10<sup> − 6</sup>)</span>）です. デフォルトの最大ステップ数は<code>1e6</code>（<span class="math inline">(10<sup>6</sup>)</span>）です.</p>
<h4 id="許容値">許容値</h4>
<p>相対許容値と絶対許容値はソルバーによって生成される解の精度を制御します. 相対許容値は解の値と比べた場合の比を制御し, 一方で絶対許容差は解の値そのものの誤差の最大値を制御します. より小さな許容値を設定するとより精度のよい解となります. また, より小さな許容値はより多くの計算時間を要します.</p>
<h5 id="感度分析">感度分析</h5>
<p>許容値は十分に小さく設定すべきです. それ以上小さくしてもStanのプログラムによって生成される事後サンプルの統計的な特徴はほとんど変わらないことが目安です.</p>
<h4 id="最大ステップ数">最大ステップ数</h4>
<p>最大ステップ数はシミュレーションが暴走するのを止めるために使われます. MCMCにおいては, 悪いジャンプが採択されたとき, 特にwarmup期間において, 暴走が起こる可能性があります. stiffでないソルバーを使う場合には, 悪いジャンプによって結果的に, パラメータ空間のstiffな領域にサンプラーが跳びこむかもしれません. stiffな領域では適度な許容値を満たすために非常に小さなステップサイズと非常に多数のステップが必要となって, シミュレーションが暴走するかもしれません.</p>
<h2 id="問題のある事後分布">21. 問題のある事後分布</h2>
<p>数学的に言うと, 正則な事後分布であればベイズ推定はできていて, 話はそこで終わりです. 有限な分散さえも, あるいは有限な平均さえも必要ではありません. 必要なのは有限な積分だけです. それにもかかわらず, モデリングは厄介な仕事で, 経験を積んだモデル作成者でも非正則な事後分布<a href="#fn18" class="footnoteRef" id="fnref18"><sup>18</sup></a>ができるようなモデルをコーディングしてしまうことがあります. さらに, 数学的には正しいにも関わらず, 実用的には挙動がおかしい事後分布もあります. この章では, 問題のある事後分布を推定してしまうモデルについて, ベイズ推定一般として, あるいはStanでの実用面から議論します.</p>
<h3 id="回帰での予測変数の共線性">21.1. 回帰での予測変数の共線性</h3>
<p>この節では, 識別可能性に関する古典的な問題について議論します. この問題は, 事後密度を尾根状にし, サンプリングと推定の両方をめちゃくちゃにしてしまうというものです.</p>
<h4 id="共線性の例">共線性の例</h4>
<h5 id="余分な切片">余分な切片</h5>
<p>共線性の最初の例は, 余分な切片パラメータを含むという人工的なものです. <a href="#fn19" class="footnoteRef" id="fnref19"><sup>19</sup></a> <span class="math inline"><em>n</em> ∈ 1 : <em>N</em></span>についての観測値<span class="math inline"><em>y</em><sub><em>n</em></sub></span>, 2つの切片パラメータ<span class="math inline"><em>λ</em><sub>1</sub></span>と<span class="math inline"><em>λ</em><sub>2</sub></span>, スケールパラメータ<span class="math inline"><em>σ</em> &gt; 0</span>があり, サンプリング分布が以下のようであるとします.</p>
<p><br /><span class="math display"><em>y</em><sub><em>n</em></sub> ∼ Normal(<em>λ</em><sub>1</sub> + <em>λ</em><sub>2</sub>, <em>σ</em>)</span><br /></p>
<p>任意の定数<span class="math inline"><em>q</em></span>について, <span class="math inline"><em>q</em></span>を<span class="math inline"><em>λ</em><sub>1</sub></span>に加え, <span class="math inline"><em>λ</em><sub>2</sub></span>から引くならば, <span class="math inline"><em>y</em></span>のサンプリング密度は不変です.</p>
<p><br /><span class="math display"><em>p</em>(<em>y</em> ∣ <em>λ</em><sub>1</sub>, <em>λ</em><sub>2</sub>, <em>σ</em>) = <em>p</em>(<em>y</em> ∣ <em>λ</em><sub>1</sub> + <em>q</em>, <em>λ</em><sub>2</sub> − <em>q</em>, <em>σ</em>)</span><br /></p>
<p>その結果, 非正則一様事前分布<span class="math inline"><em>p</em>(<em>μ</em>, <em>σ</em>) ∝ 1</span>からは非正則事後分布が導かれます. この非正則性が生じるのは, <span class="math inline"><em>λ</em><sub>1</sub> + <em>q</em>, <em>λ</em><sub>2</sub> − <em>q</em></span> <a href="#fn20" class="footnoteRef" id="fnref20"><sup>20</sup></a>の近傍では, どんな<span class="math inline"><em>q</em></span>についても質量が同じになるからです. したがって, <span class="math inline"><em>λ</em><sub>1</sub> = 1000000000</span>かつ<span class="math inline"><em>λ</em><sub>2</sub> =  − 1000000000</span>の近傍でも, <span class="math inline"><em>λ</em><sub>1</sub> = 0</span>かつ<span class="math inline"><em>λ</em>2 = 0</span>の近傍と同じ時間がサンプラーには必要になりますし, さらにもっと離れた値でも同様です.</p>
<p>このモデルの周辺事後分布<span class="math inline"><em>p</em>(<em>λ</em><sub>1</sub>, <em>λ</em><sub>2</sub> ∣ <em>y</em>)</span>はしたがって非正則です. <a href="#fn21" class="footnoteRef" id="fnref21"><sup>21</sup></a> この非正則性は, 図21.1の左側に図示したように, 視覚的には事後密度の尾根として表されます. このモデルの尾根は, <span class="math inline"><em>c</em></span>をある定数として, <span class="math inline"><em>λ</em><sub>2</sub> =  − <em>λ</em><sub>1</sub> + <em>c</em></span>という直線<a href="#fn22" class="footnoteRef" id="fnref22"><sup>22</sup></a>に沿ってできます.</p>
<p>このモデルを, 単一の切片パラメータ<span class="math inline"><em>μ</em></span>をもち, 以下のサンプリング分布をもつ単回帰と比較しましょう.</p>
<p><br /><span class="math display"><em>y</em><sub><em>n</em></sub> ∼ Normal(<em>μ</em>, <em>σ</em>)</span><br /></p>
<p>この場合には, 非正則事前分布であっても, 異なる値を持つ少なくとも2つのデータ点<span class="math inline"><em>y</em><sub><em>n</em></sub></span>がある限り事後分布は正則となります.</p>
<h5 id="irtモデルにおける能力と難易度">IRTモデルにおける能力と難易度</h5>
<p>項目反応理論（IRT）モデルで, 生徒<span class="math inline"><em>j</em> ∈ 1 : <em>J</em></span>が能力<span class="math inline"><em>α</em><sub><em>j</em></sub></span>を持ち, 試験項目<span class="math inline"><em>i</em> ∈ 1 : <em>I</em></span>が難易度<span class="math inline"><em>β</em><sub><em>i</em></sub></span>を持つとします. 観測データは<span class="math inline"><em>I</em> × <em>J</em></span>次元配列で, エントリー<span class="math inline"><em>y</em><sub><em>i</em>, <em>j</em></sub> ∈ {0, 1}</span>は, <span class="math inline"><em>y</em><sub><em>i</em>, <em>j</em></sub> = 1</span>のとき生徒<span class="math inline"><em>j</em></span>が問題<span class="math inline"><em>i</em></span>に正答したことを示すようにコード化します. このデータのサンプリング分布は以下のようになります.</p>
<p><br /><span class="math display"><em>y</em><sub><em>i</em>, <em>j</em></sub> ∼ Bernoulli(<em>l</em><em>o</em><em>g</em><em>i</em><em>t</em><sup> − 1</sup>(<em>α</em><sub><em>j</em></sub> − <em>β</em><sub><em>i</em></sub>))</span><br /></p>
<p>任意の定数<span class="math inline"><em>c</em></span>について, 定数<span class="math inline"><em>c</em></span>を能力すべてに加え, かつ難易度すべてにも加えると, <span class="math inline"><em>y</em></span>の確率は変わりません. <a href="#fn23" class="footnoteRef" id="fnref23"><sup>23</sup></a></p>
<p><br /><span class="math display"><em>p</em>(<em>y</em> ∣ <em>α</em>, <em>β</em>) = <em>p</em>(<em>y</em> ∣ <em>α</em> + <em>c</em>, <em>β</em> + <em>c</em>)</span><br /></p>
<p>このため, 上で議論した2つの切片を持つ回帰の多変量版が出現することになります.</p>
<h5 id="一般的な共線性のある回帰予測変数">一般的な共線性のある回帰予測変数</h5>
<p>共線性の問題の一般型は, 回帰の予測変数が共線であるときに発生します. 例えば, 直線回帰のサンプリング分布を考えます.</p>
<p><br /><span class="math display"><em>y</em><sub><em>n</em></sub> ∼ Normal(<em>x</em><sub><em>n</em></sub><em>β</em>, <em>σ</em>)</span><br /></p>
<p><span class="math inline"><em>y</em></span>は<span class="math inline"><em>N</em></span>次元の観測値ベクトル, <span class="math inline"><em>x</em></span>は<span class="math inline"><em>N</em> × <em>K</em></span>の予測変数行列, <span class="math inline"><em>β</em></span>は<span class="math inline"><em>K</em></span>次元の係数ベクトルです.</p>
<p>ここで, 予測変数行列の列<span class="math inline"><em>k</em></span>が列<span class="math inline"><em>k</em>′</span>の定数倍になっているとします. すなわち, すべての<span class="math inline"><em>n</em></span>について<span class="math inline"><em>x</em><sub><em>n</em>, <em>k</em></sub> = <em>c</em><em>x</em><sub><em>n</em>, <em>k</em>′</sub></span>となるような定数<span class="math inline"><em>c</em></span>が存在するとします. この場合, 係数<span class="math inline"><em>β</em><sub><em>k</em></sub></span>と<span class="math inline"><em>β</em><sub><em>k</em>′</sub></span>は予測値を変えずに共変動できます. そのため, 任意の<span class="math inline"><em>d</em> ≠ 0</span>について次式が成り立ちます.</p>
<p><br /><span class="math display">$$ p(y \mid \dots,\beta_k,\dots,\beta_{k'},\dots,\sigma) = p(y \mid \dots,d\beta_k,\dots,\frac{d}{c}\beta_{k'},\dots,\sigma) $$</span><br /> <a href="#fn24" class="footnoteRef" id="fnref24"><sup>24</sup></a></p>
<p>予測変数行列の列が, 上の議論のような完全な共線ではなくとも, 共線に近ければ推定に同じような問題が発生します.</p>
<h5 id="irtでの乗数の問題">IRTでの乗数の問題</h5>
<p>IRTモデルで, 各々の質問に識別力パラメータ<span class="math inline"><em>δ</em><sub><em>i</em></sub></span>を加えるとします. データをサンプリングするモデルは次式です.</p>
<p><br /><span class="math display"><em>y</em><sub><em>i</em>, <em>j</em></sub> ∼ Bernoulli(<em>l</em><em>o</em><em>g</em><em>i</em><em>t</em><sup> − 1</sup>(<em>δ</em><sub><em>i</em></sub>(<em>α</em><sub><em>j</em></sub> − <em>β</em><sub><em>i</em></sub>)))</span><br /></p>
<p>任意の定数<span class="math inline"><em>c</em> ≠ 0</span>について, <span class="math inline"><em>δ</em></span>を<span class="math inline"><em>c</em></span>倍し, <span class="math inline"><em>α</em></span>と<span class="math inline"><em>β</em></span>を<span class="math inline"><em>c</em></span>で割っても, 尤度は同じです.</p>
<p><br /><span class="math display">$$ p(y \mid \delta, \alpha, \beta) = p(y \mid c\delta, \frac{1}{c}\alpha, \frac{1}{c}\beta) $$</span><br /></p>
<p>もし<span class="math inline"><em>c</em> &lt; 0</span>ならば, 密度を変えずに, <span class="math inline"><em>α</em></span>および<span class="math inline"><em>β</em></span>, <span class="math inline"><em>δ</em></span>のすべての成分の符号が反転します.</p>
<h5 id="softmaxでのk-vs.-k-1パラメータ">SoftmaxでのK vs. K-1パラメータ</h5>
<p><span class="math inline"><em>K</em></span>単体（すなわち, 合計すると1になる, 非負の値からなる<span class="math inline"><em>K</em></span>次元ベクトル）をパラメータ化するためには, <span class="math inline"><em>K</em> − 1</span>個のパラメータだけが必要です. なぜなら, <span class="math inline"><em>K</em></span>番目のパラメータは, 最初から<span class="math inline"><em>K</em> − 1</span>番目までの合計を1から引いた値になるからです. したがって, <span class="math inline"><em>θ</em></span>を<span class="math inline"><em>K</em></span>単体とすると次式が成り立ちます.</p>
<p><br /><span class="math display">$$ \theta_K = 1 - \sum_{k=1}^{K-1}\theta_k $$</span><br /></p>
<p>softmax関数（35.11節参照）は, 線形予測子の<span class="math inline"><em>K</em></span>次元ベクトル<span class="math inline"><em>α</em></span>を<span class="math inline"><em>K</em></span>単体<span class="math inline"><em>θ</em></span>にマッピングします. つまり<span class="math inline"><em>θ</em> = <em>s</em><em>o</em><em>f</em><em>t</em><em>m</em><em>a</em><em>x</em>(<em>α</em>)</span>で, 定義は次式です.</p>
<p><br /><span class="math display">$$ \theta_k = \frac{\exp(\alpha_k)}{\sum_{k'=1}^{K}\exp(\alpha'_k)} $$</span><br /></p>
<p>softmax関数は多対1関数です. パラメータ<span class="math inline"><em>α</em></span>に制約がないと, 識別可能性が失われます. とくに, すべての<span class="math inline"><em>α</em><sub><em>k</em></sub></span>に定数を足したり引いたりしても, おなじ単体<span class="math inline"><em>θ</em></span>となります.</p>
<h4 id="不変さを軽減する">不変さを軽減する</h4>
<p>前の節で議論した例はすべて, データの確率密度を変えないままパラメータの平行移動や拡大縮小ができるというものです. この問題を軽減する方法はいくつかあります.</p>
<h5 id="余分なパラメータや予測変数を取り除く">余分なパラメータや予測変数を取り除く</h5>
<p>複数の切片, <span class="math inline"><em>λ</em><sub>1</sub></span>と<span class="math inline"><em>λ</em><sub>2</sub></span>がある場合には, 余分な切片を取り除くのが最も単純な解決法です. これによりモデルは, 単一の切片パラメータ<span class="math inline"><em>μ</em></span>を持ち, サンプリング分布は<span class="math inline"><em>y</em><sub><em>n</em></sub> ∼ Normal(<em>μ</em>, <em>σ</em>)</span>となります. 同じ解決法を, 共線性の問題にも使えます. 予測変数行列<span class="math inline"><em>x</em></span>から列を1つ取り除くだけです.</p>
<h5 id="ピン留めパラメータ">ピン留めパラメータ</h5>
<p>識別力パラメータのないIRTモデルは, パラメータの1つを固定した値（普通は0）にピン留めすることで固定することができます. 例えば, 最初の生徒の能力<span class="math inline"><em>α</em><sub>1</sub></span>を0に固定することができます. このとき, その他の生徒すべての能力パラメータは生徒1に対する相対値と解釈できます. 同様に難易度パラメータは, 生徒1の解答能力に対する相対値と解釈されます.</p>
<p>この解決法は, 質問の識別力パラメータ<span class="math inline"><em>δ</em><sub><em>i</em></sub></span>を導入すると発生する, 乗数による不変性を扱うには十分ではありません. この問題を解決するには, 識別力<a href="#fn25" class="footnoteRef" id="fnref25"><sup>25</sup></a>パラメータの1つ, たとえば<span class="math inline"><em>δ</em><sub>1</sub></span>にも制約をつけなくてはなりません. 乗数による不変性は, 加算とは異なり, 非零の値に制約しなければなりません. 都合が良いのは1とすることです. このとき, 識別力パラメータはすべて, 項目1の識別力に対する相対値として解釈されるでしょう.</p>
<p>softmax(<span class="math inline"><em>α</em></span>)の多対1の性質は, <span class="math inline"><em>α</em></span>の成分の1つをピン留めすることで軽減するのは普通です. 例えば, <span class="math inline"><em>α</em><sub><em>K</em></sub> = 0</span>と固定します. そうすると, <span class="math inline"><em>K</em> − 1</span>次元の制約のないパラメータから, <span class="math inline"><em>K</em></span>単体へ, 1対1対応となります. これが, 単体として制約されるパラメータをStanで定義するおおまかな方法です. 正確な定義は58.6節を参照してください. <span class="math inline"><em>K</em> − 1</span>次元ベクトルから単体を生成するStanコードは以下のとおりです.</p>
<pre><code>vector softmax_id(vector alpha) {
  vector[num_elements(alpha) + 1] alphac;
  for (k in 1:num_elements(alpha))
    alphac[k] = alpha[k];
  alphac[num_elements(alphac)] = 0;
  return softmax(alphac);
}</code></pre>
<h5 id="事前分布を加える">事前分布を加える</h5>
<p>ここまで, パラメータの事前分布が非正則一様事前分布であるとしてモデルを議論してきました.</p>
<p>こうした不変性の問題に対する, より一般的なベイジアンの解決法は, パラメータに正則な事前分布を与えることです. 加法的でも, 乗法的でも, どちらの不変性に由来する問題でも, この方法を使って解決できます.</p>
<p>例として, 複数の切片に正規分布を事前分布として与えます.</p>
<p><br /><span class="math display"><em>λ</em><sub>1</sub>, <em>λ</em><sub>2</sub> ∼ Normal(0, <em>τ</em>)</span><br /></p>
<p><span class="math inline"><em>τ</em></span>を定数値のスケールとすると, 事後最頻値が<span class="math inline"><em>λ</em><sub>1</sub> = <em>λ</em><sub>2</sub></span>となる点に位置することが保証されます. なぜならこれにより, <span class="math inline">logNormal(<em>λ</em><sub>1</sub> ∣ 0, <em>τ</em>) + logNormal(<em>λ</em><sub>2</sub> ∣ 0, <em>τ</em>)</span>が最小化されるからです. <a href="#fn26" class="footnoteRef" id="fnref26"><sup>26</sup></a> 2つの切片を持つモデルに事前分布を加えたものを図21.1の中央に示します. 図21.1の右側は, 単一の切片を持つように再パラメータ化した結果です.</p>
<p><img src="data:image/png;base64,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" alt="図21.1" /> 図21.1: 事前分布なしの2切片パラメータ化, 平均0標準偏差1の正規事前分布の2切片パラメータ化, 事前分布なしの1切片再パラメータ化のそれぞれの事後分布. 3つの場合とも, 平均0標準偏差1の正規分布から抽出された100データ点について事後分布をプロットしています. 左）2切片パラメータ化は, 北西方向と南東方向とに無限に伸びる尾根状の非正則事後分布<a href="#fn27" class="footnoteRef" id="fnref27"><sup>27</sup></a>となります. 中）平均0標準偏差1の正規事前分布を切片に対して加えると正則事後分布になります. 右）事前分布なしの単一切片パラメータ化も正則事後分布が得られます.</p>
<p><span class="math inline"><em>K</em></span>単体パラメータ化<span class="math inline"><em>θ</em> = <em>s</em><em>o</em><em>f</em><em>t</em><em>m</em><em>a</em><em>x</em>(<em>α</em>)</span>を, 制約なしの<span class="math inline"><em>K</em></span>次元ベクトル<span class="math inline"><em>α</em></span>について識別するための別の方法は, <span class="math inline"><em>α</em></span>の各成分が, ある固定した位置パラメータをもつ事前分布に従うとすることです（すなわち, 位置が変わるような階層事前分はとくに避けるようにします）. <span class="math inline"><em>α</em><sub><em>K</em></sub> = 0</span>とピン留めする方法では, <span class="math inline"><em>K</em></span>番目の値への相対値として<span class="math inline"><em>K</em> − 1</span>個の値をモデリングしていました. 事前分布に基づく方法では, そうではなく, <span class="math inline"><em>K</em></span>個の値を等しく対称的に扱ってモデリングします. 一方で, ピン留めのパラメータ化の方が通常は, （事前分布で制約された）空間内の動きに余分な自由度がないので, 統計学的にはより効率的です.</p>
<h5 id="漠然事前分布-強情報事前分布-弱情報事前分布">漠然事前分布, 強情報事前分布, 弱情報事前分布</h5>
<p>不変性を解決するために事前分布を加える際には注意が必要です. 事前分布の幅が広すぎると（すなわち漠然すぎると）, 理論的には解決するとしても, 実際にはサンプラーがやはり動くのに苦労するということになるでしょう.</p>
<p>理想的には, モデリングする問題についての本質的な知識に基づいて, 現実的な事前分布を定式化するのがよいでしょう. そのような事前分布なら, 事前の知識に基づいた適切な強さを選択できます. 強い事前の情報があるときには, 強情報事前分布を使うとよいでしょう.</p>
<p>強い事前の情報がないときには弱情報事前分布を使って, 事後分布中でデータを圧倒しないことと, 推定の計算を制御することとの適切なバランスをとります. たいていの問題では, 推定値がどのくらいのスケールになるか, 少なくとも何らかの見解をモデル作成者は持っているでしょう. そして, データを圧倒しないような, しかし事後分布を十分に制御できるような, 識別可能性の目的にあう事前分布を選ぶことができます.</p>
<p>事前分布は, IRTモデルにおける加法的な不変性を制御するためにも同じように使えます. 典型的には単純に, スケールを制御する強い事前分布を生徒の能力パラメーター<span class="math inline"><em>α</em></span>に設定することで, 基本的なIRTモデルにおける加法的な不変性と, 質問項目の識別力パラメータを含む拡張モデルにおける乗法的な不変性を制御します. そのような事前分布は, 解析対象についての事前知識を増やすものではありません. そして, 質問項目の難易度についての事前分布は, 解析対象の事前知識に基づいて, 情報のある事前分布もしくは弱情報事前分布を選ぶとよいでしょう.</p>
<h3 id="混合分布モデルでのラベルスイッチング">21.2. 混合分布モデルでのラベルスイッチング</h3>
<p>回帰モデルにおいて共線性がある場合, 事後分布を最大化するパラメータの値は無限個あります. その一方で, 混合分布モデルの成分を入れ替えることができる場合, 事後分布を最大化するパラメータの値は有限個ですが, 複数あるために問題のある事後分布となります.</p>
<h4 id="混合分布モデル">混合分布モデル</h4>
<p>2つの位置パラメータ<span class="math inline"><em>μ</em><sub>1</sub></span>と<span class="math inline"><em>μ</em><sub>2</sub></span>をもつ正規混合分布モデルを考えます. スケールはともに<span class="math inline"><em>σ</em> &gt; 0</span>で, 混合率を<span class="math inline"><em>θ</em> ∈ [0, 1]</span>とすると尤度は次式になります.</p>
<p><br /><span class="math display">$$ p(y \mid \theta,\mu_1,\mu_2,\sigma) = \prod_{n=1}^{N}(\theta\mathsf{Normal}(y_n \mid \mu_1,\sigma) + (1 - \theta)\mathsf{Normal}(y_n \mid \mu_2,\sigma)) $$</span><br /></p>
<p>ここでの問題は, 次式のように混合成分が入れ替わりうることです.</p>
<p><br /><span class="math display"><em>p</em>(<em>θ</em>, <em>μ</em><sub>1</sub>, <em>μ</em><sub>2</sub>, <em>σ</em> ∣ <em>y</em>) = <em>p</em>((1 − <em>θ</em>), <em>μ</em><sub>2</sub>, <em>μ</em><sub>1</sub>, <em>σ</em> ∣ <em>y</em>)</span><br /></p>
<p>この問題は, クラスタリングモデルのように, 混合成分<span class="math inline"><em>K</em></span>の数が大きくなるにつれて悪化します. <span class="math inline"><em>K</em>!</span>個の同一の事後最大値ができます.</p>
<h4 id="収束モニタリングと有効サンプルサイズ">収束モニタリングと有効サンプルサイズ</h4>
<p>事後分布の収束と有効サンプルサイズの分析も混合分布モデルでは難しい問題です. 例えば, Stanが報告する<span class="math inline">$\hat{R}$</span>収束統計量も, 有効サンプルサイズの計算も, ラベルスイッチングで不正確になります. 問題は, これらの計算の鍵となる要素である事後平均がラベルスイッチングの影響を受けることにあります. <span class="math inline"><em>μ</em><sub>1</sub></span>の事後平均が<span class="math inline"><em>μ</em><sub>2</sub></span>の事後平均に等しくなり, <span class="math inline"><em>θ</em></span>の事後平均が, データによらず常に1/2になってしまいます.</p>
<h4 id="不変の推定もある">不変の推定もある</h4>
<p>ある意味, 混合成分のインデックス（あるいはラベル）は重要ではありません. 事後予測推測は, 混合成分が識別できなくとも可能です. 例えば, 新しい観測値の対数確率は, 混合成分が識別されるかに依存しません. ラベルスイッチングが起きるモデルにおいて正しいベイズ推定値とは, ラベルスイッチングが起きても不変なものだけです. パラメータの事後平均は, ラベルスイッチングに対して不変ではないので意味がありません. 例えば, 2成分の混合分布モデルにおける<span class="math inline"><em>θ</em></span>の事後平均は常に1/2となるでしょう.</p>
<h4 id="非常に多峰な事後分布">非常に多峰な事後分布</h4>
<p>この場合には, 理論的には, 事後予測推測に含まれる積分はすべてうまくいくでしょうから, 推定の問題はないはずです. 実用上の問題は計算にあります.</p>
<p>そのような不変の推定が実用的にも実行可能であるかどうかは, まったく別の問題です. たった1つの事後最頻値を見つけるのにも難儀するのがほぼ毎度のことです. ましてや, 確率質量に従って, 複数ある局所最大値のうちの隣のものと探索のバランスをとるなど, さらに難儀なことです. ギブズサンプリングでは, <span class="math inline"><em>μ</em><sub>1</sub></span>が, 現在の<span class="math inline"><em>μ</em><sub>2</sub></span>と<span class="math inline"><em>θ</em></span>の値に条件づけられてサンプリングされているときに, 今まで到達したことがない新たな局所最頻値へと移動することはなさそうです. HMCとNUTSではその場合, 2つの局所最頻値のまわりの2つの「くぼみ」のうちの1つにサンプラーがはまってしまうことにより, ランダムな運動量の割り当てからは, 一方の局所最頻値から他方の局所最頻値へと動くための十分なエネルギーを集められないということになります.</p>
<p>正則な事後分布であっても, 混合分布モデルで成分の数が増える場合のように, 指数関数よりも速く局所最頻値の数が増える場合には, 既知のサンプリングと推定の技法のすべてが非効率的となることが知られています.</p>
<h4 id="修正のハック">修正のハック</h4>
<p>いくつかのハック（つまり「トリック」）が, ラベルスイッチングにより起こる問題への実用的な対処のために提案され, 採用されています.</p>
<h5 id="パラメータの順序の制約">パラメータの順序の制約</h5>
<p>よく使われる戦略の1つは, 成分を識別するようにパラメータに制約を課すことです. 例えば, 上で議論した2成分正規混合分布モデルに<span class="math inline"><em>μ</em><sub>1</sub> &lt; <em>μ</em><sub>2</sub></span>という制約を課すことを考えましょう. この方法では, 反対の順序<span class="math inline"><em>μ</em><sub>1</sub> &gt; <em>μ</em><sub>2</sub></span>に実質的な確率質量が存在する場合に問題が起こりえます. この場合, その制約によって事後分布が影響を受け, <span class="math inline"><em>μ</em><sub>1</sub></span>と<span class="math inline"><em>μ</em><sub>2</sub></span>における真の事後分布の不確実性は, 制約のあるモデルではとらえることができません. さらに, ある事象の生起確率を事後に推定する標準的な方法においても問題が発生します. 例えば, <span class="math inline">Pr[<em>μ</em><sub>1</sub> &gt; <em>μ</em><sub>2</sub>]</span>を推定するため<span class="math inline"><em>M</em></span>個の事後サンプルを使おうとすると失敗します. これは, 事後分布がモデルの制約を守るため, 次式の推定量による推定値が0になるからです.</p>
<p><br /><span class="math display">$$ \Pr[\mu_1 &gt; \mu_2] \approx \sum_{m=1}^{M}\mathrm{I}(\mu_1^{(m)} &gt; \mu_2^{(m)}) $$</span><br /></p>
<h5 id="単一の最頻値のまわりでの初期化">単一の最頻値のまわりでの初期化</h5>
<p>よく使われる別の方法は, 1個の連鎖で動かすか, 現実的な値の近くでパラメータを初期化することです. <a href="#fn28" class="footnoteRef" id="fnref28"><sup>28</sup></a> もっともな初期値を見つけて, マルコフ連鎖内でスイッチが起きなければ, これは, 強い制約を課す方法よりもうまくいくことがあります. 結果は, すべての連鎖が, 事後分布における特定の局所最頻値の近くに張り付きます.</p>
<h3 id="混合分布モデルで成分がつぶれる">21.3. 混合分布モデルで成分がつぶれる</h3>
<p>サンプリングまたは最適化の間に, 混合分布モデルの2つの混合成分がつぶれて同じ値になることがあります. 例えば, <span class="math inline"><em>K</em></span>個の正規分布からなる混合分布で, <span class="math inline"><em>i</em> ≠ <em>j</em></span>について, <span class="math inline"><em>μ</em><sub><em>i</em></sub> = <em>μ</em><sub><em>j</em></sub></span>かつ<span class="math inline"><em>σ</em><sub><em>i</em></sub> = <em>σ</em><sub><em>j</em></sub></span>となることがあります.</p>
<p>これは典型的には, MCMCの初期化または最適化によりサンプリングの初期に発生するか, あるいはMCMC中のランダムな移動により発生するでしょう. 与えられた抽出（<span class="math inline"><em>m</em></span>）において, いったん複数のパラメータが同じ値になってしまうと, 現在のパラメータの値と, 成分がつぶれていないときの値との間に質量の谷があるために, そこから出られなくなることがあります.</p>
<p>ウォームアップ中のステップサイズを小さくすることや, 各混合成分がどのインデックスになりやすいかを定める強い事前分布が役に立つかもしれません. もっと極端な手段は, いくつかのパラメータがつぶれる可能性を考えて, 混合成分を追加しておくことです.</p>
<p>一般に<span class="math inline"><em>K</em></span>が1よりも大きくなると, 混合分布モデルで, <span class="math inline"><em>K</em></span>個の正しい混合成分を正確に復元することはきわめて困難です. （そうです. 2成分の混合分布でさえ, この問題は起こりえます. ）</p>
<h3 id="上下限のない密度での事後分布">21.4. 上下限のない密度での事後分布</h3>
<p>場合によっては, パラメータがある極や境界に近づいて, 際限なく事後密度が大きくなるということがあります. そのようなときは, 事後最頻値はありませんし, サンプリングされたパラメータが制約の境界に近づくので, 数値的安定性の問題が起こることがあります.</p>
<h4 id="スケールが変わる混合分布モデル">スケールが変わる混合分布モデル</h4>
<p>そのような例の1つは, スケール<span class="math inline"><em>σ</em><sub>1</sub></span>と<span class="math inline"><em>σ</em><sub>2</sub></span>が成分によって変わる2項混合分布モデルです. 位置は<span class="math inline"><em>μ</em><sub>1</sub></span>と<span class="math inline"><em>μ</em><sub>2</sub></span>とします. この状況で, ある<span class="math inline"><em>n</em></span>について, <span class="math inline"><em>σ</em><sub>1</sub> → 0</span>かつ<span class="math inline"><em>μ</em><sub>1</sub> → <em>y</em><sub><em>n</em></sub></span>で, 密度が際限なく大きくなります. すなわち, 混合成分の1つの質量のすべてが, 単一のデータ項目<span class="math inline"><em>y</em><sub><em>n</em></sub></span>の周囲に集中するのです.</p>
<h4 id="ゆがみのあるデータと弱い事前分布のベータ二項モデル">ゆがみのあるデータと弱い事前分布のベータ二項モデル</h4>
<p>際限のなく大きくなる密度のもうひとつの例は, <span class="math inline">Beta(<em>ϕ</em> ∣ 0.5, 0.5)</span>のような事後分布で発生するものです. これは非常に「弱い」ベータ事前分布がデータのない群に使われているときに発生することがあります. この密度は<span class="math inline"><em>ϕ</em> → 0</span>かつ<span class="math inline"><em>ϕ</em> → 1</span>で際限なく大きくなります. 同様に, 「弱い」ベータ事前分布のベルヌーイ尤度モデルも次式のような事後分布になります.</p>
<p><br /><span class="math display">$$\begin{array}{ll} p(\phi \mid y) &amp;\propto \mathsf{Beta}(\phi \mid 0.5, 0.5) \times \prod_{n=1}^{N}\mathsf{Bernoulli}(y_n \mid \phi)\\ &amp;= \mathsf{Beta}(\phi \mid 0.5 + \sum_{n=1}^{N}y_n, 0.5 + N - \sum_{n=1}^{N}y_n) \end{array}$$</span><br /></p>
<p>もし<span class="math inline"><em>N</em> = 9</span>で, 各々の<span class="math inline"><em>y</em><sub><em>n</em></sub> = 1</span>であれば, 事後分布は<span class="math inline">Beta(<em>ϕ</em> ∣ 9.5, 0.5)</span>になります. この事後分布は<span class="math inline"><em>ϕ</em> → 1</span>で際限なく大きくなります. にもかかわらず, 事後分布は正則で, 事後最頻値がないにも関わらず, 事後平均は正確に0.95と明確に定義されます.</p>
<h5 id="制約のあるスケール-vs.-制約のないスケール">制約のあるスケール vs. 制約のないスケール</h5>
<p>この問題ではStanは, 制約のある(0, 1)空間を直接サンプルすることはしませんし, 制約のない密度の値を直接あつかうこともしません. そうではなく, 確率の値<span class="math inline"><em>ϕ</em></span>が(<span class="math inline"> − ∞</span>, <span class="math inline">∞</span>)にロジット変換されます. 境界の0と1はそれぞれ<span class="math inline"> − ∞</span>と<span class="math inline">∞</span>になります. ヤコビアンの調整はStanが自動的に行なうので, 制約なしの密度は正則であることが保証されます. (0, 1)の特定の場合の調整は<span class="math inline">log<em>l</em><em>o</em><em>g</em><em>i</em><em>t</em><sup> − 1</sup>(<em>ϕ</em>) + log(1 − <em>l</em><em>o</em><em>g</em><em>i</em><em>t</em><sup> − 1</sup>(<em>ϕ</em>))</span>です. <a href="#fn29" class="footnoteRef" id="fnref29"><sup>29</sup></a>導出は58.4節を参照してください.</p>
<p>しかし, 2つの問題がまだ出てきます. 1つ目として, <span class="math inline"><em>ϕ</em></span>の事後質量が境界の1近くにある場合には, ロジット変換されたパラメータは非常に長いパスを掃き出すしかなくなり, そのためno-U-turnサンプラーが課すU-turn条件ばかりが満たされ, うまくサンプリングできなくなることがあります. 2つ目の問題は, 制約のない空間から制約のある空間への逆変換のときに, 0にアンダーフローしたり, 1にオーバーフローしたりするおそれがあることです. これは, 制約のないパラメーターが無限でなくても発生します. 同様の問題は, ロジスティック回帰の期待値の項でも発生します. ベルヌーイ分布と2項分布ではロジットスケールのパラメータ化がより安定なのはこの理由によります.</p>
<h3 id="上下限のないパラメータでの事後分布">21.5. 上下限のないパラメータでの事後分布</h3>
<p>場合によっては, 事後密度は際限なく大きくはならないのに, 密度のとる値を次第に増加させながらパラメータが際限なく大きくなることがあります. 前の節で議論した, 密度が際限なく大きくなるモデルと同様に, そのようなモデルにも事後最頻値はありません.</p>
<h4 id="ロジスティック回帰での分離可能性">ロジスティック回帰での分離可能性</h4>
<p><span class="math inline"><em>N</em></span>個の結果変数<span class="math inline"><em>y</em><sub><em>n</em></sub> ∈ {0, 1}</span>と, <span class="math inline"><em>N</em> × <em>K</em></span>行列の予測変数<span class="math inline"><em>x</em></span>, <span class="math inline"><em>K</em></span>次元の係数ベクトル<span class="math inline"><em>β</em></span>からなるロジスティック回帰モデルを考えます. サンプリング分布は次式です.</p>
<p><br /><span class="math display"><em>y</em><sub><em>n</em></sub> ∼ Benoulli(<em>l</em><em>o</em><em>g</em><em>i</em><em>t</em><sup> − 1</sup>(<em>x</em><sub><em>n</em></sub><em>β</em>))</span><br /></p>
<p>ここで, 予測変数行列の<span class="math inline"><em>k</em></span>列が, <span class="math inline"><em>y</em><sub><em>n</em></sub> = 1</span>のときだけに<span class="math inline"><em>x</em><sub><em>n</em>, <em>k</em></sub> &gt; 0</span>となるとします. この条件は「分離可能性」として知られます. この場合, 観測データについての予測の正確さは, <span class="math inline"><em>β</em><sub><em>k</em></sub> → ∞</span>となるにつれ, 改善され続けます. <span class="math inline"><em>y</em><sub><em>n</em></sub> = 1</span>のとき, <span class="math inline"><em>x</em><sub><em>n</em></sub><em>β</em> → ∞</span>で, <span class="math inline"><em>l</em><em>o</em><em>g</em><em>i</em><em>t</em><sup> − 1</sup>(<em>x</em><sub><em>n</em></sub><em>β</em>) → 1</span>となるからです.</p>
<p>分離可能性の問題のあるときは, 尤度に最大値はありませんから, 最尤推定値もありません. ベイズの観点からいうと, 事後分布は非正則ですから, したがって<span class="math inline"><em>β</em><sub><em>k</em></sub></span>の周辺事後平均も定義されません. ベイズモデルではこの問題は通常, <span class="math inline"><em>β</em></span>に正則事前分布を与えて, 事後分布が正則であることを保証することにより解決します.</p>
<h3 id="一様な事後分布">21.6. 一様な事後分布</h3>
<p>区間[0, 1]で定義され, 一様事前分布<span class="math inline">Uniform(<em>ψ</em> ∣ 0, 1)</span>が与えられたパラメータ<span class="math inline"><em>ψ</em></span>を含むモデルがあるとします. ここで, データには<span class="math inline"><em>ψ</em></span>についての情報が何もないとすれば, 事後分布も<span class="math inline">Uniform(<em>ψ</em> ∣ 0, 1)</span>となります.</p>
<p><span class="math inline"><em>ψ</em></span>には最尤推定値がありませんが, 事後分布は閉じた区間で一様ですから, 正則です. 一様な事後分布[0, 1]の場合, <span class="math inline"><em>ψ</em></span>の事後平均は1/2と明確に決まります. 事後最頻値はありませんが, それにもかかわらず事後予測の推定は問題なくできるでしょう. <span class="math inline"><em>ψ</em></span>の予測値を[0, 1]のすべての点で単純に積分（すなわち平均）すればよいのです.</p>
<h3 id="問題のある事前分布によるサンプリングの難しさ">21.7. 問題のある事前分布によるサンプリングの難しさ</h3>
<p>非正則な事後分布では, 事後分布を適切に探索することは理論的には不可能です. BUGSやJAGSで実行されるギブズサンプリングでも, そのような非正則な事後分布から適切にサンプリングすることは不可能です. しかし, それにも関わらず, 21.1節で議論し, 図21.1で図示した2切片モデルのような例に直面したとき, ギブズサンプリングは, Stanで実行されるハミルトニアンモンテカルロのサンプリングとは実際はまったく異なる挙動を示します.</p>
<h4 id="ギブズサンプリング">ギブズサンプリング</h4>
<p>BUGSやJAGSで実行されるギブズサンプリングは, この識別されないモデルについて効率的で, うまく動作するように見えるかもしれませんが, 前の小節で議論したように, 実際には事後分布を適切には探索しないでしょう.</p>
<p>初期値<span class="math inline"><em>λ</em><sub>1</sub><sup>(0)</sup></span>, <span class="math inline"><em>λ</em><sub>2</sub><sup>(0)</sup></span>で起こることを考えましょう. ギブズサンプリングはiteration <span class="math inline"><em>m</em></span>で以下の抽出を行なって進行します.</p>
<p><br /><span class="math display">$$\begin{array}{ll} \lambda_1^{(m)} &amp;\sim p(\lambda_1 \mid \lambda_2^{(m-1)}, \sigma^{(m-1)},y)\\ \lambda_2^{(m)} &amp;\sim p(\lambda_2 \mid \lambda_1^{(m)}, \sigma^{(m-1)},y)\\ \sigma^{(m)} &amp;\sim p(\sigma \mid \lambda_1^{(m-1)}, \lambda_2^{(m)}, y) \end{array}$$</span><br /></p>
<p>ここで, <span class="math inline"><em>λ</em><sub>1</sub></span>（<span class="math inline"><em>λ</em><sub>2</sub></span>の抽出も対称です）の抽出を考えます. このモデルでは共役であり, したがって非常に効率的に抽出ができます. このモデルでは, 次の<span class="math inline"><em>λ</em><sub>1</sub></span>が抽出可能な範囲は, 現在の<span class="math inline"><em>λ</em><sub>2</sub></span>と<span class="math inline"><em>σ</em></span>の値により非常に制限されています. ギブズサンプラーは非常に速く動き, 外見上はもっともらしい<span class="math inline"><em>λ</em><sub>1</sub> + <em>λ</em><sub>2</sub></span>の推定値を与えるでしょう. しかし, 事後分布の範囲全体を探索はしません. 初期値付近をゆっくりランダムウォークしているに過ぎないのです. このランダムウォークの挙動は, 事後分布の相関が非常に大きいときのギブズサンプリングに典型的です. これが, 事後分布でパラメータが相関を持つモデルでは, ギブズサンプリングよりもハミルトニアンモンテカルロが好まれる主要な理由です.</p>
<h4 id="ハミルトニアンモンテカルロのサンプリング">ハミルトニアンモンテカルロのサンプリング</h4>
<p>Stanが実行するハミルトニアンモンテカルロ(HMC)は, 事後分布でパラメータに相関があるモデルでの事後分布の探索をはるかに効率的に行ないます. とくにこの例では, ハミルトニアンダイナミクス（すなわち, 負の対数事後分布により定義される場において, ランダムな運動量で与えられる, 架空粒子の運動）はポテンシャルエネルギーにより定義される谷（対数事後確率における尾根が, ポテンシャルエネルギーにおける谷に相当します）に沿って登ったり降りたりします. 実際には, <span class="math inline"><em>λ</em><sub>1</sub></span>と<span class="math inline"><em>λ</em><sub>2</sub></span>のランダムな運動量についてさえも, 対数事後確率の勾配により, 相関について調整が行なわれ, シミュレーションでは<span class="math inline"><em>λ</em><sub>1</sub></span>と<span class="math inline"><em>λ</em><sub>2</sub></span>が, 事後対数密度の尾根に相当する谷に沿って反対方向に動かされるでしょう（図21.1参照）.</p>
<h4 id="no-u-turnサンプリング">No-U-Turnサンプリング</h4>
<p>Stanのデフォルトのno-U-turnサンプラー(NUTS)はさらにもっと効率的に事後分布を探索します（Hoffman and Gelman (2011, 2014)を参照）. NUTSは, パラメータの値を表す架空粒子の運動を, それがUターンするまでシミュレートします. 問題のある事後分布からサンプリングする場合, Uターンしないでポテンシャルエネルギーの谷をいつまでも降下することになり, ほとんどの場合うまくいきません. 実際に起きることは, シミュレーションの多くのiterationでリープフロッグの最大ステップ数に到達し, 対数確率と勾配の評価回数（デフォルトのように最大tree depthが10ならば, 1000となります）が非常に大きな数となることです. したがって, サンプリングは非常に遅いように見えるでしょう. これは非正則事後分布を示すもので, NUTSアルゴリズムや実装のバグではありません. 単純に, 非正則な事後分布からサンプリングすることは不可能なのです. したがって, 非正則な事後分布の場合に明確に失敗し, 事後分布の非常に長いパスを掃き出すという分かりやすい結果を得ることは, 一般的なHMCと特定のNUTSの挙動としては当然で安心できるものです.</p>
<h4 id="例-stanでのあてはめ">例: Stanでのあてはめ</h4>
<p>識別不可能なモデルや弱くしか識別されないモデルからのサンプリングの問題を描写するため, パラメータの識別可能性の程度を増やしつつ, 3つのモデルに当てはめを行ないます. これらのモデルの事後分布は図21.1に図示したものです. 最初のモデルは, 21.1節で議論した, 2つの位置パラメータがあり, 事前分布はない識別されないモデルです.</p>
<pre><code>data {
  int N;
  real y[N];
}
parameters {
  real lambda1;
  real lambda2;
  real&lt;lower=0&gt; sigma;
}
transformed parameters {
  real mu;
  mu = lambda1 + lambda2;
}
model {
  y ~ normal(mu, sigma);
}</code></pre>
<p>2番目のモデルは, 前のモデルの<code>model</code>ブロックに, <code>lambda1</code>と<code>lambda2</code>の事前分布を加えたものです.</p>
<pre><code>  lambda1 ~ normal(0, 10);
  lambda2 ~ normal(0, 10);</code></pre>
<p>3番目は, 単一の位置パラメータを持ちますが, 事前分布は指定しません.</p>
<pre><code>data {
  int N;
  real y[N];
}
parameters {
  real mu;
  real&lt;lower=0&gt; sigma;
}
model {
  y ~ normal(mu, sigma);
}</code></pre>
<h5 id="個のスケールパラメータ-非正則事前分布">2個のスケールパラメータ, 非正則事前分布</h5>
<pre><code>Inference for Stan model: improper_stan
Warmup took (2.7, 2.6, 2.9, 2.9) seconds, 11 seconds total
Sampling took (3.4, 3.7, 3.6, 3.4) seconds, 14 seconds total
                  Mean     MCSE   StdDev        5%       95%  N_Eff  N_Eff/s  R_hat
lp__          -5.3e+01  7.0e-02  8.5e-01  -5.5e+01  -5.3e+01    150       11    1.0
n_leapfrog__   1.4e+03  1.7e+01  9.2e+02   3.0e+00   2.0e+03   2987      212    1.0
lambda1        1.3e+03  1.9e+03  2.7e+03  -2.3e+03   6.0e+03    2.1     0.15    5.2
lambda2       -1.3e+03  1.9e+03  2.7e+03  -6.0e+03   2.3e+03    2.1     0.15    5.2
sigma          1.0e+00  8.5e-03  6.2e-02   9.5e-01   1.2e+00     54      3.9    1.1
mu             1.6e-01  1.9e-03  1.0e-01  -8.3e-03   3.3e-01   2966      211    1.0</code></pre>
<h5 id="個のスケールパラメータ-弱情報事前分布">2個のスケールパラメータ, 弱情報事前分布</h5>
<pre><code>Warmup took (0.40, 0.44, 0.40, 0.36) seconds, 1.6 seconds total
Sampling took (0.47, 0.40, 0.47, 0.39) seconds, 1.7 seconds total
                 Mean     MCSE   StdDev        5%    95%  N_Eff  N_Eff/s  R_hat
lp__              -54  4.9e-02  1.3e+00  -5.7e+01    -53    728      421    1.0
n_leapfrog__      157  2.8e+00  1.5e+02   3.0e+00    511   3085     1784    1.0
lambda1          0.31  2.8e-01  7.1e+00  -1.2e+01     12    638      369    1.0
lambda2         -0.14  2.8e-01  7.1e+00  -1.2e+01     12    638      369    1.0
sigma             1.0  2.6e-03  8.0e-02   9.2e-01    1.2    939      543    1.0
mu               0.16  1.8e-03  1.0e-01  -8.1e-03   0.33   3289     1902    1.0</code></pre>
<h5 id="個のスケールパラメータ-非正則事前分布-1">1個のスケールパラメータ, 非正則事前分布</h5>
<pre><code>Warmup took (0.011, 0.012, 0.011, 0.011) seconds, 0.044 seconds total
Sampling took (0.017, 0.020, 0.020, 0.019) seconds, 0.077 seconds total
                Mean     MCSE  StdDev        5%   50%   95%  N_Eff  N_Eff/s  R_hat
lp__             -54  2.5e-02    0.91  -5.5e+01   -53   -53   1318    17198    1.0
n_leapfrog__     3.2  2.7e-01     1.7   1.0e+00   3.0   7.0     39      507    1.0
mu              0.17  2.1e-03    0.10  -3.8e-03  0.17  0.33   2408    31417    1.0
sigma            1.0  1.6e-03   0.071   9.3e-01   1.0   1.2   2094    27321    1.0</code></pre>
<p>図21.2: <span class="math inline"><em>y</em><sub><em>n</em></sub> ∼ <em>N</em><em>o</em><em>r</em><em>m</em><em>a</em><em>l</em>(0, 1)</span>から生成した100データ点を, デフォルトのパラメータを使ってStanであてはめを行なった結果. 上段は, 非正則一様事前分布と尤度<span class="math inline"><em>y</em><sub><em>n</em></sub> ∼ <em>N</em><em>o</em><em>r</em><em>m</em><em>a</em><em>l</em>(<em>λ</em><sub>1</sub> + <em>λ</em><sub>2</sub>, <em>σ</em>)</span>を使った識別できないモデル. 中段は, 上段<a href="#fn30" class="footnoteRef" id="fnref30"><sup>30</sup></a>と同じ尤度で, 事前分布を<span class="math inline"><em>λ</em><sub><em>k</em></sub> ∼ <em>N</em><em>o</em><em>r</em><em>m</em><em>a</em><em>l</em>(0, 10)</span>としたもの. 下段は, 尤度を<span class="math inline"><em>y</em><sub><em>n</em></sub> ∼ Normal(<em>μ</em>, <em>σ</em>)</span>とした, 識別できるモデル. すべてのモデルで, <span class="math inline"><em>μ</em></span>はおよそ0.16と推定され, モンテカルロ標準誤差は非常に小さいのですが, 事後標準偏差は0.1と大きくなっています. 真の値<span class="math inline"><em>μ</em> = 0</span>は, 3つのモデルすべてで事後分布の90%区間に含まれています.</p>
<p>3つの例すべてについて, Stan 2.1.0でデフォルトのパラメータ（1000ウォームアップiteration, 1000サンプリングiteration, 最大tree depthが10のNUTSサンプラー）を用いて当てはめを行ないました. 結果は図21.2に示します. この出力から分かる主要な統計量は以下のとおりです.</p>
<ul>
<li><code>R_hat</code>の列でわかるように, 識別されないモデルでは<span class="math inline"><em>λ</em><sub>1</sub></span>と<span class="math inline"><em>λ</em><sub>2</sub></span>以外のパラメータはすべて収束しました.</li>
<li>リープフロッグの平均ステップ数は, 識別されるモデルではおよそ3, 弱い事前分布で識別されるモデルでは150, 識別されないモデルでは1400です.</li>
<li><span class="math inline"><em>μ</em></span>の秒当たりの有効サンプルの数は, 識別されるモデルではおおよそ31,000ですが, 弱情報事前分布で識別されるモデルでは1900, 識別されないモデルでは200です. <span class="math inline"><em>σ</em></span>の結果も同様です.</li>
<li>識別されないモデルでは, <span class="math inline"><em>λ</em><sub>1</sub></span>の95%区間は(-2300, 6000)でした. 一方, 弱情報事前分布で識別されるモデルではわずか(-12, 12)でした.</li>
<li>3つのモデルすべてで, <span class="math inline"><em>μ</em> = 0</span>と<span class="math inline"><em>σ</em> = 1</span>のシミュレートされた値は, うまく事後分布の90%区間に収まっています.</li>
</ul>
<p>最初の2点, 収束の欠如と, リープフロッグの最大ステップ数（最大tree depthも同じ）到達は, 非正則事後分布を示すものです. ギブズサンプラーで行なわれるような貧弱なサンプリングで問題を覆い隠すのではなく, ハミルトニアンモンテカルロは事後分布を探索しようとしますし, それが失敗するなら, モデルのどこかがおかしいことを明確に示しています.</p>
<h2 id="再現性reproducibilityについて">23.再現性(Reproducibility)について</h2>
<p>現代のコンピュータにおける浮動小数点演算は, IEEE 754に準拠した基礎的な数値演算が完全には規定されていないため, 追試が難しいことで有名です. 根本的な問題点は演算の精度がハードウェアプラットフォームやソフトウェアの実装により異なっていることにあります. Stanは完全な再現性を許容すべく設計されています. しかしながら, それはあくまで浮動小数点計算により課せられた外的制約により左右されます.</p>
<p>Stanの結果は以下のすべての要素が一致しているときにのみ厳密に再現可能となります：</p>
<ul>
<li>Stanのバージョン</li>
<li>Stanのインターフェイス(Rstan, PyStan, CmdStan) およびそのバージョン, さらにインターフェイス言語(R, Python, shell)のバージョン</li>
<li>インクルードされたライブラリのバージョン(BoostおよびEigen)</li>
<li>OSのバージョン</li>
<li>CPU, マザーボード, メモリを含むコンピュータのハードウェア</li>
<li>C++コンパイラのバージョン, コンパイル時のフラグ, リンクされたライブラリ</li>
<li>乱数の種, チェーンのID, 初期化およびデータを含むStan呼び出し時の設定</li>
</ul>
<p>これはStanの安定リリースを使っているか, 特定の Git ハッシュタグを持つバージョンを使っているかには関係ありません. インターフェイスやコンパイラなどについても同様です. 重要なのはもしこれらのどれか一つでも何らかの違いがあれば, 浮動小数点計算の結果は変わる可能性があるということです.</p>
<p>具体的には, もしあるStanプログラムをCmdStanでコンパイルするときに, 最適化フラグを(-O3 から -O2 または -O0へ)変更した場合, 変更前と変更後の一連の結果は必ずしも一致しません. このため, クラスターや, IT部門に管理されている, もしくは自動更新がONになっているデスクトップのように外部に管理されたハードウェア上で再現性を保証するのは極めて困難です.</p>
<p>しかしながら, もしStanプログラムを一組のフラグを使ってコンパイルし, そのコンピュータをインターネットから取り外して一切アップデートしないようにし, 10年後に戻ってきて同じように再コンパイルした場合, 同じ結果が得られます.</p>
<p>データについてもビットレベルで同じである必要があります. 例えば, もしRStanであればRcppがRの浮動小数点数とC++の倍精度小数の間の変換を行います. もしRcppが変換のプロセスを変更したり異なる型を使うと, 結果はビットレベルで同じであることは保証されません.</p>
<p>コンパイラとコンパイラの設定も同じ問題を起こす可能性があります. インテル製のプロプライエタリなコンパイラで再現性をいかにコントロールするかについての素敵な議論はCoden and Kreirzer(2014)を読んでください.</p>
<h2 id="ステートメント-文">27. ステートメント (文)</h2>
<!--
statement を "文" とした場合, 以降説明にて何を指しているか
読み取りにくくなるためカタカナとした
-->
<p>Stan プログラムのブロック ( 28章を参照 ) は, 変数の宣言とステートメント (文) から成り立っています. BUGS と違って, Stan プログラム中の宣言とステートメントはそれらが 記載された順序で実行されます. 変数は, それが参照される前に何らかの値 (と何らかのデータ型の宣言) によって定義されていなければいけません --- そうでない場合, 実行結果は未定義となります.</p>
<p>BUGS と同じく, Stan の基本的なステートメントには代入とサンプリングの 2 種類があります. また, いくつかのステートメントは 連続した処理やfor-each ループの繰り返し処理にグルーピング できるでしょう. 加えて, Stan では ブロック内でのローカル変数の宣言や, セミコロンのみからなる 空のステートメントも許容されています.</p>
<h3 id="代入ステートメント">27.1. 代入ステートメント</h3>
<p>代入ステートメントは変数と式からなります. 変数はインデックス情報を持つ多変量の場合もあります. 代入ステートメントが実行されると, ステートメントの右辺にある式が評価され, その結果が左辺の変数 (インデックスがある場合は指定された箇所) に代入されます. 簡単な代入の例は以下のようなものです. <sup>1</sup></p>
<!--
以降も原文には都度 (indexed) の記載があるが, 日本語で簡潔に記載できる語がないため省略
-->
<pre><code>n = 0;</code></pre>
<p>このステートメントが実行されると, 式 <code>0</code>, つまり 整数のゼロが評価され, 変数 <code>n</code> に代入 されます. 代入が成立するためには, 右辺の式のデータ型と左辺の変数のデータ型は一致していな ければなりません. 上の例では, <code>0</code> という式は <code>int</code> 型になるため, 変数 <code>n</code> は <code>int</code> 型もしくは <code>real</code> 型で宣言されていなければなりません. 変数が <code>real</code> 型で宣言されている場合は, 整数のゼロは浮動小数点のゼロに変換されて 変数に代入されます. ステートメントが実行された後は, 変数 <code>n</code> はゼロ (変数のデータ型に応じて整数 もしくは浮動小数点) という値を持ちます.</p>
<p><sup>1</sup>Stan の 2.10.0 より前のバージョンでは, 代入には等号 <code>=</code>ではなく <code>&lt;-</code> 演算子が用いられていました. <code>&lt;-</code> 演算子は現在では非推奨となり警告が表示されます. <code>&lt;-</code> 演算子は将来のバージョンで削除されます.</p>
<p>文法的には, すべての代入ステートメントはセミコロンで終わっていなければなりません. それ以外では, トークンの間の空白は処理に影響しません (ここでのトークンは左辺の変数, 代入演算子, 右辺の式, セミコロンを指します).</p>
<p>右辺の式が最初に評価されるため, Stan でも C++ や他のプログラミング言語と同じように 変数を変数をインクリメントすることができます.</p>
<pre><code>n = n + 1;</code></pre>
<p>このような自己代入は BUGS では許されていません. なぜなら, 自己代入は 有向グラフィカルモデル中で循環を引き起こすからです.</p>
<p>代入ステートメントの左辺は<code>array</code>, <code>matrix</code>, <code>vector</code>といったデータ構造に対するインデックスを 含むことがあります. 例えば, <code>matrix</code>として定義された <code>Sigma</code> に対して,</p>
<pre><code>Sigma[1,1] = 1.0;</code></pre>
<p>という代入ステートメントは, <code>Sigma</code> の 1 行 1 列目の値に <code>1</code> を代入します.</p>
<p>代入ステートメントにはあらゆるデータ型の複雑なオブジェクトを含むことができます. <code>Sigma</code> と <code>Omega</code> が<code>matrix</code>, <code>sigma</code> が <code>vector</code>の場合, 次の代入ステートメントは 成立します. 変数のデータ型と式の結果はどちらも <code>matrix</code> 型になるためです.</p>
<pre><code>Sigma
  = diag_matrix(sigma)
     * Omega
     * diag_matrix(sigma);</code></pre>
<p>また, この例は複雑な代入ステートメントを複数の行に分割して記載する場合に望ましい例を 示しています.</p>
<p>Stan は より大きな多変量のデータ構造の一部に対する代入もサポートしています. 例えば, <code>a</code> が <code>real[ , ]</code> 型の<code>array</code>, <code>b</code> が <code>real[]</code> 型の<code>array</code>の場合, 以下二つの 代入ステートメントはどちらも成立します.</p>
<pre><code>a[3] = b;
b = a[4];</code></pre>
<p>同じように, <code>x</code> が <code>row_vector</code> 型, <code>Y</code> が <code>matrix</code> 型の変数として 宣言されている場合, 次の一連の処理は成立します. 処理の結果, <code>Y</code> の最初の 2 行が 入れ替わります.</p>
<pre><code>x = Y[1];
Y[1] = Y[2];
Y[2] = x;</code></pre>
<h4 id="左辺値">左辺値</h4>
<p>代入ステートメント中の左辺として適切な式を &quot;左辺値&quot; と呼びます. Stan では, 適切な左辺値はこの 2 種類しかありません.</p>
<ul>
<li>一つの変数</li>
<li>一つ以上のインデックスを持つ一つの変数</li>
</ul>
<p>インデックス指定された変数を左辺値として使うためには, その変数は少なくともインデックスと同じ数の 次元を持っていなければいけません. 実数や整数の<code>array</code>は宣言された数の次元を持ちます. <code>matrix</code>は 2 次元, <code>vector</code>や<code>row_vector</code>は 1 次元です. これは分散共分散行列 ( <code>cov_matrix</code>) や 相関行列 ( <code>corr_matrix</code> ), またそれらのコレスキー因子 ( <code>cholesky_factor_cov</code>, <code>cholesky_factor_corr</code> )や 昇順のベクトル ( <code>orderd</code> ), 昇順で正のベクトル ( <code>positive_ordered</code> ), 各要素が <code>[0, 1]</code>で合計が<code>1</code>となるベクトル (<code>simplex</code> ) といった 制約付きデータ型についてもあてはまります. 通常の<code>array</code>の次元と比べて, <code>matrix</code>の<code>array</code>の次元は 2 多くなり, <code>vector</code>や<code>row_vector</code>の<code>array</code>の次元は 1 多くなります. 左辺値のインデックスの数は左辺の変数の次元よりも 少なくてよいことに注意してください. この時, 右辺は多次元で左辺値の次元と一致している必要があります.</p>
<!--
日本語訳した場合にデータ型との対応がわからなくなるため型を追記.
Multiple Indexes の内容は上と重複 (原文の誤植?)のため省略.
-->
<h4 id="エイリアス">エイリアス</h4>
<p>すべての代入は, 事前に右辺の式のをコピーしたかのようにして行われます. これにより, 「代入ステートメントの実行中に右辺の式の値が変更される」ことによって生じる, エイリアスの潜在的な問題を解決しています.</p>
<h2 id="array型の演算">34. Array型の演算</h2>
<h3 id="arrayから1つの値の作成">34.1 Arrayから1つの値の作成</h3>
<p>次の演算子はarrayをインプットとしてとり, 一つの値をアウトプットとして返すものです. 　<strong><code>The　boundary values for size 0 arrays are the unit with respect to the combination operation (min, max, sum, or product).</code></strong></p>
<h4 id="最小最大">最小・最大</h4>
<pre class="text"><code>real min(real x[])</code></pre>
<p><code>x</code>の中の最小値を返す. ただし, <code>x</code>のサイズが0の時は+∞を返す.</p>
<pre class="text"><code>int min(int x[])</code></pre>
<p><code>x</code>の中の最小値を返す. ただし, <code>x</code>のサイズが0の時はerrorを返す.</p>
<pre class="text"><code>real max(real x[])</code></pre>
<p><code>x</code>の中の最大値を返す. ただし, <code>x</code>のサイズが0の時は-∞を返す.</p>
<pre class="text"><code>int max(int x[])</code></pre>
<p><code>x</code>の中の最大値を返す. ただし, <code>x</code>のサイズが0の時はerrorを返す.</p>
<h4 id="総和総乗log-sum-of-exp">総和・総乗・Log Sum of Exp</h4>
<pre class="text"><code>int sum(int x[])</code></pre>
<p><code>x</code>の要素の総和を返す. ただし, <code>x</code>のsize<code>N</code>によっては以下のように返す. 　　</p>
<p><br /><span class="math display">$$\mbox{\tt sum}(x) = \left\{\begin{array}{ll} \sum_{n=1}^{N}x_{n} &amp; \mbox{if} N &gt; 0 \\ 0 &amp; \mbox{if} N = 0 \end{array}\right.$$</span><br /></p>
<pre class="text"><code>real sum(real x[])</code></pre>
<p><code>x</code>の要素の総和を返す. 上の定義を参照.</p>
<pre class="text"><code>real prod(real x[])</code></pre>
<p><code>x</code>の要素の総乗を返す. ただし, <code>x</code>のsizeが0の時は1を返す.</p>
<pre class="text"><code>real prod(int x[])</code></pre>
<p><code>x</code>の要素の総乗を返す. 　　</p>
<p><br /><span class="math display">$$\mbox{\tt product}(x) = \left\{\begin{array}{ll} \prod_{n=1}^{N}x_{n} &amp; \mbox{if} N &gt; 0 \\ 1 &amp; \mbox{if} N = 0 \end{array}\right.$$</span><br /></p>
<pre class="text"><code>real log_sum_exp(real x[])</code></pre>
<p><code>x</code>の各要素のexpをとったものの総和の自然対数を返す. ただし, arrayが空の時は-∞を返す.</p>
<h4 id="標本平均標本分散標本標準偏差">標本平均・標本分散・標本標準偏差</h4>
<h2 id="点推定">55. 点推定</h2>
<p>この章では, ベイズ推定ではありませんが, 最尤推定と罰則付き最尤推定というよく使われる方法を定義し, 事後分布の平均値・中央値・最頻値を使って, それらをベイズ点推定に関連づけます. 最尤推定値は, 事後分布ではなく, モデルのパラメーター<span class="math inline"><em>θ</em></span>についての単一の値から求められるので, 「点推定値」と呼ばれます.</p>
<p>どのような尤度関数も罰則関数もStanのモデリング言語でコーディングすることができるので, Stanのオプティマイザ（optimizer）を使って（罰則付き）最尤推定を実装することができます. Stanのオプティマイザはまた, 事後最頻値に基づくベイズの枠組みでの点推定にも使うことができます. Stanのマルコフ連鎖モンテカルロサンプラーは, 事後分布の平均値や中央値に基づくベイズモデルでの点推定を実装するのにも使えます.</p>
<h3 id="最尤推定">55.1. 最尤推定</h3>
<p>尤度関数<span class="math inline"><em>p</em>(<em>y</em> ∣ <em>θ</em>)</span>と, 固定されたデータのベクトル<span class="math inline"><em>y</em></span>とが与えられたとき, 尤度を最大にするようなパラメータのベクトル<span class="math inline">$\hat{\theta}$</span>のことを最尤推定値(maximum likelihood estimate, MLE)といいます.</p>
<p><br /><span class="math display">$$\hat{\theta}=\mathrm{argmax}_{\theta}\ p(y\mid\theta)$$</span><br /></p>
<p>通常は対数スケールにするほうが便利です. 次式も, MLEの定式化として等価です. <sup>1</sup></p>
<p><br /><span class="math display">$$\hat{\theta}=\mathrm{argmax}_{\theta}\ \log p(y\mid\theta)$$</span><br /></p>
<p><sup>1</sup> この等価性は, 密度が正値であり, 対数関数が厳密に単調であるという事実から導かれます. すなわち, <span class="math inline"><em>p</em>(<em>y</em> ∣ <em>θ</em>) ≥ 0</span>, かつ, すべての<span class="math inline"><em>a</em>, <em>b</em> &gt; 0</span>について, <span class="math inline"><em>a</em> &gt; <em>b</em></span>のときのみ<span class="math inline">log<em>a</em> &gt; log<em>b</em></span>です.</p>
<h4 id="最尤推定値の存在">最尤推定値の存在</h4>
<p>すべての関数がただひとつの最大値を持っているとは限りませんので, 最尤推定値が存在することは保証されるものではありません. 20章で議論したように, この状況は以下のようなときに起こります.</p>
<ul>
<li>2つ以上の点で尤度関数が最大になるとき</li>
<li>尤度関数が発散するとき</li>
<li>尤度関数が有限のパラメータの値では達しない漸近線を持ち, 有界となるとき</li>
</ul>
<p>こうした問題は, 次の節で議論する罰則付き最尤推定でも, その後の節で議論するベイズ事後最頻値でもついて回ります.</p>
<h4 id="例-線形回帰">例: 線形回帰</h4>
<p>通常の線形回帰の問題を考えます. 観測値<span class="math inline"><em>y</em></span>が<span class="math inline"><em>N</em></span>次元のベクトル, 予測変数が<span class="math inline">(<em>N</em> × <em>K</em>)</span>次元のデータ行列<span class="math inline"><em>x</em></span>, 回帰係数が<span class="math inline"><em>K</em></span>次元のパラメータのベクトル<span class="math inline"><em>β</em></span>, 実数値のノイズのスケールが<span class="math inline"><em>σ</em> &gt; 0</span>のとき, 尤度関数は次式のようになります.</p>
<p><br /><span class="math display">$$\log p(y\mid\beta,x)=\sum_{n=1}^{N}\log\mathsf{Normal}(y_n \mid x_n \beta, \sigma)$$</span><br /></p>
<p><span class="math inline"><em>θ</em> = (<em>β</em>, <em>σ</em>)</span>の最尤推定値は次式のようになります.</p>
<p><br /><span class="math display">$$(\hat{\beta},\hat{\sigma})=\mathrm{argmax}_{\beta,\sigma}\log p(y\mid\beta,\sigma,x) = \mathrm{argmax}_{\beta,\sigma}\sum_{n=1}^{N} \log \mathsf{Normal}(y_n \mid x_n \beta,\sigma)$$</span><br /></p>
<p>（<strong>訳注</strong>: 原文では最右辺でargmaxが抜けている）</p>
<h5 id="二乗誤差">二乗誤差</h5>
<p>対数尤度関数について少し代数計算をすると, 周辺最尤推定値<span class="math inline">$\hat{\theta}=(\hat{\beta},\hat{\sigma})$</span>が, 最小二乗法で求めた<span class="math inline">$\hat{\beta}$</span>と等価に定式化できることが分かります. すなわち, <span class="math inline">$\hat{\beta}$</span>は, 二乗予測誤差の和を最小化するような係数ベクトルの値です.</p>
<p><br /><span class="math display">$$\hat{\beta}=\mathrm{argmin}_{\beta}\sum_{n=1}^{N}(y_{n}-x_{n}\beta)^2=\mathrm{argmin}_{\beta}(y-x\beta)^{\top}(y-x\beta)$$</span><br /></p>
<p><span class="math inline"><em>n</em></span>番目のデータの残差誤差は, 実際の値と予測値との差<span class="math inline">$y_n - x_n \hat{\beta}$</span>です. ノイズのスケールの最尤推定値<span class="math inline">$\hat{\sigma}$</span>は, 平均二乗残差の平方根とちょうど同じです.</p>
<p><br /><span class="math display">$$\hat{\sigma}^2=\frac{1}{N}\sum_{n=1}^{N}\left(y_{n}-x_{n}\hat{\beta}\right)^2=\frac{1}{N}(y-x\hat{\beta})^{\top}(y-x\hat{\beta})$$</span><br /></p>
<h5 id="stanでの二乗誤差の最小化">Stanでの二乗誤差の最小化</h5>
<p>線形回帰において二乗誤差を最小にするアプローチはStanでは以下のモデルで直接コーディングできます.</p>
<pre><code>data {
  int&lt;lower=0&gt; N;
  int&lt;lower=1&gt; K;
  vector[N] y;
  matrix[N,K] x;
}
parameters {
  vector[K] beta;
}
transformed parameters {
  real&lt;lower=0&gt; squared_error;
  squared_error &lt;- dot_self(y - x * beta);
}
model {
  increment_log_prob(-squared_error);
}
generated quantities {
  real&lt;lower=0&gt; sigma_squared;
  sigma_squared &lt;- squared_error / N;
}</code></pre>
<p>このモデルをStanのオプティマイザで走らせると, 二乗誤差の和を直接最小化するとともに, その値を使って生成量としてノイズのスケールを定義することで, 線形回帰のMLEが生成されます.</p>
<p><code>sigma_squared</code>の定義にある分母の<code>N</code>を<code>N-1</code>に変えると, より一般的に用いられる<span class="math inline"><em>σ</em><sup>2</sup></span>の不偏推定量が計算できます. 推定の偏りの定義と, 分散の推定についての議論については53.6節を参照してください.</p>
<h3 id="罰則付き最尤推定">55.2. 罰則付き最尤推定</h3>
<p>最適化を行なう能力に関する限り, 尤度関数については特別なことはありません. 非ベイズ統計では普通に行なわれますが, 対数尤度に「罰則」関数と呼ばれる関数を加えて, その新しく定義した関数を最適化する方法があります. 対数尤度関数<span class="math inline">log<em>p</em>(<em>y</em> ∣ <em>θ</em>)</span>と罰則関数<span class="math inline"><em>r</em>(<em>θ</em>)</span>からなる罰則付き最尤推定量は次式のように定義されます.</p>
<p><br /><span class="math display">$$\hat{\theta}=\mathrm{argmax}_{\theta}\log p(y\mid\theta)-r(\theta)$$</span><br /></p>
<p>最大化のとき, 推定値<span class="math inline">$\hat{\theta}$</span>は, 対数尤度の最大化と罰則の最小化との間でつりあいをとるようになります. 罰則をつけることは「正則化」とも呼ばれます.</p>
<h4 id="例-1">例</h4>
<h5 id="リッジ回帰">リッジ回帰</h5>
<p>リッジ回帰(Hoerl and Kennard, 1970)の基礎は, 係数ベクトル<span class="math inline"><em>β</em></span>のユークリッド長さ（<strong>訳注</strong>: 二乗のこと）に罰則をつけるところにあります. 次式がリッジ罰則関数です.</p>
<p><br /><span class="math display">$$r(\beta)=\lambda\sum_{k=1}^{K}\beta_{k}^2=\lambda\beta^{\top}\beta$$</span><br /></p>
<p>ここで<span class="math inline"><em>λ</em></span>は, 罰則の大きさを決める固定したチューニングパラメータです.</p>
<p>したがって, リッジ回帰の罰則付き最尤推定値は次式のとおりです.</p>
<p><br /><span class="math display">$$(\hat{\beta},\hat{\sigma})=\mathrm{argmax}_{\beta,\sigma}\sum_{n=1}^{N}\log\mathsf{Normal}(y_{n}\mid x_{n}\beta,\sigma)-\lambda\sum_{k=1}^{K}\beta_{k}^2$$</span><br /></p>
<p>リッジ罰則は, L2ノルムとの関連から, L2正則化あるいは縮小と呼ばれることもあります.</p>
<p>基本的なMLEと同様, 係数<span class="math inline"><em>β</em></span>についてのリッジ回帰の推定値は最小二乗法として定式化することもできます.</p>
<p><br /><span class="math display">$$\hat{\beta}=\mathrm{argmin}_{\beta}\sum_{n=1}^{N}(y_{n}-x_{n}\beta)^2+\sum_{k=1}^{K}\beta_{k}^2=\mathrm{argmin}_{\beta}(y-x\beta)^{\top}(y-x\beta)+\lambda\beta^{\top}\beta$$</span><br /></p>
<p>リッジ罰則関数を加えると, <span class="math inline"><em>β</em></span>についてのリッジ回帰の推定値がより短いベクトルとなる効果があります. すなわち<span class="math inline">$\hat{\beta}$</span>が縮小されます. リッジ推定値は必ずしもすべての<span class="math inline"><em>k</em></span>について<span class="math inline"><em>β</em><sub><em>k</em></sub></span>の絶対値が小さくなるとは限りませんし, 係数ベクトルが, 最尤推定値と同じ方向を指すとも限りません.</p>
<p>Stanでリッジ罰則を加えるには, 罰則の大きさをデータ変数として加え, 罰則自身を<code>model</code>ブロックに加えます.</p>
<pre><code>data {
  // ...
  real&lt;lower=0&gt; lambda;
}
// ...
model {
  // ...
  increment_log_prob(- lambda * dot_self(beta));
}</code></pre>
<p>ノイズ項の計算はそのままです.</p>
<h5 id="lasso">Lasso</h5>
<p>Lasso (Tibshirani, 1966)はリッジ回帰の代替法で, 係数の二乗和ではなく, 係数の絶対値和に基づいて罰則を適用します.</p>
<p><br /><span class="math display">$$r(\beta)=\lambda\sum_{k=1}^{K}|\beta_{k}|$$</span><br /></p>
<p>Lassoは, L1ノルムとの関連から, L1縮小とも呼ばれます. L1ノルムは, タクシー距離あるいはマンハッタン距離としても知られています.</p>
<p>罰則の導関数は<span class="math inline"><em>β</em><sub><em>k</em></sub></span>の値に依存しません.</p>
<p><br /><span class="math display">$$\frac{d}{d\beta_{k}}\lambda\sum_{k=1}^{K}|\beta_{k}|=\mathrm{signum}(\beta_{k})$$</span><br /></p>
<p>そのため, 縮小するパラメータは, 最尤推定値では完全に0になるという効果があります. したがって, 単に縮小だけではなく変数選択として使うこともできます. <sup>2</sup> Lassoも, 罰則の大きさをデータとして宣言し, 罰則を<code>model</code>ブロックに加えることにより, リッジ回帰と同じくらい簡単にStanで実装できます.</p>
<pre><code>data {
  // ...
  real&lt;lower=0&gt; lambda;
}
// ...
model {
  // ...
  for (k in 1:K)
    increment_log_prob(- lambda * abs(beta[k]));
}</code></pre>
<p><sup>2</sup> 実際には, 勾配に基づくStanのオプティマイザでは完全に0の値となることが保証されません. 勾配降下法で完全に0の値を得るための議論は, Langfordら(2009)を参照してください.</p>
<h5 id="elastic-net">Elastic Net</h5>
<p>ナイーブElastic Net (Zou and Hastie, 2005)は, リッジとLassoの罰則の重みづけ平均を取り入れています. 罰則関数は次式です.</p>
<p><br /><span class="math display">$$r(\beta)=\lambda_{1}\sum_{k=1}^{K}|\beta_{k}|+\lambda_{2}\sum_{k=1}^{K}\beta_{k}^2$$</span><br /></p>
<p>ナイーブElastic Netは, リッジ回帰とLassoの両方の特性を組み合わせたもので, 識別と変数選択の両方ができます.</p>
<p>ナイーブElastic Netは, Stanでは, リッジ回帰とLassoの実装を組み合わせることにより直接実装できます.</p>
<pre><code>data {
  real&lt;lower=0&gt; lambda1;
  real&lt;lower=0&gt; lambda2;
  // ...
}
// ...
model {
  // ...
  for (k in 1:K)
    increment_log_prob(-lambda1 * fabs(beta[k]));
  increment_log_prob(-lambda2 * dot_self(beta));
}</code></pre>
<p><span class="math inline"><em>r</em>(<em>β</em>)</span>は罰則関数ですので, プログラム中では符号は負であることに注意してください.</p>
<p>Elastic Net (Zou and Hastie, 2005)は, ナイーブElastic Netで生成された当てはめ値<span class="math inline">$\hat{\beta}$</span>から, 最終的な<span class="math inline"><em>β</em></span>の推定値を調整するようにしています. Elastic Netの推定値は次式です.</p>
<p><br /><span class="math display">$$\hat{\beta}=(1+\lambda_{2})\beta^{\ast}$$</span><br /></p>
<p>ここで, <span class="math inline"><em>β</em><sup> * </sup></span>は, ナイーブElastic Netの推定値です.</p>
<p>StanでElastic Netを実装するときも, <code>data</code>, <code>parameters</code>, <code>model</code>の各ブロックはナイーブElastic Netと同じままです. それに加えて, Elastic Netの推定値を<code>generated quantities</code>ブロックで計算します.</p>
<pre><code>generated quantities {
  vector[K] beta_elastic_net;
  // ...
  beta_elastic_net &lt;- (1 + lambda2) * beta;
}</code></pre>
<p>誤差のスケールも, Elastic Netの係数<code>beta_elastic_net</code>から<code>generated quantities</code>ブロックで計算する必要があります.</p>
<h5 id="他の罰則付き回帰">他の罰則付き回帰</h5>
<p>James and Stein (1961)のように, 係数の推定値を0ではない値に偏らせるような罰則関数も普通に使われます. 推定値を集団の平均に偏らせる罰則関数も使うことができます(Efron and Morris, 1975; Efron, 2012). 後者の手法は, ベイズ統計で普通に採用される階層モデルに似ています.</p>
<h3 id="事後最頻値の推定">55.3. 事後最頻値の推定</h3>
<p>観測値<span class="math inline"><em>y</em></span>が与えられたときのパラメータ<span class="math inline"><em>θ</em></span>の事後分布<span class="math inline"><em>p</em>(<em>θ</em> ∣ <em>y</em>)</span>に基づいてベイズ点推定を行なうのに普通に使うやり方は3つあります. すなわち, 最頻値（最大値）, 平均値, 中央値です. この節では, 事後分布を最大化するようなパラメータ<span class="math inline"><em>θ</em></span>をもととする推定値について述べ, 続いて次の節では平均値と中央値について議論します.</p>
<p>モデルの事後最頻値に基づく推定値は次式のように定義できます.</p>
<p><br /><span class="math display">$$\hat{\theta}=\mathrm{argmax}_{\theta}\,p(\theta\mid y)$$</span><br /></p>
<p>存在するならば, <span class="math inline">$\hat{\theta}$</span>は, 与えられたデータのもとでのパラメータの事後密度を最大化します. 事後最頻値は, 最大事後(maximum a posteriori, MAP)推定値とも呼ばれます.</p>
<p>20章と53.1節で議論したように, ただひとつの事後最頻値が存在するとは限りません. 事後最頻値を最大にするような値は, ひとつも存在しないこともありえますし, 2つ以上のこともありえます. そのような場合, 事後最頻値の推定値は定義されません. ほとんどのオプティマイザと同様に, Stanのオプティマイザでもそうした状況では問題が発生します. 大域的には最大ではないような, 局所最大値を返すこともありえます.</p>
<p>事後最頻値が存在する場合には, その値は, 対数事前分布に負号をつけたものに等しいような罰則関数を持つ罰則付き最尤推定値に対応するでしょう. これはベイズの定理から導かれます.</p>
<p><br /><span class="math display">$$p(\theta\mid y)=\frac{p(y\mid\theta)p(\theta)}{p(y)}$$</span><br /></p>
<p>これにより次式が保証されます.</p>
<p><br /><span class="math display">$$\begin{array}{ll}\mathrm{argmax}_{\theta}\ p(\theta\mid y) &amp;= \mathrm{argmax}_{\theta}\ \frac{p(y\mid\theta)p(\theta)}{p(y)}\\ &amp;= \mathrm{argmax}_{\theta}\ p(y\mid\theta)p(\theta)\end{array}$$</span><br /></p>
<p>密度は正値をとり, 対数が厳密に単調であることから次式が保証されます.</p>
<p><br /><span class="math display"><em>a</em><em>r</em><em>g</em><em>m</em><em>a</em><em>x</em><sub><em>θ</em></sub> <em>p</em>(<em>y</em> ∣ <em>θ</em>)<em>p</em>(<em>θ</em>) = <em>a</em><em>r</em><em>g</em><em>m</em><em>a</em><em>x</em><sub><em>θ</em></sub> log<em>p</em>(<em>y</em> ∣ <em>θ</em>) + log<em>p</em>(<em>θ</em>)</span><br /></p>
<p>事前分布（正則でも非正則でも）が一様である場合, 事後最頻値は最尤推定値と同じになります.</p>
<p>普通に使われる罰則関数ほとんどについて, 確率的に同じものが存在します. 例えば, リッジ罰則関数は係数への正規事前分布に対応しますし, Lassoはラプラス事前分布に対応します. この逆も常に真です. 対数事前分布に負号をつけたものは常に罰則関数と見なすことができます.</p>
<h3 id="事後平均値の推定">55.4. 事後平均値の推定</h3>
<p>標準的なベイズ法では点推定には（あると仮定して）事後平均値が使われます. 定義は次式です.</p>
<p><br /><span class="math display">$$\hat{\theta} = \int \theta p(\theta\mid y)d\theta$$</span><br /></p>
<p>事後平均値はまさにベイズ推定量とよく呼ばれます. 推定値の期待二乗誤差を最小にする推定量だからです.</p>
<p>各パラメータの事後平均値の推定値は, Stanのインターフェイスから返されます. インターフェイスとデータフォーマットの詳細はRstan, CmdStan, PyStanのユーザーズガイドを参照してください.</p>
<p>事後最頻値が存在しない場合でも, 事後平均値が存在することは少なくありません. 例えば, <span class="math inline">Beta(0.1, 0.1)</span>の場合, 事後最頻値はありませんが, 事後平均値はきちんと定義されて, 値は0.5となります.</p>
<p>事後平均値が存在しないのに, 事後最頻値は存在するという状況のひとつは, 事後分布がコーシー分布<span class="math inline">Cauchy(<em>μ</em>, <em>τ</em>)</span>の場合です. 事後最頻値は<span class="math inline"><em>μ</em></span>ですが, 事後平均値を表す積分は発散します. そのような幅の広い事後分布(<strong>訳注</strong>: 原文はpriorだがposteriorの誤り)は, 実際にモデリングを使うときにはめったに出てきません. パラメータにコーシー分布の事前分布を使うときでも, データより十分な制約が与えられるので, 事後分布は行儀が良くなり, 平均値も存在するようになります.</p>
<p>事後平均値が存在しても, 意味がないものであることもあります. 混合分布モデルで起きる多峰の事後分布の場合や, 閉区間での一様分布の場合がそれに当たります.</p>
<h3 id="事後中央値の推定">55.5. 事後中央値の推定</h3>
<p>事後中央値（すなわち50番目の百分位点または0.5分位）は, ベイズモデルの報告によく使われる, もうひとつの点推定値です. 事後中央値は, 推定値の誤差の期待絶対値を最小化します. こうした推定値は, さまざまなStanのインターフェイスで返されます. フォーマットについてのさらに情報を得るにはRStan, PyStan, CmdStanのユーザーズガイドを参照してください.</p>
<p>事後中央値が意味のないものになることもありえますが, 事後平均値が存在しないようなときでも多くの場合, 事後中央値は存在します. コーシー分布もこれにあてはまります.</p>
<h3 id="推定値の誤差-バイアス偏り-分散">55.6 推定値の誤差, バイアス（偏り）, 分散</h3>
<p>推定値<span class="math inline">$\hat{\theta}$</span>は, 特定のデータ<span class="math inline"><em>y</em></span>のほか, 対数尤度関数<span class="math inline">log<em>p</em>(<em>y</em> ∣ <em>θ</em>)</span>, 罰則付き尤度関数<span class="math inline">log<em>p</em>(<em>y</em> ∣ <em>θ</em>) − <em>r</em>(<em>θ</em>)</span>, 対数確率関数<span class="math inline">log<em>p</em>(<em>y</em>, <em>θ</em>) = log<em>p</em>(<em>y</em> ∣ <em>θ</em>) + <em>l</em><em>o</em><em>g</em><em>p</em>(<em>θ</em>)</span>（<strong>訳注</strong>: 原文は<span class="math inline">log<em>p</em>(<em>y</em>, <em>θ</em>) = log<em>p</em>(<em>y</em>, <em>θ</em>) + <em>l</em><em>o</em><em>g</em><em>p</em>(<em>θ</em>)</span>だがこれは誤り）のうちのいずれかに依存します. この節では, <span class="math inline">$\hat{\theta}$</span>という記法は推定量を示すものとしても定義します. このときの推定量とは, データと（罰則付き）尤度あるいは確率関数の非明示的な関数です.</p>
<h4 id="推定値の誤差">推定値の誤差</h4>
<p>真のパラメータ<span class="math inline"><em>θ</em></span>にしたがって生成された特定の観測値のデータセット<span class="math inline"><em>y</em></span>について, パラメータの推定値と真の値との差が推定誤差です.</p>
<p><br /><span class="math display">$$\mathrm{err}(\hat{\theta}) = \hat{\theta} - \theta$$</span><br /></p>
<h4 id="推定値のバイアス偏り">推定値のバイアス（偏り）</h4>
<p>特定の真のパラメータの値を<span class="math inline"><em>θ</em></span>, 尤度関数を<span class="math inline"><em>p</em>(<em>y</em> ∣ <em>θ</em>)</span>とすると, 推定量のバイアスとは推定誤差の期待値です.</p>
<p><br /><span class="math display">$$\mathbb{E}_{p(y\mid\theta)}[\hat{\theta}-\theta] = \mathbb{E}_{p(y\mid\theta)}[\hat{\theta}] - \theta$$</span><br /></p>
<p>ここで<span class="math inline">$\mathbb{E}_{p(y\mid\theta)}[\hat{\theta}]$</span>は以下のように, 推定値<span class="math inline">$\hat{\theta}$</span>を尤度関数<span class="math inline"><em>p</em>(<em>y</em> ∣ <em>θ</em>)</span>で重みづけてデータセット<span class="math inline"><em>y</em></span>の取りうる範囲全体について積分したものです.</p>
<p><br /><span class="math display">$$\mathbb{E}_{p(y\mid\theta)}[\hat{\theta}] = \int \left(\mathrm{argmax}_{\theta'}p(y\mid\theta')\right)p(y\mid\theta)dy$$</span><br /></p>
<p>バイアスは<span class="math inline"><em>θ</em></span>と同じ次元の多変量です. この期待推定誤差が0であれば推定量は不偏ですが, そうでなければバイアスがあります.</p>
<h5 id="例-正規分布の推定値">例: 正規分布の推定値</h5>
<p>正規分布から抽出された, <span class="math inline"><em>n</em> ∈ 1 : <em>N</em></span>についての観測値<span class="math inline"><em>y</em><sub><em>n</em></sub></span>からなるデータセットがあるとします. これは, <span class="math inline"><em>y</em><sub><em>n</em></sub> ∼ Normal(<em>μ</em>, <em>σ</em>)</span>というモデルを仮定しています. ここで, <span class="math inline"><em>μ</em></span>と<span class="math inline"><em>σ</em> &gt; 0</span>はともにパラメータです. 尤度は次式のとおりです.</p>
<p><br /><span class="math display">$$\log p(y\mid\mu,\sigma) = \sum_{n=1}^{N}\log\mathsf{Normal}(y_{n}\mid\mu,\sigma)$$</span><br /></p>
<p><span class="math inline"><em>μ</em></span>の最尤推定量はちょうど標本平均, すなわち標本の平均値となります.</p>
<p><br /><span class="math display">$$\hat{\mu} = \frac{1}{N}\sum_{n=1}^{N}y_{n}$$</span><br /></p>
<p>この平均についての最尤推定量は不偏です.</p>
<p>分散<span class="math inline"><em>σ</em><sup>2</sup></span>の最尤推定量は, 平均との差の二乗の平均です.</p>
<p><br /><span class="math display">$$\hat{\sigma}^2 = \frac{1}{N}\sum_{n=1}{N}(y_{n} - \hat{\mu})^2$$</span><br /></p>
<p>分散の最尤値は小さい方に偏っています.</p>
<p><br /><span class="math display">$$\mathbb{E}_{p(y\mid\mu,\sigma)}[\hat{\sigma}^2] &lt; \sigma$$</span><br /></p>
<p>最尤推定値が, 平均値の推定値<span class="math inline">$\hat{\mu}$</span>との差に基づいていることがこの偏りの理由です. 実際の平均値をいれてみると, 差の二乗和が大きくなります. すなわち, <span class="math inline">$\mu \neq \hat{\mu}$</span>ならば, 次式のようになります.</p>
<p><br /><span class="math display">$$\frac{1}{N}\sum_{n=1}^{N}(y_{n}-\mu)^2 &gt; \frac{1}{N}\sum_{n=1}^{N}(y_{n}-\hat{\mu})^2$$</span><br /></p>
<p>分散の推定値はもうひとつあり, それが不偏分散（<strong>訳注</strong>: 原文はsample varianceですが, これは誤り）です. 次式のように定義されます.</p>
<p><br /><span class="math display">$$\hat{\sigma}^2 = \frac{1}{N-1}\sum_{n=1}^{N}(y_{n}-\hat{\mu})^2$$</span><br /></p>
<p>(<strong>訳注</strong>: 原文は<span class="math inline">$\hat{\mu}$</span>ですが, これは誤り)</p>
<p>この値は, 最尤推定値よりも<span class="math inline"><em>N</em>/(<em>N</em> − 1)</span>倍だけ大きくなります.</p>
<h4 id="推定値の分散">推定値の分散</h4>
<p>推定量<span class="math inline">$\hat{\theta}$</span>の成分<span class="math inline"><em>k</em></span>の分散も他の分散と同じように計算されます. 期待値との差の二乗の期待値です.</p>
<p><br /><span class="math display">$$\mathrm{var}_{p(y\mid\theta)}[\hat{\theta}_{k}] = \mathbb{E}_{p(y\mid\theta)}[(\hat{\theta}_{k} - \mathbb{E}_{p(y\mid\theta)}[\hat{\theta}_{k}])^{2}]$$</span><br /></p>
<p>推定量の<span class="math inline"><em>K</em> × <em>K</em></span>共分散行列全体も, いつもどおり以下のように定義されます.</p>
<p><br /><span class="math display">$$\mathrm{covar}_{p(y\mid\theta)}[\hat{\theta}] = \mathbb{E}_{p(y\mid\theta)}[(\hat{\theta} - \mathbb{E}[\hat{\theta}])(\hat{\theta}-\mathbb{E}[\hat{\theta}])^{\top}]$$</span><br /></p>
<p>標本データから正規分布の平均と分散を推定する例での計算では, 最尤推定量（すなわち標本平均）は, 分散を最小とするような平均<span class="math inline"><em>μ</em></span>の不偏推定量です. そのことは, 正規ノイズの仮定のもとで最小二乗推定について, また等価ですが, 最尤推定について, ガウス-マルコフの定理によって一定の一般性をもって証明されました. Hastieら(2009)の3.2.2節を参照してください.</p>
<h2 id="ベイズデータ解析">56. ベイズデータ解析</h2>
<p>Gelmanら(2013)はベイズデータ解析の特徴を以下のように述べています.</p>
<blockquote>
<p>ベイズデータ解析と言うときは, 観測する量や, 知りたい量について, 確率モデルを使ってデータから推定を行なう実用的な方法という意味で言っています.</p>
</blockquote>
<p>続けて, ベイズ統計が頻度主義の方法とどのように異なるかを記述しています.</p>
<blockquote>
<p>ベイズ法の本質的な特徴は, 統計解析に基づく推定の際, 不確実性を定量化するのに確率を明示的に使うところにあります.</p>
</blockquote>
<p>厳密な頻度主義者は, 観測値の相対頻度の極値を確率と見なしますから, パラメータについて確率的に言明することを禁止します. パラメータは固定したもので, 確率変数ではないとされるのです.</p>
<p>ベイズ主義者も, パラメータを固定した, 未知のものと扱います. しかし頻度主義者とは異なり, パラメータの事前分布と, パラメータの事後分布の両方を使います. これら事前分布・事後分布と事後予測確率は, パラメータと将来の観測値についての知識を特徴づけるためにあります. 以下で述べるように, 事後分布はベイズ推定の基礎を形づくるものです.</p>
<h3 id="ベイズモデリング">56.1 ベイズモデリング</h3>
<p>Gelmanら(2013)は, 応用ベイズモデリングを以下の3ステップに分解しています.</p>
<ol type="1">
<li><p>すべての観測可能な量と観測不可能な量とについてのフル確率モデルをつくります. このモデルは, モデリングするデータとそれがどのように集められたかということについて, 既存の知識と合致するべきです.</p></li>
<li><p>観測された量を条件として未知の量の事後確率を計算します. 未知の量には, パラメータのような観測できない量や, 将来の観測の予測値のような潜在的には観測可能な量といったものが含まれるでしょう.</p></li>
<li><p>データに対するモデルの当てはまりを評価します. これには, 事後分布の効果を評価することも含まれます.</p></li>
</ol>
<p>典型的には, 3番目のステップでじゅうぶんな当てはまりが得られるまでこのサイクルは繰り返されます. Stanは, 2番目と3番目のステップに含まれる計算を自動化します.</p>
<h3 id="ベイズ推定">56.2 ベイズ推定</h3>
<h4 id="基本的な量">基本的な量</h4>
<p>ベイズ推定のメカニズムはそのままベイズの定理に従っています. 記法を定めて, <span class="math inline"><em>y</em></span>でデータのような観測された量を表し, <span class="math inline"><em>θ</em></span>でパラメータや将来の観測値のような未知の量を表すとします. <span class="math inline"><em>y</em></span>と<span class="math inline"><em>θ</em></span>の両方が確率変数としてモデリングされるでしょう. 定数や, ハイパーパラメータ, 予測変数のような, 既知ですがモデリングされない量を<span class="math inline"><em>x</em></span>で表すとします.</p>
<h4 id="確率関数">確率関数</h4>
<p>確率関数<span class="math inline"><em>p</em>(<em>y</em>, <em>θ</em>)</span>は, データ<span class="math inline"><em>y</em></span>とパラメータ<span class="math inline"><em>θ</em></span>の同時確率関数です. 定数および予測変数<span class="math inline"><em>x</em></span>は, 明示されていませんが条件に含まれていることになっています. パラメータ<span class="math inline"><em>θ</em></span>が与えられたときのデータ<span class="math inline"><em>y</em></span>の条件付き確率関数<span class="math inline"><em>p</em>(<em>y</em> ∣ <em>θ</em>)</span>はサンプリング確率関数と呼ばれます. また, <span class="math inline"><em>y</em></span>と<span class="math inline"><em>x</em></span>を固定して<span class="math inline"><em>θ</em></span>の関数と見たときには尤度関数とも呼ばれます.</p>
<p>定数<span class="math inline"><em>x</em></span>が与えられたときのパラメータについての確率関数<span class="math inline"><em>p</em>(<em>θ</em>)</span>は事前分布と呼ばれます. というのは, データがまだまったく観測されていないときのパラメータの確率を特徴づけるものだからです. 条件付き確率関数<span class="math inline"><em>p</em>(<em>θ</em> ∣ <em>y</em>)</span>は事後分布と呼ばれます. 観測データ<span class="math inline"><em>y</em></span>と定数<span class="math inline"><em>x</em></span>とが与えられたときのパラメータの確率を特徴づけるものだからです.</p>
<h4 id="ベイズの定理">ベイズの定理</h4>
<p>ベイズ推定の技術的な仕組みは, 以下の一連の式のように決まっています. これはベイズの定理のさまざまな形式として知られています（ここでも, 定数<span class="math inline"><em>x</em></span>は明示していません）.</p>
<p><br /><span class="math display">$$ \begin{array}{lll} p(\theta\mid y) &amp;= \frac{p(\theta, y)}{p(y)} &amp; [\mbox{条件付き確率の定義}]\\ &amp;= \frac{p(y\mid\theta)p(\theta)}{p(y)} &amp; [\mbox{連鎖律}] \\ &amp;= \frac{p(y\mid\theta)p(\theta)}{\int_{\Theta}p(y,\theta)d\theta} &amp; [\mbox{全確率の法則}] \\ &amp;= \frac{p(y\mid\theta)p(\theta)}{\int_{\Theta}p(y\mid\theta)p(\theta)d\theta} &amp; [\mbox{連鎖律}] \\ &amp;\propto p(y\mid\theta)p(\theta) &amp; [\mbox{yは固定値}] \end{array} $$</span><br /></p>
<p>ベイズの定理は, 事後確率<span class="math inline"><em>p</em>(<em>θ</em> ∣ <em>y</em>)</span>を「ひっくり返し」て, 尤度<span class="math inline"><em>p</em>(<em>y</em> ∣ <em>θ</em>)</span>と事前分布<span class="math inline"><em>p</em>(<em>θ</em>)</span>だけからなる項で表します（ここでも, 定数および予測変数<span class="math inline"><em>x</em></span>は明示していません）. 最後のステップがStanにとって重要で, 必要なのは定係数を除いた確率関数の部分のみです.</p>
<h4 id="予測推定">予測推定</h4>
<p>（与えられたモデルで）データ<span class="math inline"><em>y</em></span>から推定されるパラメータ<span class="math inline"><em>θ</em></span>の推定値の不確実性は, 事後分布<span class="math inline"><em>p</em>(<em>θ</em> ∣ <em>y</em>)</span>により特徴づけられます. したがって, 事後分布はベイズ予測推定にとってきわめて重要です.</p>
<p><span class="math inline">$\tilde{y}$</span>が新しい, おそらくはまだ未知の観測値で, <span class="math inline">$\tilde{x}$</span>がそれに対応する定数および予測変数とすると, 事後予測確率は次式で与えられます.</p>
<p><br /><span class="math display">$$ p(\tilde{y} \mid y) = \int_{\Theta} p(\tilde{y} \mid \theta)p(\theta\mid y)d\theta $$</span><br /></p>
<p>ここでは, もともとの定数および予測変数<span class="math inline"><em>x</em></span>と, 新しい定数および予測変数<span class="math inline">$\tilde{x}$</span>はともに明示されていません. 事後確率自身と同様, 予測推定は確率的に特徴づけられます. パラメータ<span class="math inline"><em>θ</em></span>の点推定値を使うのではなく, データ<span class="math inline"><em>y</em></span>（と定数<span class="math inline"><em>x</em></span>）が与えられたときの<span class="math inline"><em>θ</em></span>の事後確率<span class="math inline"><em>p</em>(<em>θ</em> ∣ <em>y</em>)</span>によって重みづけつつ, <span class="math inline"><em>θ</em></span>の範囲全体について予測値を平均することにより, 予測が行なわれます.</p>
<p>事後分布は, イベントの確率を推定することにも使われます. 例えば, パラメータ<span class="math inline"><em>θ</em><sub><em>k</em></sub></span>が0より大きいという確率は, 次式により確率的に特徴づけられます.</p>
<p><br /><span class="math display">Pr[<em>θ</em><sub><em>k</em></sub> &gt; 0] = ∫<sub>Θ</sub><em>I</em>(<em>θ</em><sub><em>k</em></sub> &gt; 0)<em>p</em>(<em>θ</em> ∣ <em>y</em>)<em>d</em><em>θ</em></span><br /></p>
<p>指示関数<span class="math inline"><em>I</em>(<em>ϕ</em>)</span>は, 命題<span class="math inline"><em>ϕ</em></span>が真ならば1と, そうでなければ0と評価されます.</p>
<p>将来の観測を含む比較も同様にできるでしょう. 例えば, <span class="math inline">$\tilde{y}_n &gt; \tilde{y}_{n'}$</span>となる確率は, 以下の事後予測確率関数により特徴づけることができます.</p>
<p><br /><span class="math display">$$ \Pr[\tilde{y}_{n} &gt; \tilde{y}_{n'}] = \int_{\Theta}\int_{Y}\mathrm{I}(\tilde{y}_{n} &gt; \tilde{y}_{n'})p(\tilde{y}\mid\theta)p(\theta\mid y)d\tilde{y}d\theta $$</span><br /></p>
<h4 id="事後予測チェック">事後予測チェック</h4>
<p>パラメータをデータに当てはめた後は, モデルの推定を前向きに走らせることにより, そのパラメータを新しいデータセットをシミュレートするのに使うことができます. すると, このような複製データセットと元のデータとを視覚的あるいは統計的に比較して, モデルの当てはまり具合を調べることができます(Gelman et al., 2013, 6章).</p>
<p>Stanでは, 事後シミュレーションは2通りの方法で生成できます. 1つ目の方法は, 予測される変数をパラメータとして扱い, <code>model</code>ブロックでその分布を定義します. 2つ目の方法は, 離散変数でも適用可能で, <code>generated quantitiy</code>ブロックで乱数発生器を使って複製データを生成します.</p>
<h2 id="マルコフ連鎖モンテカルロサンプリング">57. マルコフ連鎖モンテカルロサンプリング</h2>
<p>BUGSと同じく, Stanも推定のため, 事後分布からのサンプルを生成するのに, マルコフ連鎖モンテカルロ(MCMC)という技術を使います.</p>
<h3 id="モンテカルロサンプリング">57.1. モンテカルロサンプリング</h3>
<p>モンテカルロ法は, 解析的には解けずとも, 積分された関数の評価はできるような積分を数値的に近似するために開発されました(Metropolis and Ulam, 1949).</p>
<p>例を挙げると, 確率密度<span class="math inline"><em>p</em>(<em>θ</em>)</span>の平均<span class="math inline"><em>μ</em></span>は次の積分で定義されます.</p>
<p><br /><span class="math display"><em>μ</em> = ∫<sub>Θ</sub><em>θ</em> × <em>p</em>(<em>θ</em>)<em>d</em><em>θ</em></span><br /></p>
<p>非常に複雑というほどではないベイズモデルでも, 事後密度<span class="math inline"><em>p</em>(<em>θ</em> ∣ <em>y</em>)</span>は, 解析的には評価できない積分になってしまいます. 事後分布は定数と予測変数<span class="math inline"><em>x</em></span>にも依存しますが, これ以降は省略して既知のものとします.</p>
<p>ここで, <span class="math inline"><em>p</em>(<em>θ</em>)</span>から独立なサンプルを抽出できるとして, <span class="math inline"><em>θ</em><sup>(1)</sup>, <em>θ</em><sup>(2)</sup>, …, <em>θ</em><sup>(<em>N</em>)</sup></span>を<span class="math inline"><em>N</em></span>個のそのようなサンプルとします. <span class="math inline"><em>p</em>(<em>θ</em>)</span>の平均<span class="math inline"><em>μ</em></span>のモンテカルロ推定値<span class="math inline">$\hat{\mu}$</span>は標本平均として与えられます.</p>
<p><br /><span class="math display">$$ \hat{\mu} = \frac{1}{N}\sum_{n=1}^{N} \theta^{(n)} $$</span><br /></p>
<p>確率関数<span class="math inline"><em>p</em>(<em>θ</em>)</span>が有限の平均と分散を持つなら, 大数の法則により, サンプルサイズが増加するとモンテカルロ平均は正しい値に収束することが保証されます.</p>
<p><br /><span class="math display">$$ \lim_{N \to \infty} \hat{\mu} = \mu $$</span><br /></p>
<p>有限の平均と分散を仮定すると, 推定誤差は中心極限定理に従うので, <span class="math inline"><em>N</em></span>の平方根に反比例して減少します.</p>
<p><br /><span class="math display">$$ |\mu - \hat{\mu}| \propto \frac{1}{\sqrt{N}} $$</span><br /></p>
<p>したがって, 平均を1桁正確に推定するためには, 100倍のサンプルが必要になります. 2桁精度を上げるには1万倍のサンプルということになります. モンテカルロ法が, 非常に正確な推定値というよりも数桁以内の大雑把な推定に向いているのはこの理由によります. 実用上も, ある量を点推定するのに, 元になるデータのサンプルが持つ不確実性よりも精度が上がることはありません. そのため, 何桁もの精度が得られないことが, 統計モデルの実用の際に問題になることは滅多にありません.</p>
<h3 id="マルコフ連鎖モンテカルロサンプリング-1">57.2. マルコフ連鎖モンテカルロサンプリング</h3>
<p>マルコフ連鎖モンテカルロ(MCMC)法は, 独立したサンプルを単純には抽出できないような状況のために開発されました(Metropolisら, 1953).</p>
<p>マルコフ連鎖は, 確率変数<span class="math inline"><em>θ</em><sup>(1)</sup>, <em>θ</em><sup>(2)</sup>, …</span>の系列であり, ある変数は, 直前の値が与えられたとき, それ以外の変数とは条件付き独立になります. したがって, <span class="math inline"><em>θ</em> = <em>θ</em><sup>(1)</sup>, <em>θ</em><sup>(2)</sup>, …, <em>θ</em><sup>(<em>N</em>)</sup></span>ならば次式のようになります.</p>
<p><br /><span class="math display">$$ p(\theta) = p(\theta^{(1)}) \prod_{n=2}^{N} p(\theta^{(n)} \mid \theta^{(n-1)}) $$</span><br /></p>
<p>Stanでは, その次の状態を生成するのに, 60章で述べる方法でハミルトニアンモンテカルロ法を使っています.</p>
<p>Stanおよび他のMCMCサンプラーが生成するマルコフ連鎖は, マルコフ連鎖の中心極限定理に必要という意味でエルゴード的です. 大雑把にいうと, <span class="math inline"><em>θ</em></span>のある値に他の値から適当な可能性で到達するという意味です. このマルコフ連鎖はまた定常的です. これは, 連鎖中の別の位置でも推移確率は異ならないという意味です. したがって, <span class="math inline"><em>n</em>, <em>n</em>′ ≥ 0</span>について, 確率関数<span class="math inline"><em>p</em>(<em>θ</em><sup>(<em>n</em> + 1)</sup> ∣ <em>θ</em><sup>(<em>n</em>)</sup>)</span>は<span class="math inline"><em>p</em>(<em>θ</em><sup>(<em>n</em>′ + 1)</sup> ∣ <em>θ</em><sup>(<em>n</em>′)</sup>)</span>と同じです（慣例に従って確率変数や束縛変数は使い回しをしており, 確率関数は引数で区別しています）.</p>
<p>定常マルコフ連鎖は, それぞれが同じ周辺確率関数を持つような状態について平衡分布を持ちます. したがって, <span class="math inline"><em>p</em>(<em>θ</em><sup>(<em>n</em>)</sup>)</span>は<span class="math inline"><em>p</em>(<em>θ</em><sup>(<em>n</em> + 1)</sup>)</span>と同じ確率関数です. Stanでは, 平衡分布<span class="math inline"><em>p</em>(<em>θ</em><sup>(<em>n</em>)</sup>)</span>は, サンプリングされる確率関数<span class="math inline"><em>p</em>(<em>θ</em>)</span>であり, 一般にはベイズ事後密度です.</p>
<p>MCMC法を使うには, 独立サンプルのモンテカルロ法にはない2つの難題があります. 1つめの問題は, ランダムに初期化されたマルコフ連鎖が, いつその平衡分布に収束したかを決定することです. 2つめの問題は, マルコフ連鎖からの抽出に相関があることです. そうなると, 中心極限定理による推定誤差への制約はもはや当てはまりません. これらの問題は次の2つの節で扱います.</p>
<h3 id="初期化と収束モニタリング">57.3. 初期化と収束モニタリング</h3>
<p>マルコフ連鎖が目的分布からのサンプルを生成するのは, 平衡に収束した後だけです. 残念ながら, これが保証されるのは理論的には極限においてのみです. 実用的には, マルコフ連鎖が収束したかどうかをモニタリングするための診断が必要になります.</p>
<h4 id="potential-scale-reduction">Potential Scale Reduction</h4>
<p>連鎖が平衡分布に収束したかどうかをモニタリングする方法の1つは, ランダムに初期化された別の連鎖と性質を比較することです. これは, Gelman and Rubin (1992)のpotential scale reduction 統計量<span class="math inline">$\hat{R}$</span>の動機づけとなっているものです. <span class="math inline">$\hat{R}$</span>統計量は, 各連鎖内でのサンプルの分散の平均と, 連鎖をまたいでプールしたサンプルの分散との比を測定します. もしすべての連鎖が平衡に達しているなら, これらはすべて同じで<span class="math inline">$\hat{R}$</span>は1になるでしょう. 連鎖が共通の分布に収束していないなら, <span class="math inline">$\hat{R}$</span>は1より大きくなるでしょう.</p>
<p>GelmanとRubinが推奨するところでは, ばらばらの初期値をパラメータに与えて, 独立したマルコフ連鎖を初期化し, すべての値について<span class="math inline">$\hat{R}$</span>が1.1より小さくなるまでサンプリングします. Stanでは, ユーザーがパラメータの初期値を指定できますし, 乱数によってばらばらの初期値を抽出することもできます.</p>
<p><span class="math inline">$\hat{R}$</span>統計量は, <span class="math inline"><em>M</em></span>本のマルコフ連鎖の組<span class="math inline"><em>θ</em><sub><em>m</em></sub></span>について定義されます. このとき, 各連鎖は<span class="math inline"><em>N</em></span>サンプル<span class="math inline"><em>θ</em><sub><em>m</em></sub><sup>(<em>n</em>)</sup></span>からなっています. サンプル間分散の推定値は次式です.</p>
<p><br /><span class="math display">$$ B = \frac{N}{M-1} \sum_{m=1}^{M}(\bar{\theta}_{m}^{(\bullet)} - \bar{\theta}_{\bullet}^{(\bullet)})^{2} $$</span><br /></p>
<p>ここで,</p>
<p><br /><span class="math display">$$ \bar{\theta}_{m}^{(\bullet)} = \frac{1}{N}\sum_{n=1}^{N}\theta_{m}^{(n)} \quad \text{\mbox{かつ}} \quad \bar{\theta}_{\bullet}^{(\bullet)} = \frac{1}{M}\sum_{m=1}^{M}\bar{\theta}_{m}^{(\bullet)} $$</span><br /></p>
<p>サンプル内分散は次式です.</p>
<p><br /><span class="math display">$$ W = \frac{1}{M}\sum_{m=1}^{M}s_{m}^{2} $$</span><br /></p>
<p>ここで,</p>
<p><br /><span class="math display">$$ s_{m}^{2} = \frac{1}{N-1}\sum_{n=1}^{N}(\theta_{m}^{(n)}-\bar{\theta}_{m}^{(\bullet)})^{2} $$</span><br /></p>
<p>分散の推定量は次式です.</p>
<p><br /><span class="math display">$$ \widehat{\mathrm{var}}^{+}(\theta \mid y) = \frac{N-1}{N}W + \frac{1}{N}B $$</span><br /></p>
<p>最後に, potential scale reduction 統計量は以下のように定義されます.</p>
<p><br /><span class="math display">$$ \hat{R} = \sqrt{\frac{\widehat{\mathrm{var}}^{+}(\theta \mid y)}{W}} $$</span><br /></p>
<h4 id="不ぞろいな連鎖に対する一般化hatr">不ぞろいな連鎖に対する一般化<span class="math inline">$\hat{R}$</span></h4>
<p>ここで, 各連鎖が異なる数のサンプルからなっていても良いとします. <span class="math inline"><em>N</em><sub><em>m</em></sub></span>を連鎖<span class="math inline"><em>m</em></span>内のサンプルの数とします. このとき, 連鎖<span class="math inline"><em>m</em></span>についての連鎖内平均の式には連鎖のサイズ<span class="math inline"><em>N</em><sub><em>m</em></sub></span>を使います.</p>
<p><br /><span class="math display">$$ \bar{\theta}_{m}^{(\bullet)} = \frac{1}{N_{m}}\sum_{n=1}^{N_{m}}\theta_{n}^{(m)} $$</span><br /></p>
<p>連鎖内分散の推定値も同様です.</p>
<p><br /><span class="math display">$$ s_{m}^{2} = \frac{1}{N_{m}-1}\sum_{n=1}^{N_{m}}(\theta_{m}^{(n)}-\bar{\theta}_{m}^{(\bullet)})^{2} $$</span><br /></p>
<p><span class="math inline">$\bar{\theta}_{\bullet}^{(\bullet)}$</span>, <span class="math inline"><em>B</em></span>, <span class="math inline"><em>W</em></span>といった, 連鎖をまたいで平均をとる項は前と同じ定義で, 各連鎖が推定値に同じ効果を持つことが保証されます. もし平均がサイズで重み付けされるとしたら, 1本の長い連鎖が統計量に強い影響を与え, 複数の連鎖を使って収束をモニタリングするという目的は達成できないでしょう.</p>
<p><span class="math inline"><em>N</em></span>という項を含むので, 推定値<span class="math inline">$\widehat{\mathrm{var}}^{+}$</span>は一般化する必要があります. 最初の項を展開します.</p>
<p><br /><span class="math display">$$ \frac{N-1}{N}W = \frac{N-1}{N}\frac{1}{M}\sum_{m=1}^{M}\frac{1}{N-1}\sum_{n=1}^{N}(\theta_{m}^{(n)}-\bar{\theta}_{m}^{(\bullet)})^{2} = \frac{1}{M}\sum_{m=1}^{M}\frac{1}{N}\sum_{n=1}^{N}(\theta_{m}^{(n)}-\bar{\theta}_{m}^{(\bullet)})^{2} $$</span><br /></p>
<p>第2項です.</p>
<p><br /><span class="math display">$$ \frac{1}{N}B = \frac{1}{M-1}\sum_{m=1}^{M}(\bar{\theta}_{m}^{(\bullet)}-\bar{\theta}_{\bullet}^{(\bullet)})^{2} $$</span><br /></p>
<p>分散の推定量は自然に一般化されます.</p>
<p><br /><span class="math display">$$ \widehat{\mathrm{var}}^{+}(\theta \mid y) = \frac{1}{M}\sum_{m=1}^{M}\frac{1}{N_{m}}\sum_{n=1}^{N_{m}}(\theta_{m}^{(n)}-\bar{\theta}_{m}^{(\bullet)})^{2} + \frac{1}{M-1}\sum_{m=1}^{M}(\bar{\theta}_{m}^{(\bullet)}-\bar{\theta}_{\bullet}^{(\bullet)})^{2} $$</span><br /></p>
<p>連鎖がすべて同じ長さならば, この定義は前の節でのものと等価です. この一般化された分散の推定量と連鎖間分散の推定値は, 前の節の<span class="math inline">$\hat{R}$</span>の式に直接組み込めるでしょう.</p>
<h4 id="分割したhatrによる非定常性の検出">分割した<span class="math inline">$\hat{R}$</span>による非定常性の検出</h4>
<p>potential scale reduction 統計量<span class="math inline">$\hat{R}$</span>を計算する前に, 各連鎖を半分ずつ2つに分割することもできます. これにより追加の平均が得られ, 連鎖内での非定常性を検出できます. 1本の連鎖が次第に増加するような値を取っており, もう1本の連鎖が次第に減少する値を取っているとき, それらはうまく混合してはいないのに, <span class="math inline">$\hat{R}$</span>の値は1に近い値を取ることがありえます. この場合, 各連鎖を2つに分割すると, 各連鎖の前半と後半とは混合していないので, <span class="math inline">$\hat{R}$</span>の値は1よりもかなり大きくなるでしょう.</p>
<h4 id="収束は大域的である">収束は大域的である</h4>
<p>よくある疑問は, パラメータあるいは生成量の一部だけの収束をモニタリングしても良いかどうかというものです. 端的に言うと答えは「いいえ」ですが, これについてはこの節でさらに詳しく述べます.</p>
<p>例えば, <code>lp__</code>の値を考えましょう. これは, 対数事後密度（定数は除く）です. もし<code>lp__</code>が収束していないなら, いかなる実用的な意味においても収束したと結論づけるのは誤りです. これは, 全ての連鎖は実際にパラメータ空間の別々の部分に存在しているからです. とはいえ, <code>lp__</code>の収束を測定するのは, 以下に書くようにとりわけトリッキーです.</p>
<h5 id="漸近的かつ過渡的な状態-vs.-平衡">漸近的かつ過渡的な状態 vs. 平衡</h5>
<p>マルコフ連鎖の収束は, モニタリングするパラメータの関数をどう選ぶかに依存しないという意味で大域的な特性です. 収束前の「過渡的な状態」と収束後の「平衡」との間には明確な境界はありません. ここで起きていることは, 連鎖内の状態の数が無限に近づくにつれ, 連鎖内の取り得る状態の分布が目的分布に近づき, その極限では, あらゆる積分可能なモンテカルロ推定量の期待値が真の期待値に収束するということです. ここではウォームアップのようなものはありません. というのは, 極限では初期状態の影響は完全に消えているからです.</p>
<h5 id="多変量の場合の関数の収束">多変量の場合の関数の収束</h5>
<p><span class="math inline">$\hat{R}$</span>統計量は, あるマルコフ連鎖とある関数の合成を考えていることになりますので, マルコフ連鎖が収束していれば, マルコフ連鎖と関数の合成は全て収束するでしょう. 多変量の場合の関数の収束は, Cramer-Woldの定理により, 確率変数のあらゆる周辺和が収束することと等価です.</p>
<p>制約のない空間から制約のある空間への変換は単にもう一つ別の関数となるため, 収束には影響ありません.</p>
<p>異なる関数は異なる自己相関を持つ可能性があります. しかしマルコフ連鎖が平衡に達していれば, マルコフ連鎖と関数の合成はすべて一貫して収束するはずです. 正式には, 収束していないように見える関数があれば, どんなものであれ不安材料となります. あらゆる関数を検証するのは合理的ではないでしょうが, 少なくともlp__とその他の測定された量は一貫して収束しているべきです.</p>
<p><code>lp__</code>が目立って異なる点は, 位置が急速に変化する傾向があり, そのため外れ値の影響を受けやすいことです.</p>
<h5 id="有限な状態の数">有限な状態の数</h5>
<p>状態の数が有限だとどうなるでしょう? 強い幾何学的エルゴード性を証明できるなら（これは, サンプラーと目的分布に依存します）, 連鎖が初期状態を忘れるような有限な時間が, 高い確率で存在することを示すことができます. これは自己相関時間とウォームアップ時間の両方についてです. しかし, それが存在して有限であること示せるとしても（ほとんど不可能ですが）, 実際の値を解析的に計算することはできません.</p>
<p>したがって実際には, 期待値がかなり正確と言えるほどに, 有限の抽出数が十分に大きいことを期待することしかできません. ウォームアップのiterationを取り除くことは期待値の正確さを改善しますが, どれだけの有限な数のサンプルを取り除けば十分であるかは保証されません.</p>
<h5 id="なぜhatrが一貫しない">なぜ<span class="math inline">$\hat{R}$</span>が一貫しない?</h5>
<p>ここで心配すべきことは2つあります.</p>
<p>1つめとして, 上に記したように, 有限の抽出数ではどれほどであっても, 何がしかの初期状態の効果が常に残るでしょう. これは, いくらかの小さな確率（あるいは, 自己相関時間がとても大きければ大きな確率）で大きな外れ値を持つという形で現れます. そのような外れ値に頑健な関数（例えば分位数）なら, より安定して見えますし, より良い<span class="math inline">$\hat{R}$</span>を持つでしょう. そのような外れ値に脆弱な関数なら, 脆さが見える結果となるかもしれません.</p>
<p>2つめとして, <span class="math inline">$\hat{R}$</span>統計量の使用には非常に強い仮定を置いています. とりわけ, 関数が正規分布とみなせると仮定していますし, 最初の2つのモーメント（訳注: 平均と分散）のみを使い, ある種の独立性を仮定しています. ポイントは, この強い仮定が常に成り立つわけではないということです. とくに, 対数事後密度(<code>lp__</code>)の分布はほとんど正規分布に見えることがありません. <span class="math inline"><em>N</em></span>が非常に大きくなっても, 裾が長くて<span class="math inline">$\hat{R}$</span>が大きくなる可能性があります. 生の値の代わりに分位数を使うといった小技により, 興味のあるサンプルをより正規分布に近づけ, その結果<span class="math inline">$\hat{R}$</span>をより正確にする傾向を高めることができます.</p>
<h5 id="収束モニタリングについて最後に">収束モニタリングについて最後に</h5>
<p>「収束」は大域的な特性であり, すべての積分可能な関数について同時に当てはまりますが, <span class="math inline">$\hat{R}$</span>統計量を採用することには追加の仮定が必要であり, 関数すべてに同じようにうまくいくとは限りません.</p>
<p>連鎖間で期待値を比較するだけでも, マルコフ連鎖の正規分布への漸近性に関する信頼性を確認することができ, 標準的な検定も適用できることに注意してください.</p>
<h3 id="有効サンプルサイズ">57.4. 有効サンプルサイズ</h3>
<p>MCMC法がもたらす技術的な難題の2つめは, 一般に連鎖内でサンプルが自己相関を持つことです. これにより, 平均や分散, 分位数といった, 事後分布において興味のある量を推定する際に不確実性が大きくなります.</p>
<p>MCMCの結果の解析一般について, またとくに有効サンプルサイズについての良い入門的な参考文献がGeyer (2011)です. Stanがまさに使っている計算は分割<span class="math inline">$\hat{R}$</span>の計算に従っていて, これには連鎖をプールした計算（平均）と連鎖内の計算（自己相関）の両方が含まれています. これらはこのマニュアルで紹介されますが, Gelmanほか(2013)でより詳細に説明されています.</p>
<h4 id="有効サンプルサイズの定義">有効サンプルサイズの定義</h4>
<p>連鎖内の自己相関が推定値の不確実性をどのくらい増やしているかは有効サンプルサイズ(ESS)で測ることができます. 独立なサンプルならば, サンプルの数<span class="math inline"><em>N</em></span>に基づいて中心極限定理が推定値の不確実性を制限します. 独立でないサンプルの場合は, 独立なサンプルの数は有効サンプルサイズ<span class="math inline"><em>N</em><sub><em>e</em><em>f</em><em>f</em></sub></span>に置き換えられます. これは, <span class="math inline"><em>N</em></span>個の自己相関のあるサンプルと同じ推定能力を持つ独立なサンプルの数です. 例えば, 推定誤差は, <span class="math inline">$1/\sqrt{N}$</span>ではなく<span class="math inline">$1/\sqrt{N_\mathrm{eff}}$</span>に比例します.</p>
<p>ある系列の有効サンプルサイズは, その系列におけるラグの異なる自己相関で定義されます. 平均<span class="math inline"><em>μ</em></span>, 分散<span class="math inline"><em>σ</em><sup>2</sup></span>の同時確率分布<span class="math inline"><em>p</em>(<em>θ</em>)</span>を持つ連鎖の, ラグ<span class="math inline"><em>t</em> ≥ 0</span>での自己相関は次式のように定義されます.</p>
<p><br /><span class="math display">$$ \rho_{t} = \frac{1}{\sigma^2}\int_{\Theta}(\theta^{(n)}-\mu)(\theta^{(n+t)}-\mu)p(\theta)d\theta $$</span><br /></p>
<p>これはまさに, 2本の連鎖の間の相関に, 位置<span class="math inline"><em>t</em></span>だけオフセットをつけたものです. MCMCの設定上, <span class="math inline"><em>θ</em><sup>(<em>n</em>)</sup></span>と<span class="math inline"><em>θ</em><sup>(<em>n</em> + <em>t</em>)</sup></span>とは同じ周辺分布を持つことがわかっていますから, 2つの差の項を掛け算して整理すると次式になります.</p>
<p><br /><span class="math display">$$ \rho_{t} = \frac{1}{\sigma^2}\int_{\Theta}\theta^{(n)}\theta^{(n+t)}p(\theta)d\theta $$</span><br /></p>
<p>自己相関<span class="math inline"><em>ρ</em><sub><em>t</em></sub></span>を持つ過程から生成された<span class="math inline"><em>N</em></span>サンプルの有効サンプルサイズは次式で定義されます.</p>
<p><br /><span class="math display">$$ N_\mathrm{eff} = \frac{N}{\sum_{t=-\infty}^{\infty}\rho_{t}} = \frac{N}{1+2\sum_{t=1}^{\infty}\rho_{t}} $$</span><br /></p>
<h4 id="有効サンプルサイズの推定">有効サンプルサイズの推定</h4>
<p>実用的には, 問題とする確率関数は簡単には積分できませんし, したがって自己相関も有効サンプルサイズも計算できません. そのかわり, こうした量はサンプル自身から推定される必要があります. この節の残りでは, バリオグラムに基づいた自己相関の推定量, それから複数の連鎖に基づいた有効サンプルサイズについて記述します. 簡単のため, 各連鎖<span class="math inline"><em>θ</em><sub><em>m</em></sub></span>の長さはどれも<span class="math inline"><em>N</em></span>であると仮定しましょう.</p>
<p>有効サンプルサイズを推定する方法の1つは, ラグ<span class="math inline"><em>t</em> ∈ {0, 1, …}</span>におけるバリオグラム<span class="math inline"><em>V</em><sub><em>t</em></sub></span>に基づくものです. バリオグラムは, （1変量の）サンプル<span class="math inline"><em>θ</em><sub><em>m</em></sub><sup>(<em>n</em>)</sup></span>について以下のように定義されます. ここで, <span class="math inline"><em>m</em> ∈ {1, …, <em>M</em>}</span>は連鎖であり, <span class="math inline"><em>N</em><sub><em>m</em></sub></span>は連鎖<span class="math inline"><em>m</em></span>のサンプルの数です.</p>
<p><br /><span class="math display">$$ V_t = \frac{1}{M}\sum_{m=1}^{M}\left(\frac{1}{N_{m}-t}\sum_{n=t+1}^{N_{m}}(\theta_{m}^{(n)}-\theta_{m}^{(n-t)})^{2}\right) $$</span><br /></p>
<p>前の節で導入した複数連鎖の分散の推定値<span class="math inline">$\widehat{\mathrm{var}}^{+}$</span>とバリオグラムを使うと, ラグ<span class="math inline"><em>t</em></span>における自己相関を推定できます.</p>
<p><br /><span class="math display">$$ \hat{\rho}_{t} = 1 - \frac{V_{t}}{2\widehat{\mathrm{var}}^{+}} $$</span><br /></p>
<p>もし連鎖が収束していなければ, 分散の推定量<span class="math inline">$\widehat{\mathrm{var}}^{+}$</span>は分散を過大に推定し, そのため自己相関を過大に推定し, 有効サンプルサイズを過小に推定することになります.</p>
<p><span class="math inline"><em>t</em></span>が増加すると, 相関の推定値<span class="math inline">$\hat{\rho}_t$</span>にノイズが増えるため, 通常は<span class="math inline">$\hat{\rho}_t &gt; 0$</span>となるような<span class="math inline">$\hat{\rho}_t$</span>の初期の推定値だけが使われます. <span class="math inline"><em>T</em>′</span>を, <span class="math inline"><em>ρ</em><sub><em>T</em>′ + 1</sub> &lt; 0</span>となるような最初のラグとします.</p>
<p><br /><span class="math display">$$ T' = \mathop{\mathrm{argmin}}\limits_{t} \hat{\rho}_{t+1} &lt; 0 $$</span><br /></p>
<p>有効サンプルサイズの推定量は次式のように定義されます.</p>
<p><br /><span class="math display">$$ \hat{N}_\mathrm{eff} = \frac{1}{2}\frac{MN}{1 + \sum_{t=1}^{T\prime}\hat{\rho}_{t}} $$</span><br /></p>
<p>（訳注: Gelmanほか(2013)では, <span class="math inline">$\hat{N}_\mathrm{eff} = \frac{MN}{1 + 2 \sum_{t=1}^{T}\hat{\rho}_{t}}$</span>, ただし<span class="math inline"><em>T</em></span>は<span class="math inline">$\hat{\rho}_{T+1} + \hat{\rho}_{T+2}$</span>が負になるような最初の奇数, となっています. ）</p>
<p>適正な自己相関が生じ得るのは, ラグが奇数のときだけです(Geyer, 2011). 自己相関はペア間で足し合わせることにより, positive modulo estimator noiseとなることが保証されます. これは, Geyer (2011)において多数の打ち切り基準が提案された背景になっています. Stanは（まだ）ペアでの期待値を計算しませんが, これは, そのつくりからNUTSが負の自己相関の領域をほぼ避けるためです. したがって, 負の自己相関が現れたところで打ち切りを行うのは, 自己相関推定量の中でノイズが大半を占めるようになったら打ち切りを行うことの合理的な近似になっています.</p>
<p>Stanはすべてのラグに対する自己相関を同時に計算します. その際, 適当なパディングを付与して, Eigenの高速フーリエ変換(FFT)パッケージを利用します. 自己相関の計算にFFTを使うことの詳細についてはGeyer (2011)を参照してください.</p>
<h4 id="サンプルの間引き">サンプルの間引き</h4>
<p>一般的な状況では, 自己相関<span class="math inline"><em>ρ</em><sub><em>t</em></sub></span>は, ラグ<span class="math inline"><em>t</em></span>が増加するにつれて減少します. このようなとき, サンプルを間引くことにより, 自己相関が減るでしょう. 例えば, 以下の2つの方法のうちの1つで1000サンプルを生成するとします.</p>
<ol type="1">
<li>収束後に1000サンプルを生成し, そのすべてを保存する.</li>
<li>収束後に10000サンプルを生成し, 10個ごとにサンプルを保存する.</li>
</ol>
<p>両方とも1000サンプルを生成するとはいえ, 間引きのある2番目の方法がより有効なサンプルを生成するでしょう. これは, 間引かれた系列の自己相関<span class="math inline"><em>ρ</em><sub><em>t</em></sub></span>は, 間引かれていないサンプルの<span class="math inline"><em>ρ</em><sub>10<em>t</em></sub></span>に相当するからです. そのため, 自己相関の和は小さくなり, そして有効サンプルサイズは大きくなるでしょう.</p>
<p>一方, メモリとデータの容量に問題がないなら, 10000サンプルすべてを保存する方が, 1000サンプルに間引くよりも有効サンプルサイズは大きくなるでしょう.</p>
<h2 id="ハミルトニアンモンテカルロサンプリング">60. ハミルトニアンモンテカルロサンプリング</h2>
<p>ここではStanで実装されているアルゴリズムの実装の詳細とそれをどのように設定するかを説明します. この章ではハミルトニアンモンテカルロ(Hamiltonian Monte Carlo, HMC)アルゴリズムとその適応的な変種であるno-U-turn sampler(NUTS)をStanでの実装と構成に基づいて説明します. 次の2つの章ではStanのオプティマイザとdiagnostics(診断の方法)について説明します.</p>
<h3 id="ハミルトニアンモンテカルロ">60.1 ハミルトニアンモンテカルロ</h3>
<p>ハミルトニアンモンテカルロ(HMC)とは事後分布間の効率的な遷移を生成するためのサンプリングとして密度関数の導関数を使うようなマルコフ連鎖モンテカルロ法(Markov chain Monte Carlo, MCMC)です(より詳細はBetancourt and Girolami, 2013; Neal, 2011).</p>
<p>この手法はメトロポリス法の受理ステップを行うことで補正された数値積分に基づいて近似されたハミルトン力学系のシミュレーションを用いています. このセクションはBetancourt and Girolami (2013)によるHMCのプレゼンテーションをGelman et al. (2013)の記法に直したものです.</p>
<h4 id="目標とする密度関数">目標とする密度関数</h4>
<p>サンプリングの最終目的はパラメータθに対する密度p(θ)から値を抽出することです. これは典型的には与えられたデータyに対するベイズ事後分布p(θ|y), 特にStanプログラムで書かれたベイズ事後分布になります.</p>
<h4 id="補助運動量変数">補助運動量変数</h4>
<p>HMCでは補助運動量変数ρが導入され, 結合密度</p>
<p><br /><span class="math display"><em>p</em>(<em>ρ</em>, <em>θ</em>) = <em>p</em>(<em>ρ</em>|<em>θ</em>)<em>p</em>(<em>θ</em>0)</span><br /></p>
<p>から値が抽出されます. Stanを含むHMCのほとんどの応用では補助分布としてはパラメータθに依存しない多変量正規分布</p>
<p><br /><span class="math display"><em>ρ</em> ∼ <em>M</em><em>u</em><em>l</em><em>t</em><em>i</em><em>N</em><em>o</em><em>r</em><em>m</em><em>a</em><em>l</em>(0, <em>σ</em>)</span><br /></p>
<p>を用います. 共分散行列Σは目標とする分布の回転とスケール変換のためのユークリッド計量として振舞います(幾何学の詳細についてはBetancourt and Stein, 2011を参照). Stanではこの行列はidentity matrix(単位行列)またはwarmup期間のサンプルから推測され, 制限された対角行列になります. 逆行列<br /><span class="math display"><em>Σ</em> − </span><br />は質量行列(mass matrix)という名前で知られ, Σが単位, 対角, 密行列のいずれかであればそれと同じ性質を持ちます.</p>
<h4 id="ハミルトニアン">ハミルトニアン</h4>
<p>結合密度p(ρ,θ)はハミルトニアン</p>
<figure>
<img src="data:image/png;base64,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" alt="ハミルトニアン" /><figcaption>ハミルトニアン</figcaption>
</figure>
<p>で定義され, 項T</p>
<figure>
<img src="data:image/png;base64,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" alt="運動エネルギー" /><figcaption>運動エネルギー</figcaption>
</figure>
<p>は&quot;運動エネルギー&quot;,項V</p>
<figure>
<img src="data:image/png;base64,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" alt="ポテンシャルエネルギー" /><figcaption>ポテンシャルエネルギー</figcaption>
</figure>
<p>は&quot;ポテンシャルエネルギー&quot;と呼ばれます. このポテンシャルエネルギーはStanでは対数密度の定義から求まります.</p>
<h4 id="遷移の生成">遷移の生成</h4>
<p>パラメータθの現在の値から新しい状態への遷移はメトロポリス法の受理ステップとその前の2つの手順で生成されます.</p>
<p>最初に運動量の値が現在のパラメータの値とは独立に分布</p>
<p><br /><span class="math display"><em>ρ</em> ∼ <em>M</em><em>u</em><em>l</em><em>t</em><em>i</em><em>N</em><em>o</em><em>r</em><em>m</em><em>a</em><em>l</em>0(0, <em>σ</em>)</span><br /></p>
<p>から抽出されます. 運動量はiterationのたびに新しい値になり以前の値に影響されません.</p>
<p>次に現在のパラメータの値θと新しい運動量ρを結合した系(θ, ρ)をハミルトン方程式</p>
<figure>
<img src="data:image/png;base64,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" alt="ハミルトン方程式.PNG" /><figcaption>ハミルトン方程式.PNG</figcaption>
</figure>
<p>にしたがって発展させます. 運動量密度と目標の密度関数は独立, つまりp(ρ|θ)=p(ρ)なので運動量の時間微分のはじめの項∂T/∂θ=0になります. よって時間微分の式は</p>
<p><br /><span class="math display">$$ \dot{\theta},\dot{\rho} $$</span><br /></p>
<p>となります.</p>
<h4 id="蛙飛び積分leapfrog-integrator">蛙飛び積分(Leapfrog Integrator)</h4>
<p>このセクションの最後に2状態の微分方程式を解く作業が残されています. Stanは他のほとんどのHMCの実装と同様に蛙飛び積分(Leapfrog Integrator)を使っています. これはハミルトン方程式の結果を安定させるために調整された数値積分アルゴリズムです.</p>
<p>ほとんどの数値積分と同じ様に蛙飛びアルゴリズムは小さな時間間隔εの離散的なステップごとに行われます.</p>
<p>蛙飛びアルゴリズムは新しい運動量の項をパラメータθとは独立に抽出するか前の運動量の値</p>
<p><br /><span class="math display"><em>ρ</em></span><br /></p>
<p>から始め, ステップの半分だけの運動量の更新と1ステップ分の位置の更新を交互に行います.</p>
<p><br /><span class="math display"><em>L</em><em>e</em><em>a</em><em>p</em><em>f</em><em>r</em><em>o</em><em>g</em><em>o</em><em>f</em><em>ρ</em>, <em>θ</em></span><br /></p>
<p>L回の蛙飛びステップでLεだけの時間の動きがシミュレートされます. この結果(上記の3つのステップのL回繰り返し)で得られる状態を(ρ<em>,θ</em>)と書きます.</p>
<p>蛙飛び積分の誤差は1ステップあたりの時間間隔(step size)であるεの3乗, 大域的にはεの2乗のオーダーになります. Leimkuhler and Reich(2004)には蛙飛び積分の誤差の範囲の導出を含めたハミルトン系の数値積分の詳細な解析が紹介されています.</p>
<h4 id="メトロポリス受理ステップ">メトロポリス受理ステップ</h4>
<p>蛙飛び積分が完全に数値的であれば運動量ベクトルをランダムに生成するのとは別に遷移ごとにランダム化を施す必要はなくなります. しかし実際には積分による数値誤差をメトロポリス受理ステップを適用する際には考慮する必要があります. (ρ,θ)からの遷移で生成された提案値(ρ<em>,θ</em>)が受理される確率は</p>
<p><br /><span class="math display"><em>m</em><em>i</em><em>n</em>(1, exp(<em>H</em>(<em>ρ</em>, <em>θ</em>) − <em>H</em>(<em>ρ</em><sup> * </sup>, <em>θ</em><sup> * </sup>)))</span><br /></p>
<p>であり, もし提案分布が受理されなければ前のパラメータの値が次の値としてサンプルされ, 次のiterationにも使われます.</p>
<h4 id="アルゴリズムのまとめ">アルゴリズムのまとめ</h4>
<p>ハミルトニアンモンテカルロアルゴリズムは初期設定されたパラメータの組θからはじめられます. Stanではこの値はユーザーが設定することもランダムに生成することも出来ます. そして指定された数だけ新しい運動量のサンプリングとパラメータθの現在の値が 時間間隔εで離散化された蛙飛び積分がL回実行が繰り替えされることで更新されます. その後メトロポリス受理ステップが適用され, 提案された状態値(ρ<em>,θ</em>)が使われるか元の状態値のままにするかが決定されます.</p>
<h3 id="hmcアルゴリズムのパラメータ">60.2 HMCアルゴリズムのパラメータ</h3>
<p>ハミルトンモンテカルロ法は以下の3つのパラメータを設定しなければいけません.</p>
<ul>
<li>時間を離散化する間隔 ε</li>
<li>質量行列 <br /><span class="math display"><em>σ</em><sup> − 1</sup></span><br /></li>
<li>ステップの数 L</li>
</ul>
<p>実際, サンプリングの効率, 1回のサンプリングとその繰り返しの速度はこの3つのパラメータに大きく依存します(Neal, 2011;Hoffman and Gelman, 2014).</p>
<p>もしεが大きすぎれば蛙飛び積分は正確なものではなくなりメトロポリス受理ステップにおける提案の多くが棄却されるでしょう. もしεが小さすぎれば蛙飛び積分のシミュレーション時間は長くなってしまうでしょう. これらの困難な点をバランスさせるような受理率を決めることが目標になります.</p>
<p>もしLが小さすぎれば1回の繰り返しあたりのハミルトン方程式の積分軌道は短すぎサンプリングはランダムウォークによって移り変わって行ってしまうことでしょう. Lが大きすぎれば1回の繰り返しに対するアルゴリズムの負荷は大きくなってしまいます.</p>
<p>質量行列Σが事後分布の共分散に合致していない場合, 計算の正確性を確保するためにステップサイズεは小さくしなければなりませんし, 一方同時に統計的な効率性を保つために必要なステップ数Lは大きくなければなりません.</p>
<h4 id="積分時間">積分時間</h4>
<p>実際に積分される時間間隔はLεでステップ数の関数です. いくつかのStanのインターフェースは積分時間tと離散化の間隔(step size)εの近似値を設定します. そのような場合ステップ数は <br /><span class="math display">$$ L= \frac{t}{\epsilon} $$</span><br /> と丸められ実際の積分時間はLεとなるでしょう.</p>
<p>図58.1 warmup中の適応は３段階に分けて行われる: 期間(I) 最初の速いステップ, 期間(II) ゆっくりした適応の展開列, 期間(III) 最後の速い適応. HMCでは速い期間, ゆっくりした期間の双方がステップサイズを調整する為に使われていて, ゆっくりした期間は計量に必要な(共)分散の学習の為に使われる. iteration回数は左端が1で図の右に行くに従って増えていく.</p>
<h4 id="自動パラメータチューニング">自動パラメータチューニング</h4>
<p>Stanはno-U-turn sampling(NUTS)アルゴリズムを使うことで目標とする受理率になるように自動的にεを最適化すること, warmupでのサンプリングのiterationに基づいてΣを推定すること, サンプリング(とwarmup)中に動的にLを調整することができます(Hoffman and Gelman, 2014). .</p>
<p>パラメータの適応を行う場合には (step size, 質量行列を調整することで適応をoffにすることもできます)warmupは図58.1のように3つの期間(interval), 1つのゆっくりと成長する期間の連なりとそれをそれを挟む2つの速い期間とに分けて行われます. ここで速い期間, 遅い期間とはパラメータの適応に局所的な情報か大域的な情報を使うかの違いです. ハミルトニアンモンテカルロサンプラーでは, 例えばstep sizeを速いパラメータ, (共)分散を遅いパラメータとして定義しています. 最初と最後の速い期間のサイズと最初の遅い期間のサイズはカスタマイズすることができ, ユーザーの設定した値はwarpup期間中にalignmentを満たすために少し変更されることがあります.</p>
<p>warmup期間を分割する動機は適応をより安定したものにすることで, 各段階は以下のようになります.</p>
<ul>
<li><p>最初の速い間隔ではチェーンは局所的な情報からのみ学習できるパラメータで典型的集合(typical set) に収束することが許されています. ((典型的集合(typical set)は情報理論で使われている概念を借用したものでここでは平衡状態でマルコフ連鎖(Markov chain)が遷移しているような 実体のある事後確率質量の近傍(または多変量モデルにおける近傍)です. ))</p></li>
<li>この大域的情報を必要とする最初の期間のパラメータ, 例えば(共)分散, が無記憶性の窓期間の列内で推定されます. たいていは速いパラメータもここで推定されます.</li>
<li><p>最後に遅いパラメータの最終的な更新の後に速いパラメータの適応が行われます.</p></li>
</ul>
<p>これらの期間は以下のような調整パラメータにのっとって制御されます. 各パラメータは正の整数である必要があります.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">パラメータ</th>
<th style="text-align: right;">説明</th>
<th style="text-align: center;">デフォルト値</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">初期バッファ</td>
<td style="text-align: right;">最初の速い期間の長さ</td>
<td style="text-align: center;">25</td>
</tr>
<tr class="even">
<td style="text-align: left;">期間バッファ</td>
<td style="text-align: right;">最後の速い期間の長さ</td>
<td style="text-align: center;">50</td>
</tr>
<tr class="odd">
<td style="text-align: left;">窓</td>
<td style="text-align: right;">最初の遅い適応期間の長さ</td>
<td style="text-align: center;">25</td>
</tr>
</tbody>
</table>
<h4 id="パラメータ適応期間の離散化">パラメータ適応期間の離散化</h4>
<p>StanのHMCアルゴリズムはstep sizeを最適化するのに双平均化法(dual averaging, Nesterov, 2009)を使っています. このwarmupの手続きはとても柔軟で完全なもので, StanではHoff- man and Gelman (2014)の記法を用いて, オプションとして指定できるようにそのインターフェイスをユーザに提供しています. 実際最適化の有効性はこのパラメータの値に対して敏感に変化します. しかし我々は双平均化法を使った経験があるのではない限りパラメータの値をデフォルト値から動かすことは推奨しません. より詳細な情報は(Hoffman and Gelman, 2011, 2014)の双平均化法に関する議論を参照して下さい.</p>
<p>双平均法の全てのパラメータを以下に示します.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">パラメータ</th>
<th style="text-align: right;">説明</th>
<th style="text-align: center;">拘束条件</th>
<th style="text-align: center;">デフォルト値</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">δ</td>
<td style="text-align: right;">目標とするメトロポリス受理率</td>
<td style="text-align: center;">δ∈[0,1]</td>
<td style="text-align: center;">0.80</td>
</tr>
<tr class="even">
<td style="text-align: left;">γ</td>
<td style="text-align: right;">適応正規化スケール</td>
<td style="text-align: center;">γ &gt; 0</td>
<td style="text-align: center;">0.05</td>
</tr>
<tr class="odd">
<td style="text-align: left;">κ</td>
<td style="text-align: right;">適応緩和指数</td>
<td style="text-align: center;">κ &gt; 0</td>
<td style="text-align: center;">0.75</td>
</tr>
<tr class="even">
<td style="text-align: left;">t_0</td>
<td style="text-align: right;">適応のイテレーションのオフセット</td>
<td style="text-align: center;">t_0&gt; 0</td>
<td style="text-align: center;">10</td>
</tr>
</tbody>
</table>
<p>目標とする受理率δ(1より小さい値に設定する必要があり, デフォルト値は0.8)を1に近い値に設定することで適応は小さなstep sizeを用いて行われることに成ります. これはイテレーションの増加のコストに対するサンプリングの効率(イテレーションあたりの有効なサンプリング)を高めます. 　 δの値を上げることでいくつかのモデルではそれ以外の方法では行き詰まってしまうような場合にその行き詰まり(blockage)に打ち勝つことができます.</p>
<h4 id="step-sizeのゆらぎ">step sizeのゆらぎ</h4>
<p>数値積分を使った全てのHMCの実装はstep size(時間間隔の離散化)を必要とします. Stanはstep sizeを適応的に決める方法, 明示的に決める方法の両方に対応しています. またStanでは曲率が大きな場合に固定したstep sizeと合わさって悪影響がでるのを避けるためにサンプリングごとランダムなゆらぎ(jitter)を持たせたstep sizeを使うことができます.</p>
<p>ゆらぎの値はもとのstep sizeの値に足される, あるいはもとの値から引かれることになります. それなのでとりうるゆらぎの最大値は1となり, この場合step sizeは0から適応値の2倍までの範囲になります.</p>
<p>小さなstep sizeを設定することでより大きな値ではHMCサンプラーが行き詰まってしまうような場合((訳注：受理率が低い？))にも進むようにできます. 良くない点として値が適応値を下回って揺らいでいる場合必要な蛙飛びのステップ回数が多くなるのでイテレーションを遅くし, 値が適応値を上回って揺らいでいる場合はハミルトン力学系の計算のシミュレーションエラーによる早すぎる棄却を引き起こすということがあります. step sizeのゆらぎに関するより詳細な議論は(Neal, 2011)を参照してください.</p>
<h4 id="ユークリッド計量">ユークリッド計量</h4>
<p>Stanの中の全てのHMCの実装では質量行列(mass matrix), より形式的には計量(metric)と呼ばれる対称で正定値の行列の中から選ばれた2次の運動エネルギー関数として使われています. もし計量が定数であればその実装はユークリッドHMCと呼ばれます. Stanでは以下の3つのユークリッドHMCの実装が使えます.</p>
<p>+単位計量(対角要素が1の対角行列) +対角計量(対角要素が正の値の対角行列) +密な計量(密な正定値対称行列)</p>
<p>ユーザーは計量の形を設定することができます.</p>
<p>もし質量行列が対角に設定されていれば, 正規化された分散((訳注: 共分散？))は1イテレーションの遅い期間(図58.1のIIの期間)毎に推定され, それぞれの推定値はその期間内のイテレーションによってのみ決まります. これはwarmupで指針として使われる早い段階での推定に使われますが, 後には忘れされるので最終的な共分散の値の推定には影響しません.</p>
<p>もし質量行列が密に設定されれば正規化された共分散の推定が行われ, それ自体は単位行列に正規化されるような対角行列に正規化されます.</p>
<p>分散または共分散は多くの浮動小数点演算の繰り返しによる精度の喪失を避けるためにWelford累積を用いて推定されます.</p>
<h4 id="warmup回数と質量行列の推定">warmup回数と質量行列の推定</h4>
<p>質量行列は事後分布の線形(ここでは大域的と同義になる)な相関を補正し, いくつかの問題ではHMCのパフォーマンスを劇的に改善します. これには大域的な相関を知る必要があります.</p>
<p>複雑なモデルでは大域的な相関は大抵は難しく, 不可能でなければ解析的に導かれます. 例えばデータのスケーリングがねじれているような非線形なモデルではデータの正規化(standardizing)は必ずしも助けには成りません. それゆえStanは適応的なwarmupでオンラインに補正を行います. 強い非線形性(ここでは局所性と同義)の相関は正規化を行った場合でさえ学習が遅くなり得ます. これがStanでなぜwarmupが必要でしばしばそれが長い時間を必要とするかの究極的な理由であり, 十分長いwarmupが実質的なパフォーマンスの改善をもたらすことが出来るかの理由でもあります.</p>
<h5 id="非線形性">非線形性</h5>
<p>質量行列は事後分布での線形(ここでは大域的または位置に依存しないことと等価)な相関のみを補正します. 階層的なパラメータ一方では階層的なモデルでよく現れるたちの悪い非線形(ここでは局所的または位置に依存したことと等価)な相関しています. ((リーマニアンHMCだけが計量を実現でき, それによって(パラメータ空間での)位置に依存した質量行列, 非線形な相関の補正が有効になり始めます. ))</p>
<p>密な質量行列の最も大きな問題は質量行列それ自体の推定が卵と鶏のような関係を引き起こすことです. 適切な質量行列をサンプリングから推定するには収束する必要があり, それには適切な質量行列が必要になります.</p>
<h5 id="質量行列が密か対角か">質量行列が密か対角か</h5>
<p>サンプリングに問題があるような統計モデルは一般には密な行列で調節できるような線形相関だけに支配されている訳ではありません. むしろもっと複雑な非線形の相関に支配されていてリーマニアンHMC(Riemannian HMC)のようなより高度なアルゴリズムを用いたパラメータの採取するのがよいです.</p>
<h5 id="warmup回数と曲率">warmup回数と曲率</h5>
<p>MCMCの収束にかかる時間は大まかには自己相関時間と同じです. HMC(とNUTS)のチェーンは自己相関が低くなりがちなのでとても速く収束しがちだからです.</p>
<p>これが唯一の適用できる場合は事後分布の幅の中に一様に分布している場合で多くの複雑なモデルではこの仮定は破れています. 実に多くの場合事後分布の質量は良い性質を満たしていて分布の裾はの曲率は大きく, 別の言い方をすればwarmupがゆっくりな理由は分布の裾においてHMCのイテレーションのコストが高いからと言えます.</p>
<p>分布の裾における低性能は数回のwarmupのイテレーションではカバーできないような振る舞いです. 最初の数回のイテレーションの受理確率とstep sizeを見れば, 解こうとしている問題がいかに悪い問題であるかといったことや, 事前分布をより問題にあうようタイトに設定するとか, パラメータを再調整するなどのモデリングの努力が必要であるかがわかります.</p>
<h4 id="nutsとその調整">NUTSとその調整</h4>
<p>no-U-turnサンプラー(NUTS)は各イテレーションでの 不必要に事後分布の中を横切ることなしに値を提案出来るような適切な数の蛙飛びステップを自動的に選択できます. そのような手法を使う動機としては各ステップでのジャンプ距離の２乗の期待値を最大化すること(例えば(Roberts et al., 1997)を参照)と, 事後分布に相関があるときにメトロポリスサンプラーまたはギブスサンプラーで生じるのランダムウォーク的特性を避けるためというのがあります. NUTSアルゴリズムの厳密な定義と詳細釣り合いの照明は(Hoffman and Gelman, 2011, 2014)を参照してください.</p>
<p>NUTSはひとつ前のイテレーションで抽出されたパラメータで決められる初期位置から出発して提案値を生成します. そして独立な単位正規分布に従う運動量ベクトルを生成します. この最初の系は平衡２分木(balanced binary tree)を作るために時間的に前向き, 後ろ向きの両方に発展させます. NUTSアルゴリズムにおけるそれぞれのイテレーションにおいて木の深さは蛙飛びステップの数が倍になる, 実効的には計算時間が倍になると１増えます. アルゴリズムは以下の2つの条件のうち１つを満たすと停止します.</p>
<ul>
<li>NUTSの基準(部分木上のユークリッド空間内のU-turn)が新しい部分木または木全体で満たされたとき</li>
<li>木全体の深さが定めた最大値に達した時</li>
</ul>
<p>標準的なメトロポリスステップではなく, 最後の発展ステップ(生成されたイテレーションの後半)のスライスサンプリングのパラメータ値が選ばれます.</p>
<p>no-U-turnサンプリングの調整は各イテレーションで評価される木の深さに上限を定めることを伴います. これは深さの最大値のパラメータによって調節することができます. 蛙飛びステップの数は2の深さの最大値-1乗によって抑えられます.</p>
<p>木の深さと実際に計算された蛙飛びステップの数の双方はパラメータと一緒にレポートされ, それぞれtreedepth__ , n_leapfrog__ として出力されます. 最後の部分木は部分的にしか作られていないのでこの2つの値は常に</p>
<p><br /><span class="math display">2<sup><em>t</em><em>r</em><em>e</em><em>e</em><em>d</em><em>e</em><em>p</em><em>t</em><em>h</em> − 1</sup> &lt; <em>N</em><sub><em>l</em></sub><em>e</em><em>a</em><em>p</em><em>f</em><em>r</em><em>o</em><em>g</em> &lt; 2<sup><em>t</em><em>r</em><em>e</em><em>e</em><em>d</em><em>e</em><em>p</em><em>t</em><em>h</em></sup> − 1</span><br /></p>
<p>を満たします. 木の深さはNUTSの診断ツールとして重要です. 例えば木の深さが0というのは最初の蛙飛びステップによる提案値がすぐに拒絶(受理されず)最初の状態に戻ったときにおきます. これは極端な曲率か(少なくとも現在の状態に対して)不適切なステップサイズが選ばれたときに起こりえます. 一方木の深さが最大値に等しいときはNUTSは多くの蛙飛びステップを行い計算時間が非常に長くなるのを避けるために未熟な状態で停止することを示唆しています. 非常に多くの回数のステップは適応がうまく出来ていないことを示していて, 目標とする受理率が非常に高いかあるいは単にサンプルをするには難しい事後分布の形をしているのを示唆していると思われます. 後者の場合パラメータの再設定(reparameterization)が有効でしょう. しかし稀にモデルが正しく定義されていても多くのステップが必要になり, 最大の深さはNUTSの木が必要な大きさだけ成長できるような数まで大きくなるでしょう.</p>
<h5 id="パラメータなしでのサンプリング">パラメータなしでのサンプリング</h5>
<p>有効グラフィカルモデル上での純粋に前向きなデータのシミュレーション(あるいはパラメータとデータをシミュレートするための既知の事前分布から生成的に行われるようなもの)のような状況においては Stanではいかなるパラメータ(<code>parameters</code>)も定義する必要がなくなります. <code>model block</code>は空になり, 全ての出力される量は<code>generated quantities block</code>で作られることになります. 例えばK回試行, 成功率θの二項分布からのN回抽出を生成するようなプログラムは以下のようになります.</p>
<pre><code>data {
      real&lt;lower=0,upper=1&gt; theta;
      int&lt;lower=0&gt; K;
      int&lt;lower=0&gt; N;
    }
    model {
    }
    generated quantities {
      int&lt;lower=0,upper=K&gt; y[N];
      for (n in 1:N)
        y[n] &lt;- binomial_rng(K, theta);
    }</code></pre>
<p>全てのStanプログラムは<code>model block</code>を持たなければいけないのでこのプログラムは空の<code>model block</code>を含みます. このモデルではサンプラーはパラメータがないために固定されたパラメータで調整されなければいけません. パラメータサンプリングなしでは適応をする必要がなく, warpupのイテレーションもする必要がないのでその回数は0にすべきです.</p>
<p>パラメータなしでのサンプリング用に書かれたほとんどのモデルではパラメータが定義される代わりにパラメータのような振る舞いをする<code>data block</code>が置かれます. それでも固定パラメータサンプリングと通常の方法(ランダム, 0固定, または指定した値)による初期化のためにパラメータを含めることは可能です. 例えば上記の例の<code>theta</code>はパラメータとして定義, 初期化することが出来ます.</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>RやPythonなどの高レベルなスクリプト言語から使う場合, Stanのプログラムは動的にリンク可能なオブジェクトファイルに変換されます.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>これらの推定された質量行列は大域的です. これは, サンプリングされるパラメータ空間の全ての点に同じ質量行列が適用されるという意味です. Riemann-manifold HMCではこれが一般化され, 質量行列によって示唆される曲率が位置ごとに変化することが許されます.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>十分に有効サンプルが得られないことは, warmupの期間の長さが不十分であることがしばしば原因です. この再実行戦略は, 最初に正しいiteration数を推測するよりも, 高々約50%多いiterationを消費するだけです.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>このモデルは<a href="https://github.com/stan-dev/example-models/tree/master/misc/cluster" class="uri">https://github.com/stan-dev/example-models/tree/master/misc/cluster</a>から入手可能です.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>クラスタリングではあらゆる混合モデルにおいて識別不可能性が問題となりますが, 分類ではそのような問題はありません. なお, クラスタリングのフルベイズ推定がむずかしいにもかかわらず, 研究者に利用され続けているのは, 予測のためのモデル化というよりも探索的データ分析としての位置づけによるものです.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>このモデルは<a href="https://github.com/stan-dev/example-models/tree/master/misc/cluster" class="uri">https://github.com/stan-dev/example-models/tree/master/misc/cluster</a>から入手可能です.<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>対照的に（罰則付き）最尤推定は, パラメータ化によって不変ではありません.<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>Gelmanの便利な統計用語集（<a href="http://andrewgelman.com/2009/05/24/handy_statistic/" class="uri">http://andrewgelman.com/2009/05/24/handy_statistic/</a>）ではピノキオの原則に言及しています. この原則は「計算上の理由だけで作られたモデルにも魂が宿り, ひとり歩きする可能性がある」というものです. この原則はGelmanがフォーク定理と呼んでいる経験則にも関係があります. その定理は「計算がうまくいかないときは, しばしばあなたのモデルに問題がある」というものです.<a href="#fnref8">↩</a></p></li>
<li id="fn9"><p>この例は説明のためのものです. Stanで対数正規分布を実装するオススメの方法はビルトインの確率分布関数<code>lognormal</code>を使うことです（45.1節を見てください）.<a href="#fnref9">↩</a></p></li>
<li id="fn10"><p>プログラムは<a href="https://github.com/stan-dev/example-models/tree/master/basic_distributions" class="uri">https://github.com/stan-dev/example-models/tree/master/basic_distributions</a>から入手可能です.<a href="#fnref10">↩</a></p></li>
<li id="fn11"><p>問題は三角分布の裾が（とても！）軽いことです. 標準のHMCやNUTSサンプラーは三角分布の角にうまく入っていくことができません. 一方はじめのStanコードでは, <code>y</code>の型を<code>real&lt;lower=-1,upper=1&gt;</code>と宣言しているので, 制約のない変数に逆ロジット変換が適用され, その変換のヤコビアンの絶対値の対数が対数確率に足されます. 結果的に得られるロジット変換された<code>y</code>の分布は問題なく振る舞います. Stanで使われる変換についてもっと情報が知りたい場合は58章を参照してください.<a href="#fnref11">↩</a></p></li>
<li id="fn12"><p>コメントの主な問題は誤解を招く恐れがあることです. それはプログラマー側の誤解によるかもしれませんし, コメントがかかれたあとにプログラムの振る舞いが変えられたためかもしれません. プログラムは常にコードに書かれたように振る舞います. そのため, 複雑なコードをリファクタリングして理解しやすい部品にすることは単にコメントを加えることよりも好ましいのです.<a href="#fnref12">↩</a></p></li>
<li id="fn13"><p>さまざまなビルトインのバリデーション方法はもうすぐStanにも実装されます！現状では代わりに, 単体の制約をチェックするために<code>reject</code>文を使うことができるでしょう.<a href="#fnref13">↩</a></p></li>
<li id="fn14"><p>Stan 2.10.0現在, ユーザー定義関数で例外を発生させる唯一の方法は, 関数が呼ばれる際に（サンプリング文で呼ばれる場合も含む）<code>reject</code>文を使って例外を発生させることです.<a href="#fnref14">↩</a></p></li>
<li id="fn15"><p>Stanの将来のバージョンでは前もって宣言しなくても動くようになるでしょう.<a href="#fnref15">↩</a></p></li>
<li id="fn16"><p>この例は, Stanが<code>rk45</code>ソルバーを実装するために使ったBoost Numeric Odeint library (Ahnert and　Mulansky, 2011)のドキュメントからとってきている.<a href="#fnref16">↩</a></p></li>
<li id="fn17"><p>偶然の一致ではなく, 一般のStanのモデルの事後分布に曲率が高い箇所があると, ユークリッド距離を使うハミルトニアンモンテカルロ（HMC）のサンプリングにおいて同様の問題が起こります. 理由はHMCは, 蛙跳び積分のアルゴリズムや傾きをもとにしたステップを使う微分方程式のソルバーを使っているからです. その微分方程式のソルバーはポテンシャルエネルギーと運動エネルギーの項を分けることができるハミルトン系に特化しています.<a href="#fnref17">↩</a></p></li>
<li id="fn18"><p>訳注: 原文はpriorsとなっていますが, 文脈からすると事後分布です.<a href="#fnref18">↩</a></p></li>
<li id="fn19"><p>この例は, Richard McElreathがStan users groupで提起しました. BUGSおよびJAGSで使われるギブズサンプリングと, Stanで使われるハミルトニアンモンテカルロ(HMC)およびno-U-turnサンプラー(NUTS)との挙動の違いについての質問の中でのことです.<a href="#fnref19">↩</a></p></li>
<li id="fn20"><p>訳注: 原文では, <span class="math inline"><em>λ</em><sub>1</sub> + <em>q</em>, <em>λ</em><sub>1</sub> − <em>q</em></span>ですが, 誤植のようです.<a href="#fnref20">↩</a></p></li>
<li id="fn21"><p><span class="math inline"><em>σ</em></span>の周辺事後分布<span class="math inline"><em>p</em>(<em>σ</em> ∣ <em>y</em>)</span>はこの場合, 少なくとも異なる2つのデータ点がある限りは正則です.<a href="#fnref21">↩</a></p></li>
<li id="fn22"><p>訳注: 原文では, <span class="math inline"><em>λ</em><sub>2</sub> = <em>λ</em><sub>1</sub> + <em>c</em></span>となっていますが, 誤りのようです.<a href="#fnref22">↩</a></p></li>
<li id="fn23"><p>訳注: 原文では<span class="math inline"><em>c</em></span>を能力に加え, 難易度から引くとなっていますが, 上の式からすると双方に加えるのが正しいようです.<a href="#fnref23">↩</a></p></li>
<li id="fn24"><p>訳注: <span class="math inline"><em>x</em><sub><em>k</em>′</sub></span>の係数は正しくは<span class="math inline">(<em>β</em><sub><em>k</em>′</sub> + <em>c</em>(1 − <em>d</em>)<em>β</em><sub><em>k</em></sub>)</span>のようです.<a href="#fnref24">↩</a></p></li>
<li id="fn25"><p>訳注: 原文はdifficultyですが, discriminationの誤りと思われます<a href="#fnref25">↩</a></p></li>
<li id="fn26"><p>ラプラス分布を事前分布にすること（あるいは罰則付き最尤推定のL1正規化）は加法的な不変性を取り除くには十分ではありません. 縮小はしますが, それ自体がパラメータを識別するというわけではありません. <span class="math inline"><em>λ</em><sub>1</sub></span>に定数を加え, <span class="math inline"><em>λ</em><sub>2</sub></span>からそれを引くと, 事前分布で同じ値となる場合があるからです.<a href="#fnref26">↩</a></p></li>
<li id="fn27"><p>訳注: 原文はpriorですが, 文脈からすると事後分布と思われます.<a href="#fnref27">↩</a></p></li>
<li id="fn28"><p>テンパリング法が, 局所最頻値を探す自動化した方法と見られることもあります. しかし, ほとんどのMCMCテンパリング法は, 止めることが難しく, そのまま局所最頻値を探し続けます. Swendsen and Wang, 1986; Neal, 1996bを参照.<a href="#fnref28">↩</a></p></li>
<li id="fn29"><p>訳注: 原文は誤っているようです.<a href="#fnref29">↩</a></p></li>
<li id="fn30"><p>訳注: 原文はmiddleですが, 誤りと思われます.<a href="#fnref30">↩</a></p></li>
</ol>
</section>
</body>
</html>
